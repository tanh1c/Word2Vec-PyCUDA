{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jK_Z_hxwGz"
      },
      "source": [
        "# **Install library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU3wUDVBxnhi",
        "outputId": "eb501dd7-5199-4480-bff3-f41adec71888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pynvjitlink-cu12\n",
            "  Downloading pynvjitlink_cu12-0.7.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Downloading pynvjitlink_cu12-0.7.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynvjitlink-cu12\n",
            "Successfully installed pynvjitlink-cu12-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pynvjitlink-cu12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FBpun1jxD1"
      },
      "source": [
        "# **Setup & Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnZeOFcOd7FM",
        "outputId": "5eb635cb-f958-4d14-ccd7-2eb21af83b31"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package_with_uv(package: str, quiet: bool = True) -> bool:\n",
        "    try:\n",
        "        cmd = [\"uv\", \"pip\", \"install\"]\n",
        "        if quiet:\n",
        "            cmd.append(\"-q\")\n",
        "        cmd.extend([\"--system\", package])\n",
        "\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            capture_output=quiet,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "\n",
        "        if not quiet:\n",
        "            print(f\"✅ {package} installed successfully\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"❌ Failed to install {package}: {e}\")\n",
        "        if not quiet and e.stdout:\n",
        "            print(f\"stdout: {e.stdout}\")\n",
        "        if not quiet and e.stderr:\n",
        "            print(f\"stderr: {e.stderr}\")\n",
        "        return False\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"⚠️ uv not found, trying regular pip for {package}...\")\n",
        "        try:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
        "            if quiet:\n",
        "                cmd.append(\"-q\")\n",
        "            cmd.append(package)\n",
        "\n",
        "            subprocess.check_call(cmd)\n",
        "            if not quiet:\n",
        "                print(f\"✅ {package} installed successfully (via pip)\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"❌ Failed to install {package} with pip: {e2}\")\n",
        "            return False\n",
        "\n",
        "def check_numba_cuda_installed():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 1: Checking numba-cuda installation\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Checking if numba-cuda is installed...\")\n",
        "\n",
        "    try:\n",
        "        import numba\n",
        "        from numba import cuda\n",
        "        print(\"✅ numba-cuda is already installed\")\n",
        "\n",
        "        try:\n",
        "            import numba_cuda\n",
        "            print(f\"  numba version: {numba.__version__ if hasattr(numba, '__version__') else 'unknown'}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return True\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"❌ numba-cuda is NOT installed\")\n",
        "        print(\"⚠️ Please install manually first:\")\n",
        "        print(\"!uv pip install -q --system numba-cuda==0.4.0\")\n",
        "        return False\n",
        "\n",
        "def setup_numba_cuda_config():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 2: Configuring numba-cuda\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Setting up numba-cuda (Official Solution)\")\n",
        "    print(\"Based on: https://github.com/googlecolab/colabtools/issues/5081\")\n",
        "    print()\n",
        "\n",
        "    print(\"Configuring numba-cuda...\")\n",
        "    try:\n",
        "        from numba import config\n",
        "        config.CUDA_ENABLE_PYNVJITLINK = 1\n",
        "        config.CUDA_LOW_OCCUPANCY_WARNINGS = 0\n",
        "        print(\"✅ numba-cuda configuration set\")\n",
        "        print(\"  - CUDA_ENABLE_PYNVJITLINK = 1\")\n",
        "        print(\"  - CUDA_LOW_OCCUPANCY_WARNINGS = 0\")\n",
        "    except ImportError:\n",
        "        print(\"❌ numba not installed - cannot configure\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to configure numba-cuda: {e}\")\n",
        "        return False\n",
        "\n",
        "    print(\"\\nTesting CUDA functionality...\")\n",
        "    try:\n",
        "        from numba import cuda\n",
        "        import numpy as np\n",
        "\n",
        "        if cuda.is_available():\n",
        "            device = cuda.get_current_device()\n",
        "            print(f\"CUDA available: {device.name}\")\n",
        "\n",
        "            @cuda.jit\n",
        "            def increment_by_one(an_array):\n",
        "                pos = cuda.grid(1)\n",
        "                if pos < an_array.size:\n",
        "                    an_array[pos] += 1\n",
        "\n",
        "            test_array = np.zeros(10, dtype=np.float32)\n",
        "            increment_by_one[16, 16](test_array)\n",
        "\n",
        "            expected = np.ones(10, dtype=np.float32)\n",
        "            if np.allclose(test_array, expected):\n",
        "                print(\"✅ CUDA kernel test passed!\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"❌ CUDA kernel test failed\")\n",
        "                return False\n",
        "        else:\n",
        "            print(\"❌ CUDA not available\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CUDA test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def install_all_requirements():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 3: Installing all required packages\")\n",
        "    print(\"Installing required packages for Google Colab...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    packages = [\n",
        "        \"numpy>=1.20.0\",\n",
        "        \"gensim>=4.0.0\",\n",
        "        \"scikit-learn>=1.0.0\",\n",
        "        \"matplotlib>=3.5.0\",\n",
        "        \"seaborn>=0.11.0\",\n",
        "        \"tqdm>=4.60.0\",\n",
        "        \"requests>=2.25.0\",\n",
        "        \"pynvml>=11.0.0\"\n",
        "    ]\n",
        "\n",
        "    success_count = 0\n",
        "    failed_packages = []\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"Installing {package}...\", end=\" \", flush=True)\n",
        "        if install_package_with_uv(package, quiet=True):\n",
        "            print(\"✅\")\n",
        "            success_count += 1\n",
        "        else:\n",
        "            print(\"❌\")\n",
        "            failed_packages.append(package)\n",
        "\n",
        "    print(f\"\\nInstalled {success_count}/{len(packages)} packages successfully\")\n",
        "\n",
        "    if failed_packages:\n",
        "        print(f\"⚠️  Failed packages: {', '.join(failed_packages)}\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def check_gpu():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Checking GPU availability...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"✅ NVIDIA GPU detected:\")\n",
        "            print(result.stdout)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"❌ No NVIDIA GPU detected\")\n",
        "            return False\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"❌ nvidia-smi not found\")\n",
        "        return False\n",
        "\n",
        "def check_cuda():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Checking CUDA availability...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        from numba import cuda\n",
        "        if cuda.is_available():\n",
        "            device = cuda.get_current_device()\n",
        "            print(f\"✅ CUDA available: {device.name}\")\n",
        "\n",
        "            try:\n",
        "                import pynvml\n",
        "                pynvml.nvmlInit()\n",
        "                handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "                total_memory = memory_info.total / 1024**3\n",
        "                print(f\"  Memory: {total_memory:.1f} GB\")\n",
        "            except (ImportError, Exception) as e:\n",
        "                print(f\"  Device: {device.name}\")\n",
        "                print(f\"  (Memory info unavailable: {e})\")\n",
        "\n",
        "            return True\n",
        "        else:\n",
        "            print(\"❌ CUDA not available\")\n",
        "            return False\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"❌ Numba not installed\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"  Word2Vec Implementation - Complete Google Colab Setup\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nThis script combines all setup steps:\")\n",
        "    print(\"  1. Check numba-cuda installation\")\n",
        "    print(\"  2. Configure numba-cuda\")\n",
        "    print(\"  3. Install all required packages\")\n",
        "    print(\"  4. Check GPU and CUDA availability\")\n",
        "\n",
        "    results = {\n",
        "        \"numba_cuda_installed\": False,\n",
        "        \"numba_cuda_configured\": False,\n",
        "        \"requirements_installed\": False,\n",
        "        \"gpu_available\": False,\n",
        "        \"cuda_available\": False\n",
        "    }\n",
        "\n",
        "    results[\"numba_cuda_installed\"] = check_numba_cuda_installed()\n",
        "\n",
        "    if not results[\"numba_cuda_installed\"]:\n",
        "        print(\"\\n⚠️  Warning: numba-cuda is not installed. Please install it first:\")\n",
        "        print(\"   !uv pip install -q --system numba-cuda==0.4.0\")\n",
        "        print(\"   Continuing with other setup steps...\")\n",
        "\n",
        "    results[\"numba_cuda_configured\"] = setup_numba_cuda_config()\n",
        "\n",
        "    if not results[\"numba_cuda_configured\"]:\n",
        "        print(\"\\n⚠️  Warning: Failed to configure numba-cuda. Continuing anyway...\")\n",
        "\n",
        "    results[\"requirements_installed\"] = install_all_requirements()\n",
        "\n",
        "    results[\"gpu_available\"] = check_gpu()\n",
        "\n",
        "    results[\"cuda_available\"] = check_cuda()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"  SETUP SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  ✅ numba-cuda installed: {'✅' if results['numba_cuda_installed'] else '❌'}\")\n",
        "    print(f\"  ✅ numba-cuda configured: {'✅' if results['numba_cuda_configured'] else '❌'}\")\n",
        "    print(f\"  ✅ Requirements installed: {'✅' if results['requirements_installed'] else '❌'}\")\n",
        "    print(f\"  ✅ GPU available: {'✅' if results['gpu_available'] else '❌'}\")\n",
        "    print(f\"  ✅ CUDA available: {'✅' if results['cuda_available'] else '❌'}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if results['gpu_available'] and results['cuda_available']:\n",
        "        print(\"\\nSetup complete! Ready to run Word2Vec training\")\n",
        "        print(\"\\nTo run the full pipeline:\")\n",
        "        print(\"  !python run_all.py\")\n",
        "\n",
        "    elif results['numba_cuda_installed'] and results['numba_cuda_configured']:\n",
        "        print(\"\\nSetup completed successfully\")\n",
        "        print(\"⚠️ Note: GPU/CUDA may not be available, but CPU training is still possible\")\n",
        "        print(\"\\nTo run the full pipeline:\")\n",
        "        print(\"  !python run_all.py\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nSetup completed with some warnings\")\n",
        "        print(\"Some features may not work correctly\")\n",
        "        print(\"\\nTo run anyway:\")\n",
        "        print(\"  !python run_all.py\")\n",
        "\n",
        "    return 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZI-FINNkFma"
      },
      "source": [
        "# **Common Utilities**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3lbJApckDPa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import time\n",
        "import hashlib\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "from numba import cuda\n",
        "import numpy as np\n",
        "from numpy import linalg, ndarray\n",
        "\n",
        "\n",
        "W2V_VERSION = \"1.0\"\n",
        "BLANK_TOKEN = \"<BLANK>\"\n",
        "\n",
        "# Constants for Hierarchical Softmax and Exp Table\n",
        "EXP_TABLE_SIZE = 1000\n",
        "MAX_EXP = 6\n",
        "MAX_CODE_LENGTH = 40\n",
        "\n",
        "\n",
        "def build_vocab(data_path: str) -> List[Tuple[str, int, int]]:\n",
        "    \"\"\"\n",
        "    Build vocabulary from data files -> Returns list of (word, total_count, sentence_count)\n",
        "    \"\"\"\n",
        "    files = [fn for fn in os.listdir(data_path) if fn.startswith(\"0\")]\n",
        "    sentences_per_word = defaultdict(int)\n",
        "    totals_per_word = defaultdict(int)\n",
        "    \n",
        "    for file in files:\n",
        "        with open(os.path.join(data_path, file), encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                less_spacey = re.sub(r\"[ ]{2,}\", \" \", line.strip())\n",
        "                words = less_spacey.split(\" \")\n",
        "                if len(words) > 1:\n",
        "                    uniques = set()\n",
        "                    for word in words:\n",
        "                        uniques.add(word)\n",
        "                        totals_per_word[word] += 1\n",
        "                    for deduped in uniques:\n",
        "                        sentences_per_word[deduped] += 1\n",
        "    \n",
        "    r = []\n",
        "    for word, total in totals_per_word.items():\n",
        "        sent = sentences_per_word[word]\n",
        "        r.append((word, total, sent))\n",
        "    return r\n",
        "\n",
        "\n",
        "def sort_vocab(my_vocab: List[Tuple[str, int, int]]) -> List[Tuple[str, int, int]]:\n",
        "    \"\"\"\n",
        "    Sort vocabulary by frequency (descending) then alphabetically\n",
        "    \"\"\"\n",
        "    vs = [(BLANK_TOKEN, 0, 0)] + sorted(my_vocab, key=lambda t: (-t[1], t[0]))\n",
        "    return vs\n",
        "\n",
        "\n",
        "def prune_vocab(min_occrs: int, my_vocab: List[Tuple[str, int, int]]) -> List[Tuple[str, int]]:\n",
        "    \"\"\"\n",
        "    Prune vocabulary based on minimum sentence occurrences -> Returns only total counts\n",
        "    \"\"\"\n",
        "    if min_occrs > 1:\n",
        "        totals = [(wrd, total_count) for wrd, total_count, sentence_count in my_vocab \n",
        "                 if sentence_count >= min_occrs or wrd == BLANK_TOKEN]\n",
        "        return totals\n",
        "    else:\n",
        "        return [(word, total) for word, total, _ in my_vocab]\n",
        "\n",
        "\n",
        "def bias_freq_counts(vocab: List[Tuple[str, int]], exponent: float) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Apply frequency biasing with given exponent for negative sampling\n",
        "    \"\"\"\n",
        "    totalsson = sum(count for _, count in vocab)\n",
        "    plain = [(word, count / totalsson) for word, count in vocab]\n",
        "    \n",
        "    if exponent == 1.0:\n",
        "        return plain\n",
        "    \n",
        "    exped = [(word, math.pow(count, exponent)) for word, count in plain]\n",
        "    sum_exped = sum([q for _, q in exped])\n",
        "    jooh = [(word, f/sum_exped) for word, f in exped]\n",
        "    return jooh\n",
        "\n",
        "\n",
        "def _get_vocab_cache_key(data_path: str, min_occurs_by_sentence: int, freq_exponent: float) -> str:\n",
        "    \"\"\"\n",
        "    Generate cache key based on vocabulary parameters\n",
        "    \"\"\"\n",
        "    key_string = f\"{data_path}_{min_occurs_by_sentence}_{freq_exponent}\"\n",
        "    return hashlib.md5(key_string.encode()).hexdigest()\n",
        "\n",
        "\n",
        "def _get_vocab_cache_path(cache_key: str) -> str:\n",
        "    \"\"\"\n",
        "    Get path to vocabulary cache file\n",
        "    \"\"\"\n",
        "    cache_dir = \"./output/vocab_cache\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    return os.path.join(cache_dir, f\"vocab_{cache_key}.pkl\")\n",
        "\n",
        "\n",
        "def _save_vocab_cache(vocab: List[Tuple[str, float]], w_to_i: Dict[str, int], word_counts: List[int], cache_path: str):\n",
        "    \"\"\"\n",
        "    Save vocabulary to cache file\n",
        "    \"\"\"\n",
        "    cache_data = {\n",
        "        'vocab': vocab,\n",
        "        'w_to_i': w_to_i,\n",
        "        'word_counts': word_counts\n",
        "    }\n",
        "    with open(cache_path, 'wb') as f:\n",
        "        pickle.dump(cache_data, f)\n",
        "\n",
        "\n",
        "def _load_vocab_cache(cache_path: str) -> Optional[Tuple[List[Tuple[str, float]], Dict[str, int], List[int]]]:\n",
        "    \"\"\"\n",
        "    Load vocabulary from cache file -> Returns None if cache doesn't exist or is invalid\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(cache_path):\n",
        "            return None\n",
        "        with open(cache_path, 'rb') as f:\n",
        "            cache_data = pickle.load(f)\n",
        "        return (cache_data['vocab'], cache_data['w_to_i'], cache_data['word_counts'])\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def handle_vocab(data_path: str, min_occurs_by_sentence: int, freq_exponent: float, use_cache: bool = True):\n",
        "    \"\"\"\n",
        "    Complete vocabulary handling pipeline with optional caching -> Returns: (biased_vocab, w_to_i, word_counts)\n",
        "    - biased_vocab: List of (word, frequency) for negative sampling\n",
        "    - w_to_i: Dictionary mapping word to index\n",
        "    - word_counts: List of word counts (for Huffman tree construction)\n",
        "    \n",
        "    Args:\n",
        "        use_cache: If True, try to load from cache or save to cache after building\n",
        "                   Cache is based on data_path, min_occurs_by_sentence, and freq_exponent\n",
        "                   Changing epochs or embed_dim will not invalidate the cache\n",
        "    \"\"\"\n",
        "    # Try to load from cache\n",
        "    if use_cache:\n",
        "        cache_key = _get_vocab_cache_key(data_path, min_occurs_by_sentence, freq_exponent)\n",
        "        cache_path = _get_vocab_cache_path(cache_key)\n",
        "        cached_vocab = _load_vocab_cache(cache_path)\n",
        "        if cached_vocab is not None:\n",
        "            return cached_vocab\n",
        "    \n",
        "    # Build vocabulary\n",
        "    vocab: List[Tuple[str, int, int]] = build_vocab(data_path)\n",
        "    sorted_vocab: List[Tuple[str, int, int]] = sort_vocab(vocab)\n",
        "    pruned_vocab: List[Tuple[str, int]] = prune_vocab(min_occurs_by_sentence, sorted_vocab)\n",
        "\n",
        "    # Store word counts before biasing\n",
        "    word_counts = [count for _, count in pruned_vocab]\n",
        "    biased_vocab: List[Tuple[str, float]] = bias_freq_counts(pruned_vocab, freq_exponent)\n",
        "    w_to_i: Dict[str, int] = {word: idx for idx, (word, _) in enumerate(biased_vocab)}\n",
        "    \n",
        "    # Save to cache\n",
        "    if use_cache:\n",
        "        cache_key = _get_vocab_cache_key(data_path, min_occurs_by_sentence, freq_exponent)\n",
        "        cache_path = _get_vocab_cache_path(cache_key)\n",
        "        _save_vocab_cache(biased_vocab, w_to_i, word_counts, cache_path)\n",
        "    \n",
        "    return biased_vocab, w_to_i, word_counts\n",
        "\n",
        "\n",
        "def get_subsampling_weights_and_negative_sampling_array(vocab: List[Tuple[str, float]], t: float) -> Tuple[ndarray, ndarray]:\n",
        "    \"\"\"\n",
        "    Calculate subsampling weights and create negative sampling array\n",
        "    \n",
        "    Negative sampling array size is dynamically adjusted based on vocabulary size:\n",
        "    - For small vocabs (< 10k): uses 1M (original default)\n",
        "    - For medium vocabs (10k-100k): uses 10M\n",
        "    - For large vocabs (> 100k): uses 100M (same as word2vec.c original)\n",
        "    \n",
        "    This ensures all words appear in the array and maintains distribution accuracy\n",
        "    \"\"\"\n",
        "    # Subsampling weights\n",
        "    tot_wgt: int = sum([c for _, c in vocab])\n",
        "    freqs: List[float] = [c/tot_wgt for _, c in vocab]\n",
        "\n",
        "    # Clamp negative probabilities to zero\n",
        "    probs: List[float] = [max(0.0, 1-math.sqrt(t/freq)) if freq > 0 else 0.0 for freq in freqs]\n",
        "\n",
        "    # Negative sampling array - precompute for efficient sampling\n",
        "    vocab_size = len(vocab)\n",
        "    \n",
        "    # Dynamically adjust arr_len based on vocabulary size\n",
        "    # Original source code of the Word2Vec paper uses 1e8 (100M), we scale based on vocab size\n",
        "    if vocab_size < 10000:\n",
        "        arr_len = 1000000  # 1M for small vocabs\n",
        "    elif vocab_size < 100000:\n",
        "        arr_len = 10000000  # 10M for medium vocabs\n",
        "    else:\n",
        "        arr_len = 100000000  # 100M for large vocabs (same as word2vec.c in original source code)\n",
        "    \n",
        "    print(f\"Creating negative sampling array with size {arr_len:,} for vocab size {vocab_size:,}\")\n",
        "    \n",
        "    w2 = [round(f*arr_len) for f in freqs]\n",
        "    \n",
        "    # Check if any words would be excluded (rounded to 0)\n",
        "    excluded_count = sum(1 for scaled in w2 if scaled == 0)\n",
        "    if excluded_count > 0:\n",
        "        print(f\"⚠️ WARNING: {excluded_count} words have frequency too low and will be excluded from negative sampling\")\n",
        "        print(f\"⚠️ Consider increasing arr_len or reducing min_occurs threshold\")\n",
        "    \n",
        "    neg_arr = []\n",
        "    for i, scaled in enumerate(w2):\n",
        "        if scaled > 0:  # Only add words that appear at least once\n",
        "            neg_arr.extend([i]*scaled)\n",
        "    \n",
        "    actual_arr_size = len(neg_arr)\n",
        "    print(f\"Negative sampling array created: {actual_arr_size:,} entries ({actual_arr_size/1e6:.2f}M)\")\n",
        "    \n",
        "    return np.asarray(probs, dtype=np.float32), np.asarray(neg_arr, dtype=np.int32)\n",
        "\n",
        "\n",
        "def get_data_file_names(path: str, seed: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Get shuffled list of data file names\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    qq = [fn for fn in os.listdir(path) if fn.startswith(\"0\")]\n",
        "    data_files = sorted(qq)\n",
        "    rng.shuffle(data_files)\n",
        "    return data_files\n",
        "\n",
        "\n",
        "def read_all_data_files_ever(dat_path: str, file_names: List[str], w_to_i: Dict[str, int], \n",
        "                             max_words: int = None) -> Tuple[List[int], List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Read all data files and convert to indices\n",
        "    \n",
        "    Args:\n",
        "        dat_path: Path to data directory\n",
        "        file_names: List of file names to read\n",
        "        w_to_i: Word to index mapping\n",
        "        max_words: Maximum number of words to read (None = all). If specified, will stop reading when total words reach this limit.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (inps, offs, lens) where:\n",
        "        - inps: List of word indices\n",
        "        - offs: List of offsets for each sentence\n",
        "        - lens: List of sentence lengths\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    inps, offs, lens = [], [], []\n",
        "    offset_total = 0\n",
        "    stats = defaultdict(int)\n",
        "    total_words_read = 0\n",
        "    stopped_early = False\n",
        "    \n",
        "    for fn in file_names:\n",
        "        fp = os.path.join(dat_path, fn)\n",
        "        ok_lines = 0\n",
        "        too_short_lines = 0\n",
        "        with open(fp, encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                # Check if we've reached max_words limit\n",
        "                if max_words is not None and total_words_read >= max_words:\n",
        "                    stopped_early = True\n",
        "                    break\n",
        "                \n",
        "                words = [word for word in re.split(r\"[ .]+\", line.strip()) if word]\n",
        "                if len(words) < 2:\n",
        "                    too_short_lines += 1\n",
        "                    continue\n",
        "\n",
        "                idcs = [w_to_i[w] for w in words if w in w_to_i]\n",
        "                le = len(idcs)\n",
        "                \n",
        "                # Check if adding this sentence would exceed max_words\n",
        "                if max_words is not None and total_words_read + le > max_words:\n",
        "                    # Only add words up to the limit\n",
        "                    remaining_words = max_words - total_words_read\n",
        "                    if remaining_words > 0:\n",
        "                        idcs = idcs[:remaining_words]\n",
        "                        le = len(idcs)\n",
        "                    else:\n",
        "                        stopped_early = True\n",
        "                        break\n",
        "                \n",
        "                ok_lines += 1\n",
        "                offs.append(offset_total)\n",
        "                lens.append(le)\n",
        "                inps.extend(idcs)\n",
        "                offset_total += le\n",
        "                total_words_read += le\n",
        "                \n",
        "                # Break if we've reached the limit exactly\n",
        "                if max_words is not None and total_words_read >= max_words:\n",
        "                    stopped_early = True\n",
        "                    break\n",
        "        \n",
        "        stats[\"file_read_lines_ok\"] += ok_lines\n",
        "        stats[\"one_word_sentence_lines_which_were_ignored\"] += too_short_lines\n",
        "        \n",
        "        # Break outer loop if we've reached the limit\n",
        "        if stopped_early:\n",
        "            break\n",
        "\n",
        "    print(f\"read_all_data_files_ever() STATS: {stats}\")\n",
        "    if max_words is not None and stopped_early:\n",
        "        print(f\"⚠️ Stopped early: reached max_words limit of {max_words:,} words\")\n",
        "    tot_tm = time.time()-start\n",
        "    print(f\"read_all_data_files_ever() Total time {tot_tm} s for {len(file_names)} files (avg {tot_tm/len(file_names)} s/file)\")\n",
        "    return inps, offs, lens\n",
        "\n",
        "\n",
        "def init_weight_matrices(vocab_size: int, embed_dim: int, seed: int) -> Tuple[ndarray, ndarray]:\n",
        "    \"\"\"\n",
        "    Initialize weight matrices with Gaussian distribution\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    rows, cols = vocab_size, embed_dim\n",
        "    sigma: float = math.sqrt(1.0/cols)\n",
        "    zs = rng.standard_normal(size=(rows, cols), dtype=np.float32)\n",
        "    xs = sigma * zs\n",
        "    # First row all zero since it represents the blank token\n",
        "    xs[0, :] = 0.0\n",
        "    zs2 = rng.standard_normal(size=(rows, cols), dtype=np.float32)\n",
        "    xs2 = sigma * zs2\n",
        "    xs2[0, :] = 0.0\n",
        "    return xs, xs2\n",
        "\n",
        "\n",
        "def print_norms(weights_cuda):\n",
        "    \"\"\"\n",
        "    Print statistics about vector norms\n",
        "    \"\"\"\n",
        "    w = weights_cuda.copy_to_host()\n",
        "    norms = [linalg.norm(v) for v in w]\n",
        "    a, med, b = np.percentile(norms, [2.5, 50, 97.5])\n",
        "    avg = float(sum(norms) / len(norms))\n",
        "    print(f\"Vector norms (count {len(norms)}) 2.5% median mean 97.5%: {a:0.4f}  {med:0.4f}  {avg:0.4f}  {b:0.4f}\")\n",
        "\n",
        "\n",
        "def write_vectors(weights_cuda, vocab: List[Tuple[str, float]], out_path: str):\n",
        "    \"\"\"\n",
        "    Write vectors to file in word2vec format\n",
        "    \"\"\"\n",
        "    w = weights_cuda.copy_to_host()\n",
        "    pathlib.Path(os.path.dirname(out_path)).mkdir(parents=True, exist_ok=True)\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        # len-1: skip first which is the blank token & all zero\n",
        "        f.write(f\"{len(w)-1} {len(w[0])}\\n\")\n",
        "        for i, v in enumerate(w):\n",
        "            # skip first which is the blank token & all zero\n",
        "            if i == 0:\n",
        "                continue\n",
        "            v_str = \" \".join([str(f) for f in v])\n",
        "            word, _ = vocab[i]\n",
        "            f.write(f\"{word} {v_str}\\n\")\n",
        "\n",
        "\n",
        "def write_json(to_jsonify: Dict[str, Any], json_path: str):\n",
        "    \"\"\"\n",
        "    Write dictionary to JSON file\n",
        "    \"\"\"\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(to_jsonify))\n",
        "        f.write(\"\\n\")\n",
        "        f.flush()\n",
        "\n",
        "\n",
        "def create_exp_table(exp_table_size: int = EXP_TABLE_SIZE, max_exp: float = MAX_EXP) -> ndarray:\n",
        "    \"\"\"\n",
        "    Create precomputed exp table for fast sigmoid calculation\n",
        "    Based on word2vec.c from the original source code of the Word2Vec paper\n",
        "    \n",
        "    Args:\n",
        "        exp_table_size: Size of the exp table (default: 1000)\n",
        "        max_exp: Maximum exponent value (default: 6)\n",
        "    \n",
        "    Returns:\n",
        "        numpy array of precomputed sigmoid values\n",
        "    \"\"\"\n",
        "    exp_table = np.zeros(exp_table_size, dtype=np.float32)\n",
        "    for i in range(exp_table_size):\n",
        "        # Precompute exp((i / exp_table_size * 2 - 1) * max_exp)\n",
        "        exp_value = math.exp((i / exp_table_size * 2 - 1) * max_exp)\n",
        "        # Precompute sigmoid: exp(x) / (exp(x) + 1)\n",
        "        exp_table[i] = exp_value / (exp_value + 1)\n",
        "    return exp_table\n",
        "\n",
        "\n",
        "def init_hs_weight_matrix(vocab_size: int, embed_dim: int) -> ndarray:\n",
        "    \"\"\"\n",
        "    Initialize Hierarchical Softmax weight matrix (syn1)\n",
        "    Based on word2vec.c from the original source code of the Word2Vec paper\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Vocabulary size\n",
        "        embed_dim: Embedding dimension\n",
        "    \n",
        "    Returns:\n",
        "        Weight matrix for internal nodes: (vocab_size - 1, embed_dim)\n",
        "        Initialized with zeros\n",
        "    \"\"\"\n",
        "    # Internal nodes: vocab_size - 1\n",
        "    syn1 = np.zeros((vocab_size - 1, embed_dim), dtype=np.float32)\n",
        "    return syn1\n",
        "\n",
        "\n",
        "def create_huffman_tree(word_counts: List[int], max_code_length: int = MAX_CODE_LENGTH) -> Tuple[ndarray, ndarray, ndarray]:\n",
        "    \"\"\"\n",
        "    Create binary Huffman tree from word counts\n",
        "    Based on word2vec.c from the original source code of the Word2Vec paper\n",
        "    Frequent words will have short unique binary codes\n",
        "    \n",
        "    Args:\n",
        "        word_counts: List of word counts (frequencies)\n",
        "        max_code_length: Maximum code length (default: 40)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (codes_array, points_array, code_lengths):\n",
        "        - codes_array: (vocab_size, max_code_length) binary codes, padded with -1\n",
        "        - points_array: (vocab_size, max_code_length) node indices in path, padded with -1\n",
        "        - code_lengths: (vocab_size,) code length for each word\n",
        "    \"\"\"\n",
        "    vocab_size = len(word_counts)\n",
        "    \n",
        "    # Initialize arrays\n",
        "    count = np.zeros(vocab_size * 2 + 1, dtype=np.int64)\n",
        "    binary = np.zeros(vocab_size * 2 + 1, dtype=np.int32)\n",
        "    parent_node = np.zeros(vocab_size * 2 + 1, dtype=np.int64)\n",
        "    \n",
        "    # Set initial counts\n",
        "    for a in range(vocab_size):\n",
        "        count[a] = word_counts[a]\n",
        "    for a in range(vocab_size, vocab_size * 2):\n",
        "        count[a] = int(1e15)  # Large value for internal nodes\n",
        "    \n",
        "    # Build Huffman tree\n",
        "    pos1 = vocab_size - 1\n",
        "    pos2 = vocab_size\n",
        "    \n",
        "    for a in range(vocab_size - 1):\n",
        "        # Find two smallest nodes\n",
        "        if pos1 >= 0:\n",
        "            if count[pos1] < count[pos2]:\n",
        "                min1i = pos1\n",
        "                pos1 -= 1\n",
        "            else:\n",
        "                min1i = pos2\n",
        "                pos2 += 1\n",
        "        else:\n",
        "            min1i = pos2\n",
        "            pos2 += 1\n",
        "        \n",
        "        if pos1 >= 0:\n",
        "            if count[pos1] < count[pos2]:\n",
        "                min2i = pos1\n",
        "                pos1 -= 1\n",
        "            else:\n",
        "                min2i = pos2\n",
        "                pos2 += 1\n",
        "        else:\n",
        "            min2i = pos2\n",
        "            pos2 += 1\n",
        "        \n",
        "        count[vocab_size + a] = count[min1i] + count[min2i]\n",
        "        parent_node[min1i] = vocab_size + a\n",
        "        parent_node[min2i] = vocab_size + a\n",
        "        binary[min2i] = 1\n",
        "    \n",
        "    # Assign binary codes to each word\n",
        "    codes_array = np.full((vocab_size, max_code_length), -1, dtype=np.int32)\n",
        "    points_array = np.full((vocab_size, max_code_length), -1, dtype=np.int32)\n",
        "    code_lengths = np.zeros(vocab_size, dtype=np.int32)\n",
        "    \n",
        "    for a in range(vocab_size):\n",
        "        b = a\n",
        "        i = 0\n",
        "        code = np.zeros(max_code_length, dtype=np.int32)\n",
        "        point = np.zeros(max_code_length, dtype=np.int64)\n",
        "        \n",
        "        # Traverse from leaf to root\n",
        "        while True:\n",
        "            code[i] = binary[b]\n",
        "            point[i] = b\n",
        "            i += 1\n",
        "            b = parent_node[b]\n",
        "            if b == vocab_size * 2 - 2:\n",
        "                break\n",
        "            if i >= max_code_length:\n",
        "                break  # Safety check\n",
        "        \n",
        "        code_lengths[a] = i\n",
        "        # Store code and point arrays (reversed)\n",
        "        points_array[a, 0] = vocab_size - 2  # Root node\n",
        "        for b_idx in range(i):\n",
        "            codes_array[a, i - b_idx - 1] = code[b_idx]\n",
        "            if b_idx < i - 1:\n",
        "                points_array[a, i - b_idx] = int(point[b_idx] - vocab_size)\n",
        "    \n",
        "    return codes_array, points_array, code_lengths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5KqsC97lmeI"
      },
      "source": [
        "# **Data Handler**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgycGerWlp4P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import time\n",
        "import zipfile\n",
        "import gzip\n",
        "import json\n",
        "from typing import List, Tuple, Dict\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "\n",
        "def clean_text_remove_punctuation(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean text by removing punctuation and normalizing whitespace\n",
        "    \n",
        "    Args:\n",
        "        text: Input text line\n",
        "    \n",
        "    Returns:\n",
        "        Cleaned text with only lowercase letters and spaces\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    # Replace tabs and newlines with spaces\n",
        "    text = re.sub(r'[\\t\\n]', ' ', text)\n",
        "    \n",
        "    # Normalize multiple spaces to single space\n",
        "    text = re.sub(r'[ ]{2,}', ' ', text)\n",
        "    \n",
        "    # Remove all punctuation, keep only letters and spaces\n",
        "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "    \n",
        "    # Convert to lowercase and strip\n",
        "    text = text.lower().strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "def detect_phrases(text: str, word_counts: Dict[str, int], bigram_counts: Dict[Tuple[str, str], int], \n",
        "                   train_words: int, min_count: int = 5, threshold: float = 100.0) -> str:\n",
        "    \"\"\"\n",
        "    Detect and combine phrases in text based on bigram scores\n",
        "    Based on word2phrase.c TrainModel() function from the original source code of the Word2Vec paper\n",
        "    \n",
        "    Args:\n",
        "        text: Input text (space-separated words)\n",
        "        word_counts: Dictionary mapping words to their counts\n",
        "        bigram_counts: Dictionary mapping (word1, word2) tuples to bigram counts\n",
        "        train_words: Total number of words in training data\n",
        "        min_count: Minimum word count threshold\n",
        "        threshold: Score threshold for phrase formation (higher = fewer phrases)\n",
        "    \n",
        "    Returns:\n",
        "        Text with phrases combined (e.g., \"new york\" -> \"new_york\")\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) < 2:\n",
        "        return text\n",
        "    \n",
        "    result = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        if i == len(words) - 1:\n",
        "            # Last word, no bigram possible\n",
        "            result.append(words[i])\n",
        "            break\n",
        "        \n",
        "        word1 = words[i]\n",
        "        word2 = words[i + 1]\n",
        "        \n",
        "        # Check if both words meet min_count\n",
        "        count1 = word_counts.get(word1, 0)\n",
        "        count2 = word_counts.get(word2, 0)\n",
        "        \n",
        "        if count1 < min_count or count2 < min_count:\n",
        "            # One word doesn't meet threshold, keep as separate\n",
        "            result.append(word1)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        bigram = (word1, word2)\n",
        "        count_bigram = bigram_counts.get(bigram, 0)\n",
        "        \n",
        "        if count_bigram == 0:\n",
        "            # Bigram not found, keep as separate\n",
        "            result.append(word1)\n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        # score = (pab - min_count) / pa / pb * train_words (Score formula from word2phrase.c)\n",
        "        score = (count_bigram - min_count) / count1 / count2 * train_words\n",
        "        \n",
        "        if score > threshold:\n",
        "            # Combine into phrase\n",
        "            result.append(f\"{word1}_{word2}\")\n",
        "            i += 2  # Skip both words\n",
        "        else:\n",
        "            # Keep as separate\n",
        "            result.append(word1)\n",
        "            i += 1\n",
        "    \n",
        "    return \" \".join(result)\n",
        "\n",
        "\n",
        "def learn_phrase_vocab(data_path: str, min_count: int = 5) -> Tuple[Dict[str, int], Dict[Tuple[str, str], int], int]:\n",
        "    \"\"\"\n",
        "    Learn vocabulary and bigram counts from training data\n",
        "    Based on word2phrase.c LearnVocabFromTrainFile() function from the original source code of the Word2Vec paper\n",
        "    \n",
        "    Args:\n",
        "        data_path: Path to training data directory\n",
        "        min_count: Minimum word count threshold\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (word_counts, bigram_counts, total_words)\n",
        "    \"\"\"\n",
        "    word_counts = defaultdict(int)\n",
        "    bigram_counts = defaultdict(int)\n",
        "    total_words = 0\n",
        "\n",
        "    data_files = [f for f in os.listdir(data_path) if f.startswith(\"0\")]\n",
        "    data_files.sort()\n",
        "    \n",
        "    print(f\"Learning phrase vocabulary from {len(data_files)} files...\")\n",
        "    \n",
        "    for file_idx, filename in enumerate(data_files):\n",
        "        filepath = os.path.join(data_path, filename)\n",
        "        last_word = None\n",
        "        start = True\n",
        "        \n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    start = True\n",
        "                    last_word = None\n",
        "                    continue\n",
        "                \n",
        "                words = line.split()\n",
        "                for word in words:\n",
        "                    word = word.lower().strip()\n",
        "                    if not word:\n",
        "                        continue\n",
        "                    \n",
        "                    total_words += 1\n",
        "                    \n",
        "                    # Count unigram\n",
        "                    word_counts[word] += 1\n",
        "                    \n",
        "                    # Count bigram (if not at start of sentence)\n",
        "                    if not start and last_word:\n",
        "                        bigram = (last_word, word)\n",
        "                        bigram_counts[bigram] += 1\n",
        "                    \n",
        "                    last_word = word\n",
        "                    start = False\n",
        "                \n",
        "                # Reset at end of line\n",
        "                start = True\n",
        "                last_word = None\n",
        "        \n",
        "        if (file_idx + 1) % 10 == 0:\n",
        "            print(f\"  Processed {file_idx + 1}/{len(data_files)} files...\")\n",
        "\n",
        "    filtered_word_counts = {w: c for w, c in word_counts.items() if c >= min_count}\n",
        "    \n",
        "    print(f\"Vocabulary: {len(filtered_word_counts):,} words (min_count={min_count})\")\n",
        "    print(f\"Bigrams: {len(bigram_counts):,} unique bigrams\")\n",
        "    print(f\"Total words: {total_words:,}\")\n",
        "    \n",
        "    return filtered_word_counts, bigram_counts, total_words\n",
        "\n",
        "\n",
        "def apply_phrases_to_data(data_path: str, output_path: str, word_counts: Dict[str, int], \n",
        "                          bigram_counts: Dict[Tuple[str, str], int], train_words: int,\n",
        "                          min_count: int = 5, threshold: float = 100.0) -> str:\n",
        "    \"\"\"\n",
        "    Apply phrase detection to all data files\n",
        "    \n",
        "    Args:\n",
        "        data_path: Input data directory\n",
        "        output_path: Output data directory\n",
        "        word_counts: Word count dictionary\n",
        "        bigram_counts: Bigram count dictionary\n",
        "        train_words: Total number of words\n",
        "        min_count: Minimum word count\n",
        "        threshold: Phrase score threshold\n",
        "    \n",
        "    Returns:\n",
        "        Path to output directory\n",
        "    \"\"\"\n",
        "    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    data_files = [f for f in os.listdir(data_path) if f.startswith(\"0\")]\n",
        "    data_files.sort()\n",
        "    \n",
        "    print(f\"Applying phrase detection (threshold={threshold}) to {len(data_files)} files...\")\n",
        "    \n",
        "    for file_idx, filename in enumerate(data_files):\n",
        "        input_filepath = os.path.join(data_path, filename)\n",
        "        output_filepath = os.path.join(output_path, filename)\n",
        "        \n",
        "        with open(input_filepath, 'r', encoding='utf-8') as fin, \\\n",
        "             open(output_filepath, 'w', encoding='utf-8') as fout:\n",
        "            \n",
        "            for line in fin:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    fout.write('\\n')\n",
        "                    continue\n",
        "                \n",
        "                # Apply phrase detection\n",
        "                processed_line = detect_phrases(line, word_counts, bigram_counts, \n",
        "                                                train_words, min_count, threshold)\n",
        "                fout.write(processed_line + '\\n')\n",
        "        \n",
        "        if (file_idx + 1) % 10 == 0:\n",
        "            print(f\"  Processed {file_idx + 1}/{len(data_files)} files...\")\n",
        "    \n",
        "    print(f\"Phrase detection complete. Output: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def preprocess_with_phrases(data_path: str, output_path: str, min_count: int = 5, \n",
        "                            threshold1: float = 200.0, threshold2: float = 100.0) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess data with phrase detection (2 passes, like word2phrase.c from the original source code)\n",
        "    \n",
        "    Args:\n",
        "        data_path: Input data directory\n",
        "        output_path: Final output directory\n",
        "        min_count: Minimum word count\n",
        "        threshold1: First pass threshold (higher, fewer phrases)\n",
        "        threshold2: Second pass threshold (lower, more phrases)\n",
        "    \n",
        "    Returns:\n",
        "        Path to final output directory\n",
        "    \"\"\"\n",
        "    print(f\"Preprocessing with phrase detection...\")\n",
        "    print(f\" -Input: {data_path}\")\n",
        "    print(f\" -Output: {output_path}\")\n",
        "    print(f\" -Threshold 1: {threshold1} (first pass)\")\n",
        "    print(f\" -Threshold 2: {threshold2} (second pass)\")\n",
        "    \n",
        "    print(\"\\nStep 1: Learning vocabulary and bigram counts...\")\n",
        "    word_counts, bigram_counts, train_words = learn_phrase_vocab(data_path, min_count)\n",
        "\n",
        "    print(f\"\\nStep 2: First pass phrase detection (threshold={threshold1})...\")\n",
        "    temp_path1 = output_path + \"_phrase1\"\n",
        "    apply_phrases_to_data(data_path, temp_path1, word_counts, bigram_counts, \n",
        "                          train_words, min_count, threshold1)\n",
        "\n",
        "    print(\"\\nStep 3: Relearning vocabulary from first pass...\")\n",
        "    word_counts2, bigram_counts2, train_words2 = learn_phrase_vocab(temp_path1, min_count)\n",
        "    \n",
        "    print(f\"\\nStep 4: Second pass phrase detection (threshold={threshold2})...\")\n",
        "    apply_phrases_to_data(temp_path1, output_path, word_counts2, bigram_counts2, \n",
        "                          train_words2, min_count, threshold2)\n",
        "    \n",
        "    import shutil\n",
        "    if os.path.exists(temp_path1):\n",
        "        shutil.rmtree(temp_path1)\n",
        "        print(f\"Cleaned up temporary directory: {temp_path1}\")\n",
        "    \n",
        "    print(f\"\\nPhrase preprocessing complete: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def download_wmt14_news(output_dir: str = \"./data\") -> str:\n",
        "    \"\"\"\n",
        "    Download and combine multiple years of WMT14 and WMT15 News Crawl datasets\n",
        "    Downloads WMT14 year 2012 and WMT15 year 2014, combines them into a single file\n",
        "    Returns path to combined news file\n",
        "    \"\"\"\n",
        "    datasets = [\n",
        "        (\"WMT14\", 2012, \"http://www.statmt.org/wmt14/training-monolingual-news-crawl\"),\n",
        "        (\"WMT15\", 2014, \"https://www.statmt.org/wmt15/training-monolingual-news-crawl\"),\n",
        "    ]\n",
        "    \n",
        "    output_path = os.path.join(output_dir, \"wmt14\")\n",
        "    combined_file = os.path.join(output_path, \"news.combined.en.shuffled\")\n",
        "    \n",
        "    # Create output directory\n",
        "    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Check if combined file already exists\n",
        "    if os.path.isfile(combined_file):\n",
        "        print(f\"WMT14/WMT15 News combined file already exists at: {combined_file}\")\n",
        "        return combined_file\n",
        "    \n",
        "    # Download and extract each dataset\n",
        "    downloaded_files = []\n",
        "    for wmt_version, year, base_url in datasets:\n",
        "        train_file = f\"news.{year}.en.shuffled\"\n",
        "        train_gz = f\"{train_file}.gz\"\n",
        "        train_url = f\"{base_url}/{train_gz}\"\n",
        "        news_file = os.path.join(output_path, train_file)\n",
        "        gz_path = os.path.join(output_path, train_gz)\n",
        "        \n",
        "        # Check if already extracted\n",
        "        if os.path.isfile(news_file):\n",
        "            print(f\"{wmt_version} News {year} already exists at: {news_file}\")\n",
        "            downloaded_files.append(news_file)\n",
        "            continue\n",
        "        \n",
        "        # Download if missing\n",
        "        if not os.path.isfile(gz_path):\n",
        "            print(f\"Downloading {wmt_version} News {year} ({train_gz})...\")\n",
        "            try:\n",
        "                with requests.get(train_url, stream=True, timeout=30) as response:\n",
        "                    response.raise_for_status()\n",
        "                    total_size = int(response.headers.get('content-length', 0))\n",
        "                    \n",
        "                    with open(gz_path, 'wb') as f:\n",
        "                        with tqdm.tqdm(total=total_size, unit='B', unit_scale=True, \n",
        "                                     desc=f\"Downloading {year}\") as pbar:\n",
        "                            for chunk in response.iter_content(chunk_size=8192):\n",
        "                                if chunk:\n",
        "                                    f.write(chunk)\n",
        "                                    pbar.update(len(chunk))\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"⚠️ Warning: Could not download {train_url}: {e}\")\n",
        "                print(f\"⚠️ Skipping {wmt_version} year {year}\")\n",
        "                continue\n",
        "        \n",
        "        # Extract if needed\n",
        "        if os.path.isfile(gz_path) and not os.path.isfile(news_file):\n",
        "            print(f\"Extracting {gz_path}...\")\n",
        "            try:\n",
        "                with gzip.open(gz_path, \"rb\") as source, open(news_file, \"wb\") as target:\n",
        "                    target.write(source.read())\n",
        "                downloaded_files.append(news_file)\n",
        "                print(f\"✅ Extracted {train_file}\")\n",
        "                # Remove gz file to save space\n",
        "                os.remove(gz_path)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error extracting {gz_path}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    if not downloaded_files:\n",
        "        raise FileNotFoundError(\"No WMT14/WMT15 News files were successfully downloaded\")\n",
        "    \n",
        "    # Combine all downloaded files into one\n",
        "    print(f\"\\nCombining {len(downloaded_files)} WMT14/WMT15 News files into: {combined_file}\")\n",
        "    total_lines = 0\n",
        "    \n",
        "    with open(combined_file, 'w', encoding='utf-8') as outfile:\n",
        "        for i, news_file in enumerate(downloaded_files):\n",
        "            if not os.path.isfile(news_file):\n",
        "                print(f\"⚠️ Warning: {news_file} not found, skipping\")\n",
        "                continue\n",
        "            \n",
        "            print(f\"Adding file {i+1}/{len(downloaded_files)}: {os.path.basename(news_file)}\")\n",
        "            line_count = 0\n",
        "            \n",
        "            with open(news_file, 'r', encoding='utf-8') as infile:\n",
        "                for line in infile:\n",
        "                    cleaned = line.strip()\n",
        "                    if cleaned:  # Skip empty lines\n",
        "                        outfile.write(cleaned + '\\n')\n",
        "                        line_count += 1\n",
        "                        total_lines += 1\n",
        "            \n",
        "            print(f\"Added {line_count:,} lines\")\n",
        "    \n",
        "    # Get file size\n",
        "    file_size = os.path.getsize(combined_file) / (1024**3)  # GB\n",
        "    \n",
        "    print(f\"\\n✅ Combined WMT14/WMT15 News dataset created:\")\n",
        "    print(f\" -File: {combined_file}\")\n",
        "    print(f\" -Total lines: {total_lines:,}\")\n",
        "    print(f\" -Size: {file_size:.2f} GB\")\n",
        "    print(f\" -Estimated words: ~{total_lines * 20:,} (assuming ~20 words/line)\")\n",
        "    \n",
        "    return combined_file\n",
        "\n",
        "\n",
        "def download_text8(output_dir: str = \"./data\") -> str:\n",
        "    \"\"\"\n",
        "    Download text8 dataset from http://mattmahoney.net/dc/text8.zip\n",
        "    Returns path to downloaded text8 file\n",
        "    \"\"\"\n",
        "    url = \"http://mattmahoney.net/dc/text8.zip\"\n",
        "    output_path = os.path.join(output_dir, \"text8\")\n",
        "    text8_file = os.path.join(output_path, \"text8\")\n",
        "    \n",
        "    # Create output directory\n",
        "    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Check if already exists\n",
        "    if os.path.isfile(text8_file):\n",
        "        print(f\"Text8 file already exists at: {text8_file}\")\n",
        "        return text8_file\n",
        "    \n",
        "    zip_path = os.path.join(output_path, \"text8.zip\")\n",
        "    \n",
        "    print(f\"Downloading text8 from {url}...\")\n",
        "    with requests.get(url, stream=True) as response:\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        \n",
        "        with open(zip_path, 'wb') as f:\n",
        "            with tqdm.tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "                        pbar.update(len(chunk))\n",
        "    \n",
        "    print(f\"Extracting {zip_path}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_path)\n",
        "    \n",
        "    # Remove zip file to save space\n",
        "    os.remove(zip_path)\n",
        "    \n",
        "    print(f\"Text8 dataset ready at: {text8_file}\")\n",
        "    return text8_file\n",
        "\n",
        "\n",
        "def preprocess_wmt14_news(news_file_path: str, output_dir: str, words_per_sentence: int = 1000, \n",
        "                        max_sentences: int = None, max_files: int = None, use_phrases: bool = False,\n",
        "                        phrase_threshold1: float = 200.0, phrase_threshold2: float = 100.0) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess WMT14 news file into sentence files\n",
        "    \n",
        "    Args:\n",
        "        news_file_path: Path to WMT14 news file\n",
        "        output_dir: Output directory for processed files\n",
        "        words_per_sentence: Number of words per sentence (default: 1000)\n",
        "        max_sentences: Maximum number of sentences to process (None = all)\n",
        "        max_files: Maximum number of files to create (None = all)\n",
        "        use_phrases: Whether to apply phrase detection (default: False)\n",
        "        phrase_threshold1: First pass phrase threshold (default: 200.0)\n",
        "        phrase_threshold2: Second pass phrase threshold (default: 100.0)\n",
        "    \n",
        "    Returns:\n",
        "        Path to output directory\n",
        "    \"\"\"\n",
        "    print(f\"Preprocessing WMT14 news file: {news_file_path}\")\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "    print(f\"Words per sentence: {words_per_sentence}\")\n",
        "    print(\"Note: Punctuation will be removed from text (commas, periods, etc.)\")\n",
        "    if max_sentences:\n",
        "        print(f\"Max sentences: {max_sentences:,}\")\n",
        "    if max_files:\n",
        "        print(f\"Max files: {max_files}\")\n",
        "    if use_phrases:\n",
        "        print(f\"Phrase detection: Enabled (threshold1={phrase_threshold1}, threshold2={phrase_threshold2})\")\n",
        "    \n",
        "    # Create output directory\n",
        "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Check if already processed\n",
        "    existing_files = [f for f in os.listdir(output_dir) if f.startswith(\"0\")]\n",
        "    if existing_files:\n",
        "        print(f\"Found {len(existing_files)} existing processed files. Skipping preprocessing.\")\n",
        "        print(\"⚠️ WARNING: If these files contain punctuation, delete them and reprocess to apply cleaning.\")\n",
        "        return output_dir\n",
        "    \n",
        "    # Step 1: Basic preprocessing\n",
        "    temp_dir = output_dir + \"_temp\"\n",
        "    pathlib.Path(temp_dir).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Read news file (one sentence per line)\n",
        "    sentences = []\n",
        "    sentence_count = 0\n",
        "    \n",
        "    with open(news_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            # Clean text: remove punctuation and normalize\n",
        "            cleaned_line = clean_text_remove_punctuation(line)\n",
        "            if cleaned_line:  # Skip empty lines after cleaning\n",
        "                # Split into words and group into chunks\n",
        "                words = cleaned_line.split()\n",
        "                for i in range(0, len(words), words_per_sentence):\n",
        "                    sentence_words = words[i:i + words_per_sentence]\n",
        "                    if len(sentence_words) >= 2:  # Skip very short sentences\n",
        "                        sentences.append(\" \".join(sentence_words))\n",
        "                        sentence_count += 1\n",
        "                        \n",
        "                        # Stop if we've reached max_sentences\n",
        "                        if max_sentences and sentence_count >= max_sentences:\n",
        "                            print(f\"Reached max_sentences limit: {max_sentences:,}\")\n",
        "                            break\n",
        "                \n",
        "                # Break outer loop if we've reached max_sentences\n",
        "                if max_sentences and sentence_count >= max_sentences:\n",
        "                    break\n",
        "    \n",
        "    print(f\"Total sentences: {len(sentences):,}\")\n",
        "    \n",
        "    # Save to temporary files\n",
        "    sentences_per_file = 100000\n",
        "    file_count = 0\n",
        "    current_file_sentences = []\n",
        "    \n",
        "    for i, sentence in enumerate(sentences):\n",
        "        current_file_sentences.append(sentence)\n",
        "        \n",
        "        # Write file when it reaches sentences_per_file or we're at the end\n",
        "        if len(current_file_sentences) >= sentences_per_file or i == len(sentences) - 1:\n",
        "            filename = f\"{file_count:04d}\"\n",
        "            filepath = os.path.join(temp_dir, filename)\n",
        "            \n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                for sent in current_file_sentences:\n",
        "                    f.write(sent + '\\n')\n",
        "            \n",
        "            print(f\"Wrote {len(current_file_sentences):,} sentences to {filename}\")\n",
        "            file_count += 1\n",
        "            current_file_sentences = []\n",
        "            \n",
        "            # Stop if we've reached max_files\n",
        "            if max_files and file_count >= max_files:\n",
        "                print(f\"Reached max_files limit: {max_files}\")\n",
        "                break\n",
        "    \n",
        "    # Step 2: Apply phrase detection if enabled\n",
        "    if use_phrases:\n",
        "        print(\"\\nApplying phrase detection...\")\n",
        "        preprocess_with_phrases(temp_dir, output_dir, min_count=5, \n",
        "                               threshold1=phrase_threshold1, threshold2=phrase_threshold2)\n",
        "        # Cleanup temp directory\n",
        "        import shutil\n",
        "        if os.path.exists(temp_dir):\n",
        "            shutil.rmtree(temp_dir)\n",
        "    else:\n",
        "        # Just move files from temp to output\n",
        "        import shutil\n",
        "        if os.path.exists(output_dir):\n",
        "            shutil.rmtree(output_dir)\n",
        "        shutil.move(temp_dir, output_dir)\n",
        "    \n",
        "    print(f\"Preprocessing complete. Created {file_count} files in {output_dir}\")\n",
        "    return output_dir\n",
        "\n",
        "\n",
        "def preprocess_text8(text8_file_path: str, output_dir: str, words_per_sentence: int = 1000,\n",
        "                    use_phrases: bool = False, phrase_threshold1: float = 200.0, \n",
        "                    phrase_threshold2: float = 100.0) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess text8 file into sentence files.\n",
        "    \n",
        "    Args:\n",
        "        text8_file_path: Path to text8 file\n",
        "        output_dir: Output directory for processed files\n",
        "        words_per_sentence: Number of words per sentence (default: 1000)\n",
        "        use_phrases: Whether to apply phrase detection (default: False)\n",
        "        phrase_threshold1: First pass phrase threshold (default: 200.0)\n",
        "        phrase_threshold2: Second pass phrase threshold (default: 100.0)\n",
        "    \n",
        "    Returns:\n",
        "        Path to output directory\n",
        "    \"\"\"\n",
        "    print(f\"Preprocessing text8 file: {text8_file_path}\")\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "    print(f\"Words per sentence: {words_per_sentence}\")\n",
        "    if use_phrases:\n",
        "        print(f\"Phrase detection: Enabled (threshold1={phrase_threshold1}, threshold2={phrase_threshold2})\")\n",
        "    \n",
        "    # Create output directory\n",
        "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Check if already processed\n",
        "    existing_files = [f for f in os.listdir(output_dir) if f.startswith(\"0\")]\n",
        "    if existing_files:\n",
        "        print(f\"Found {len(existing_files)} existing processed files. Skipping preprocessing.\")\n",
        "        return output_dir\n",
        "    \n",
        "    # Step 1: Basic preprocessing\n",
        "    temp_dir = output_dir + \"_temp\"\n",
        "    pathlib.Path(temp_dir).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Read text8 file (single long line)\n",
        "    with open(text8_file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read().strip()\n",
        "    \n",
        "    # Split into words\n",
        "    words = text.split()\n",
        "    print(f\"Total words: {len(words):,}\")\n",
        "    \n",
        "    # Group into sentences\n",
        "    sentences = []\n",
        "    for i in range(0, len(words), words_per_sentence):\n",
        "        sentence_words = words[i:i + words_per_sentence]\n",
        "        if len(sentence_words) >= 2:  # Skip very short sentences\n",
        "            sentences.append(\" \".join(sentence_words))\n",
        "    \n",
        "    print(f\"Created {len(sentences):,} sentences\")\n",
        "    \n",
        "    # Save to temporary files (similar to myw2v format)\n",
        "    sentences_per_file = 100000\n",
        "    file_count = 0\n",
        "    current_file_sentences = []\n",
        "    \n",
        "    for i, sentence in enumerate(sentences):\n",
        "        current_file_sentences.append(sentence)\n",
        "        \n",
        "        # Write file when it reaches sentences_per_file or we're at the end\n",
        "        if len(current_file_sentences) >= sentences_per_file or i == len(sentences) - 1:\n",
        "            filename = f\"{file_count:04d}\"\n",
        "            filepath = os.path.join(temp_dir, filename)\n",
        "            \n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                for sent in current_file_sentences:\n",
        "                    f.write(sent + '\\n')\n",
        "            \n",
        "            print(f\"Wrote {len(current_file_sentences):,} sentences to {filename}\")\n",
        "            file_count += 1\n",
        "            current_file_sentences = []\n",
        "    \n",
        "    # Step 2: Apply phrase detection if enabled\n",
        "    if use_phrases:\n",
        "        print(\"\\nApplying phrase detection...\")\n",
        "        preprocess_with_phrases(temp_dir, output_dir, min_count=5, \n",
        "                               threshold1=phrase_threshold1, threshold2=phrase_threshold2)\n",
        "        # Cleanup temp directory\n",
        "        import shutil\n",
        "        if os.path.exists(temp_dir):\n",
        "            shutil.rmtree(temp_dir)\n",
        "    else:\n",
        "        # Just move files from temp to output\n",
        "        import shutil\n",
        "        if os.path.exists(output_dir):\n",
        "            shutil.rmtree(output_dir)\n",
        "        shutil.move(temp_dir, output_dir)\n",
        "    \n",
        "    print(f\"Preprocessing complete. Created {file_count} files in {output_dir}\")\n",
        "    return output_dir\n",
        "\n",
        "\n",
        "def get_data_file_names(path: str, seed: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Get shuffled list of data file names\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    qq = [fn for fn in os.listdir(path) if fn.startswith(\"0\")]\n",
        "    # Sort first to ensure consistent shuffling\n",
        "    data_files = sorted(qq)\n",
        "    rng.shuffle(data_files)\n",
        "    return data_files\n",
        "\n",
        "\n",
        "def read_all_data_files(data_path: str, file_names: List[str], word_to_idx: dict) -> Tuple[List[int], List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Read all data files and convert words to indices\n",
        "    Returns (inputs, offsets, lengths)\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "    \n",
        "    start = time.time()\n",
        "    inps, offs, lens = [], [], []\n",
        "    offset_total = 0\n",
        "    stats = defaultdict(int)\n",
        "    \n",
        "    for fn in file_names:\n",
        "        fp = os.path.join(data_path, fn)\n",
        "        ok_lines = 0\n",
        "        too_short_lines = 0\n",
        "        with open(fp, encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                words = [word for word in re.split(r\"[ .]+\", line.strip()) if word]\n",
        "                if len(words) < 2:\n",
        "                    too_short_lines += 1\n",
        "                    continue\n",
        "                idcs = [word_to_idx[w] for w in words if w in word_to_idx]\n",
        "                le = len(idcs)\n",
        "                ok_lines += 1\n",
        "                offs.append(offset_total)\n",
        "                lens.append(le)\n",
        "                inps.extend(idcs)\n",
        "                offset_total += le\n",
        "        stats[\"file_read_lines_ok\"] += ok_lines\n",
        "        stats[\"one_word_sentence_lines_which_were_ignored\"] += too_short_lines\n",
        "\n",
        "    print(f\"read_all_data_files() STATS: {stats}\")\n",
        "    tot_tm = time.time()-start\n",
        "    print(f\"read_all_data_files() Total time {tot_tm} s for {len(file_names)} files (avg {tot_tm/len(file_names)} s/file)\")\n",
        "    return inps, offs, lens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy1oEFb5mTRG"
      },
      "source": [
        "# **Skip-gram Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNat0U4dmWye"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import time\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "from numba import cuda\n",
        "from numba.cuda import random as c_random\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "\n",
        "@cuda.jit\n",
        "def calc_skipgram(\n",
        "        rows: int,\n",
        "        c: int,\n",
        "        k: int,\n",
        "        learning_rate: float,\n",
        "        w1,\n",
        "        w2,\n",
        "        calc_aux,\n",
        "        random_states,\n",
        "        subsample_weights,\n",
        "        negsample_array,\n",
        "        inp,\n",
        "        offsets,\n",
        "        lengths,\n",
        "        use_hs,\n",
        "        syn1,\n",
        "        codes_array,\n",
        "        points_array,\n",
        "        code_lengths,\n",
        "        exp_table,\n",
        "        exp_table_size,\n",
        "        max_exp):\n",
        "    \"\"\"\n",
        "    CUDA kernel for Skip-gram training\n",
        "    Based on word2vec.c Skip-gram implementation from the original source code of the Word2Vec paper\n",
        "    Supports both Hierarchical Softmax and Negative Sampling\n",
        "    \"\"\"\n",
        "    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    if idx >= rows:\n",
        "        return\n",
        "    le = lengths[idx]\n",
        "    off = offsets[idx]\n",
        "    \n",
        "    for centre in range(0, le):\n",
        "        word_idx = inp[off + centre]\n",
        "        prob_to_reject = subsample_weights[word_idx]\n",
        "        rnd = c_random.xoroshiro128p_uniform_float32(random_states, idx)\n",
        "        \n",
        "        if rnd > prob_to_reject:\n",
        "            r_f = c_random.xoroshiro128p_uniform_float32(random_states, idx)\n",
        "            r: int = math.ceil(r_f * c)\n",
        "            \n",
        "            # Context before center word\n",
        "            for context_pre in range(max(0, centre-r), centre):\n",
        "                step_skipgram(idx, w1, w2, calc_aux, inp[off+centre], inp[off+context_pre], \n",
        "                             k, learning_rate, negsample_array, random_states,\n",
        "                             use_hs, syn1, codes_array, points_array, code_lengths,\n",
        "                             exp_table, exp_table_size, max_exp)\n",
        "            \n",
        "            # Context after center word\n",
        "            for context_post in range(centre + 1, min(le, centre + 1 + r)):\n",
        "                step_skipgram(idx, w1, w2, calc_aux, inp[off+centre], inp[off+context_post], \n",
        "                             k, learning_rate, negsample_array, random_states,\n",
        "                             use_hs, syn1, codes_array, points_array, code_lengths,\n",
        "                             exp_table, exp_table_size, max_exp)\n",
        "\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def fast_sigmoid(f, exp_table, exp_table_size, max_exp):\n",
        "    \"\"\"\n",
        "    Fast sigmoid using precomputed exp table\n",
        "    Based on word2vec.c exp table lookup from the original source code of the Word2Vec paper\n",
        "    \"\"\"\n",
        "    if f <= -max_exp:\n",
        "        return 0.0\n",
        "    elif f >= max_exp:\n",
        "        return 1.0\n",
        "    else:\n",
        "        idx = int((f + max_exp) * (exp_table_size / max_exp / 2.0))\n",
        "        if idx < 0:\n",
        "            idx = 0\n",
        "        if idx >= exp_table_size:\n",
        "            idx = exp_table_size - 1\n",
        "        return exp_table[idx]\n",
        "\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def step_skipgram(thread_idx, w1, w2, calc_aux, x, y, k, learning_rate, negsample_array, random_states,\n",
        "                  use_hs, syn1, codes_array, points_array, code_lengths,\n",
        "                  exp_table, exp_table_size, max_exp):\n",
        "    \"\"\"\n",
        "    Device function for Skip-gram gradient calculation\n",
        "    Based on word2vec.c Skip-gram implementation from the original source code of the Word2Vec paper\n",
        "    Supports both Hierarchical Softmax and Negative Sampling\n",
        "    \"\"\"\n",
        "    emb_dim = w1.shape[1]\n",
        "    negs_arr_len = len(negsample_array)\n",
        "    \n",
        "    # Initialize error accumulator\n",
        "    for i in range(emb_dim):\n",
        "        calc_aux[thread_idx, i] = 0.0\n",
        "    \n",
        "    # Hierarchical Softmax (if enabled) - traverse tree for context word y\n",
        "    if use_hs:\n",
        "        codelen = code_lengths[y]\n",
        "        max_code_len = codes_array.shape[1]\n",
        "        for d in range(codelen):\n",
        "            if d >= max_code_len:\n",
        "                break\n",
        "            node_idx = points_array[y, d]\n",
        "            if node_idx < 0:\n",
        "                continue\n",
        "            \n",
        "            # Calculate dot product: w1[x] • syn1[node]\n",
        "            f = 0.0\n",
        "            for i in range(emb_dim):\n",
        "                f += w1[x, i] * syn1[node_idx, i]\n",
        "            \n",
        "            # Early skip if f is outside range (same as original code)\n",
        "            # This prevents unnecessary updates when sigmoid is saturated\n",
        "            if f <= -max_exp:\n",
        "                continue\n",
        "            if f >= max_exp:\n",
        "                continue\n",
        "            \n",
        "            # Get sigmoid from exp table (only if in range)\n",
        "            sigmoid_val = fast_sigmoid(f, exp_table, exp_table_size, max_exp)\n",
        "            \n",
        "            # Get code bit (0 or 1)\n",
        "            code_bit = codes_array[y, d]\n",
        "            if code_bit < 0:\n",
        "                continue\n",
        "            \n",
        "            # Calculate gradient: g = (1 - code_bit - sigmoid) * learning_rate\n",
        "            g = (1.0 - float(code_bit) - sigmoid_val) * learning_rate\n",
        "            \n",
        "            # Propagate errors output -> hidden\n",
        "            for i in range(emb_dim):\n",
        "                calc_aux[thread_idx, i] += g * syn1[node_idx, i]\n",
        "            \n",
        "            # Learn weights hidden -> output\n",
        "            for i in range(emb_dim):\n",
        "                syn1[node_idx, i] += g * w1[x, i]\n",
        "    \n",
        "    # Negative Sampling (if enabled)\n",
        "    if k > 0:\n",
        "        # Positive sample: predict context word y\n",
        "        dot_xy = 0.0\n",
        "        for i in range(emb_dim):\n",
        "            dot_xy += w1[x, i] * w2[y, i]\n",
        "        s_xdy_m1 = fast_sigmoid(dot_xy, exp_table, exp_table_size, max_exp) - 1.0\n",
        "        \n",
        "        # Positive sample gradients\n",
        "        for i in range(emb_dim):\n",
        "            calc_aux[thread_idx, i] += -learning_rate * s_xdy_m1 * w2[y, i]\n",
        "            w2[y, i] -= learning_rate * s_xdy_m1 * w1[x, i]\n",
        "        \n",
        "        # Negative samples\n",
        "        for neg_sample in range(0, k):\n",
        "            rnd = c_random.xoroshiro128p_uniform_float32(random_states, thread_idx)\n",
        "            q_idx: int = int(math.floor(negs_arr_len * rnd))\n",
        "            neg = negsample_array[q_idx]\n",
        "            dot_xq = 0.0\n",
        "            for i in range(emb_dim):\n",
        "                dot_xq += w1[x, i] * w2[neg, i]\n",
        "            s_dxq = fast_sigmoid(dot_xq, exp_table, exp_table_size, max_exp)\n",
        "            \n",
        "            # Negative sample gradients\n",
        "            for i in range(emb_dim):\n",
        "                calc_aux[thread_idx, i] -= learning_rate * s_dxq * w2[neg, i]\n",
        "                w2[neg, i] -= learning_rate * s_dxq * w1[x, i]\n",
        "    \n",
        "    # Note: Original code does NOT use gradient clipping, only early skip\n",
        "    # Gradient clipping may reduce training effectiveness\n",
        "    # Update center word vector (same as original code)\n",
        "    for i in range(emb_dim):\n",
        "        w1[x, i] += calc_aux[thread_idx, i]\n",
        "\n",
        "\n",
        "def train_skipgram(\n",
        "        data_path: str,\n",
        "        out_file_path: str,\n",
        "        epochs: int,\n",
        "        embed_dim: int = 100,\n",
        "        min_occurs: int = 3,\n",
        "        c: int = 5,\n",
        "        k: int = 5,\n",
        "        t: float = 1e-5,\n",
        "        vocab_freq_exponent: float = 0.75,\n",
        "        lr_max: float = 0.025,\n",
        "        lr_min: float = 0.0025,\n",
        "        cuda_threads_per_block: int = 32,\n",
        "        hs: int = 0,\n",
        "        max_memory_gb: float = 70.0,\n",
        "        max_words: int = None,\n",
        "        vocab: list = None,\n",
        "        w_to_i: dict = None,\n",
        "        word_counts: list = None,\n",
        "        ssw: np.ndarray = None,\n",
        "        negs: np.ndarray = None):\n",
        "    \"\"\"\n",
        "    Train Skip-gram model\n",
        "    Based on word2vec.c Skip-gram implementation from the original source code of the Word2Vec paper\n",
        "    \n",
        "    Args:\n",
        "        hs: Hierarchical Softmax flag (0=NS only, 1=HS only). Cannot combine with k>0\n",
        "        k: Negative sampling count (0=HS only, >0=NS only). Cannot combine with hs=1\n",
        "        max_memory_gb: Maximum GPU memory usage in GB. If estimated memory exceeds this,\n",
        "                       the dataset will be automatically split into batches for processing\n",
        "                       Default: 70.0 GB (safe for A100 80GB GPU)\n",
        "    \n",
        "    Raises:\n",
        "        ValueError: If both hs=1 and k>0 are specified (HS and NS cannot be combined)\n",
        "    \"\"\"\n",
        "    # Validate: HS and NS cannot be used together\n",
        "    if hs == 1 and k > 0:\n",
        "        raise ValueError(\n",
        "            \"Error: Cannot use HS (hs=1) and Negative Sampling (k>0) together. \"\n",
        "            \"Please choose either HS only (hs=1, k=0) or NS only (hs=0, k>0)\"\n",
        "        )\n",
        "    \n",
        "    params = {\n",
        "        \"model_type\": \"skipgram\",\n",
        "        \"w2v_version\": W2V_VERSION,\n",
        "        \"data_path\": data_path,\n",
        "        \"out_file_path\": out_file_path,\n",
        "        \"epochs\": epochs,\n",
        "        \"embed_dim\": embed_dim,\n",
        "        \"min_occurs\": min_occurs,\n",
        "        \"c\": c,\n",
        "        \"k\": k,\n",
        "        \"t\": t,\n",
        "        \"vocab_freq_exponent\": vocab_freq_exponent,\n",
        "        \"lr_max\": lr_max,\n",
        "        \"lr_min\": lr_min,\n",
        "        \"cuda_threads_per_block\": cuda_threads_per_block,\n",
        "        \"hs\": hs\n",
        "    }\n",
        "    stats = {}\n",
        "    params_path = out_file_path + \"_params.json\"\n",
        "    stats_path = out_file_path + \"_stats.json\"\n",
        "\n",
        "    seed = 12345\n",
        "    \n",
        "    # Adjust learning rate based on training method\n",
        "    original_lr_max = lr_max\n",
        "    original_lr_min = lr_min\n",
        "    \n",
        "    # Learning rate handling: HS only and NS only use the same learning rate \n",
        "    # (as per word2vec.c original implementation)\n",
        "    # No special adjustment needed for either method\n",
        "    \n",
        "    # Learning rate schedule\n",
        "    # For multiple epochs: decrease between epochs\n",
        "    # For all epochs: decrease linearly within epoch (as per word2vec.c)\n",
        "    if epochs > 1:\n",
        "        lr_step = (lr_max - lr_min) / (epochs - 1)\n",
        "    else:\n",
        "        lr_step = 0.0  # Not used for single epoch (LR decays within epoch)\n",
        "\n",
        "    print(f\"Skip-gram Training Parameters:\")\n",
        "    print(f\"Seed: {seed}\")\n",
        "    print(f\"Window size: {c}\")\n",
        "    if hs == 1:\n",
        "        print(f\"Hierarchical Softmax: Enabled\")\n",
        "    if k > 0:\n",
        "        print(f\"Negative samples: {k}\")\n",
        "    if original_lr_max != lr_max:\n",
        "        print(f\"Learning rate adjusted: {original_lr_max} -> {lr_max} (reduced for stability)\")\n",
        "    if epochs == 1:\n",
        "        print(f\"Learning rate: {lr_max} -> ~0 (will decrease linearly within epoch, as per word2vec.c)\")\n",
        "    else:\n",
        "        print(f\"Learning rate: {lr_max} -> {lr_min} (step: {lr_step:.6f} between epochs, also decreases linearly within each epoch)\")\n",
        "    print(f\"Embedding dimension: {embed_dim}\")\n",
        "    print(f\"Min word count: {min_occurs}\")\n",
        "\n",
        "    # Start timing for total execution\n",
        "    start = time.time()\n",
        "\n",
        "    # Build vocabulary if not provided (for reuse when training both models)\n",
        "    if vocab is None or w_to_i is None or word_counts is None:\n",
        "        print(f\"\\nBuilding vocabulary from: {data_path}\")\n",
        "        vocab_start = time.time()\n",
        "        vocab, w_to_i, word_counts = handle_vocab(data_path, min_occurs, freq_exponent=vocab_freq_exponent, use_cache=True)\n",
        "        vocab_size = len(vocab)\n",
        "        build_time = time.time() - vocab_start\n",
        "        print(f\"Vocabulary {'loaded from cache' if build_time < 1.0 else 'built'} in {build_time:.2f}s. Vocab size: {vocab_size:,}\")\n",
        "    else:\n",
        "        vocab_size = len(vocab)\n",
        "        print(f\"\\nUsing pre-built vocabulary. Vocab size: {vocab_size:,}\")\n",
        "    \n",
        "    # Build subsampling weights and negative sampling array if not provided\n",
        "    if ssw is None or negs is None:\n",
        "        ssw, negs = get_subsampling_weights_and_negative_sampling_array(vocab, t=t)\n",
        "    \n",
        "    # Create exp table\n",
        "    print(\"Creating exp table for fast sigmoid...\")\n",
        "    exp_table = create_exp_table(EXP_TABLE_SIZE, MAX_EXP)\n",
        "    \n",
        "    # Setup Hierarchical Softmax if enabled\n",
        "    use_hs = (hs == 1)\n",
        "    syn1_cuda = None\n",
        "    codes_array_cuda = None\n",
        "    points_array_cuda = None\n",
        "    code_lengths_cuda = None\n",
        "    \n",
        "    if use_hs:\n",
        "        print(\"Creating Huffman tree for Hierarchical Softmax...\")\n",
        "        hs_start = time.time()\n",
        "        codes_array, points_array, code_lengths = create_huffman_tree(word_counts, MAX_CODE_LENGTH)\n",
        "        syn1 = init_hs_weight_matrix(vocab_size, embed_dim)\n",
        "        print(f\"Huffman tree created in {time.time() - hs_start:.2f}s\")\n",
        "        print(f\" -Codes array shape: {codes_array.shape}\")\n",
        "        print(f\" -Points array shape: {points_array.shape}\")\n",
        "        print(f\" -Syn1 matrix shape: {syn1.shape}\")\n",
        "\n",
        "    data_files = get_data_file_names(data_path, seed=seed)\n",
        "    print(f\"Processing {len(data_files)} data files...\")\n",
        "    if max_words is not None:\n",
        "        print(f\"⚠️ Limiting to {max_words:,} total words (will stop early if reached)\")\n",
        "    inps_, offs_, lens_ = read_all_data_files_ever(data_path, data_files, w_to_i, max_words=max_words)\n",
        "    inps, offs, lens = (np.asarray(inps_, dtype=np.int32), \n",
        "                       np.asarray(offs_, dtype=np.int32), \n",
        "                       np.asarray(lens_, dtype=np.int32))\n",
        "    sentence_count = len(lens)\n",
        "    total_words = len(inps)  # Total words for LR decay calculation\n",
        "    \n",
        "    print(f\"Data loaded: {sentence_count:,} sentences, {total_words:,} total words\")\n",
        "\n",
        "    # Initialize weight matrices\n",
        "    data_init_start = time.time()\n",
        "    w1, w2 = init_weight_matrices(vocab_size, embed_dim, seed=seed)\n",
        "    data_size_weights = 4 * (w1.size + w2.size)\n",
        "    data_size_inputs = 4 * (inps.size + offs.size + lens.size + ssw.size + negs.size)\n",
        "    \n",
        "    # Calculate memory usage and determine batch size\n",
        "    weights_gb = data_size_weights / (1024**3)\n",
        "    inputs_gb = data_size_inputs / (1024**3)\n",
        "    \n",
        "    # Estimate calc_aux memory for full dataset\n",
        "    calc_aux_size_full = sentence_count * embed_dim * 4\n",
        "    calc_aux_gb_full = calc_aux_size_full / (1024**3)\n",
        "    total_memory_gb = weights_gb + inputs_gb + calc_aux_gb_full\n",
        "    \n",
        "    # Determine if batch processing is needed\n",
        "    use_batch_processing = (total_memory_gb > max_memory_gb)\n",
        "    \n",
        "    if use_batch_processing:\n",
        "        # Calculate batch size based on available memory\n",
        "        available_memory_gb = max_memory_gb - weights_gb - inputs_gb\n",
        "        # Reserve 5GB for overhead\n",
        "        available_memory_gb = max(1.0, available_memory_gb - 5.0)\n",
        "        \n",
        "        # Calculate max sentences per batch\n",
        "        bytes_per_sentence = embed_dim * 4  # float32\n",
        "        max_batch_sentences = int((available_memory_gb * 1024**3) / bytes_per_sentence)\n",
        "        \n",
        "        if max_batch_sentences >= 10_000_000:\n",
        "            batch_size = 10_000_000\n",
        "        elif max_batch_sentences >= 5_000_000:\n",
        "            batch_size = 5_000_000\n",
        "        elif max_batch_sentences >= 2_000_000:\n",
        "            batch_size = 2_000_000\n",
        "        elif max_batch_sentences >= 1_000_000:\n",
        "            batch_size = 1_000_000\n",
        "        else:\n",
        "            batch_size = max(100_000, max_batch_sentences)\n",
        "        \n",
        "        num_batches = math.ceil(sentence_count / batch_size)\n",
        "        batch_aux_gb = (batch_size * embed_dim * 4) / (1024**3)\n",
        "        batch_total_gb = weights_gb + inputs_gb + batch_aux_gb\n",
        "        \n",
        "        print(f\"\\n⚠️ Memory usage would be {total_memory_gb:.1f} GB (exceeds {max_memory_gb} GB limit)\")\n",
        "        print(f\"Using batch processing: {num_batches} batches, {batch_size:,} sentences/batch\")\n",
        "        print(f\"Memory per batch: {batch_total_gb:.1f} GB (calc_aux: {batch_aux_gb:.1f} GB)\")\n",
        "    else:\n",
        "        batch_size = sentence_count\n",
        "        num_batches = 1\n",
        "        print(f\"\\n✅ Memory usage: {total_memory_gb:.1f} GB (within {max_memory_gb} GB limit)\")\n",
        "        print(f\"Processing all {sentence_count:,} sentences in one batch\")\n",
        "    \n",
        "    blocks: int = math.ceil(batch_size / cuda_threads_per_block)\n",
        "    print(f\"CUDA config: {cuda_threads_per_block} threads/block, {blocks} blocks per batch\")\n",
        "\n",
        "    # Transfer to GPU - Transfer weights and vocab arrays (these are shared across batches)\n",
        "    print(\"Transferring data to GPU...\")\n",
        "    data_transfer_start = time.time()\n",
        "    ssw_cuda, negs_cuda = cuda.to_device(ssw), cuda.to_device(negs)\n",
        "    w1_cuda, w2_cuda = cuda.to_device(w1), cuda.to_device(w2)\n",
        "    exp_table_cuda = cuda.to_device(exp_table)\n",
        "    \n",
        "    # Keep input arrays on CPU - will slice and transfer per batch\n",
        "    # This saves GPU memory\n",
        "    \n",
        "    if use_hs:\n",
        "        syn1_cuda = cuda.to_device(syn1)\n",
        "        codes_array_cuda = cuda.to_device(codes_array)\n",
        "        points_array_cuda = cuda.to_device(points_array)\n",
        "        code_lengths_cuda = cuda.to_device(code_lengths)\n",
        "    \n",
        "    print(f\"Data transfer completed in {time.time()-data_transfer_start:.2f}s\")\n",
        "\n",
        "    stats[\"sentence_count\"] = len(lens)\n",
        "    stats[\"word_count\"] = len(inps)\n",
        "    stats[\"vocab_size\"] = vocab_size\n",
        "    stats[\"approx_data_size_weights\"] = data_size_weights\n",
        "    stats[\"approx_data_size_inputs\"] = data_size_inputs\n",
        "    stats[\"use_batch_processing\"] = use_batch_processing\n",
        "    if use_batch_processing:\n",
        "        stats[\"batch_size\"] = batch_size\n",
        "        stats[\"num_batches\"] = num_batches\n",
        "        batch_aux_size = batch_size * embed_dim * 4\n",
        "        stats[\"approx_data_size_aux_per_batch\"] = batch_aux_size\n",
        "        stats[\"approx_data_size_total\"] = data_size_weights + data_size_inputs + batch_aux_size\n",
        "    else:\n",
        "        data_size_aux = 4 * (sentence_count * embed_dim)\n",
        "        stats[\"approx_data_size_aux\"] = data_size_aux\n",
        "        stats[\"approx_data_size_total\"] = data_size_weights + data_size_inputs + data_size_aux\n",
        "\n",
        "    # Prepare HS parameters (use dummy arrays if HS disabled)\n",
        "    if not use_hs:\n",
        "        # Create dummy arrays for HS (will not be used, but needed for kernel signature)\n",
        "        dummy_syn1 = cuda.device_array((1, embed_dim), dtype=np.float32)\n",
        "        dummy_codes = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)\n",
        "        dummy_points = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)\n",
        "        dummy_lengths = cuda.device_array(vocab_size, dtype=np.int32)\n",
        "        syn1_param = dummy_syn1\n",
        "        codes_param = dummy_codes\n",
        "        points_param = dummy_points\n",
        "        lengths_param = dummy_lengths\n",
        "    else:\n",
        "        syn1_param = syn1_cuda\n",
        "        codes_param = codes_array_cuda\n",
        "        points_param = points_array_cuda\n",
        "        lengths_param = code_lengths_cuda\n",
        "    \n",
        "    print_norms(w1_cuda)\n",
        "    print(f\"\\nStarting Skip-gram training - {epochs} epochs...\")\n",
        "    epoch_times = []\n",
        "    calc_start = time.time()\n",
        "    \n",
        "    # Track total words processed across all epochs (as per word2vec.c)\n",
        "    # Learning rate decays based on total words processed, not per epoch\n",
        "    # Use int64 to avoid overflow with large datasets and multiple epochs\n",
        "    words_processed_total = np.int64(0)\n",
        "    total_words_for_training = np.int64(epochs) * np.int64(total_words)\n",
        "    \n",
        "    for epoch in range(0, epochs):\n",
        "        epoch_start = time.time()\n",
        "        \n",
        "        # Process each batch\n",
        "        for batch_idx in range(num_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = min((batch_idx + 1) * batch_size, sentence_count)\n",
        "            batch_sentence_count = batch_end - batch_start\n",
        "            \n",
        "            if num_batches > 1:\n",
        "                print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{num_batches}: sentences {batch_start:,}-{batch_end:,}\")\n",
        "            \n",
        "            # Calculate word offset for this batch (offsets are cumulative)\n",
        "            batch_word_start = offs[batch_start] if batch_start < len(offs) else 0\n",
        "            batch_word_end = offs[batch_end] if batch_end < len(offs) else len(inps)\n",
        "            batch_word_count = batch_word_end - batch_word_start\n",
        "            \n",
        "            # Calculate learning rate for this batch (linear decay as per word2vec.c)\n",
        "            # Formula from word2vec.c: alpha = starting_alpha * (1 - word_count_actual / (iter * train_words + 1))\n",
        "            # word_count_actual is total words processed across all epochs\n",
        "            # This ensures LR decreases linearly from lr_max to ~0 over entire training\n",
        "            denominator = total_words_for_training + 1\n",
        "            current_lr = lr_max * (1.0 - words_processed_total / denominator) if denominator > 0 else lr_max\n",
        "            \n",
        "            # Apply minimum threshold (as per word2vec.c: min = starting_alpha * 0.0001)\n",
        "            min_lr_threshold = lr_max * 0.0001\n",
        "            current_lr = max(current_lr, min_lr_threshold)\n",
        "            \n",
        "            # Also apply lr_min as additional constraint (for multi-epoch training)\n",
        "            if epochs > 1:\n",
        "                current_lr = max(current_lr, lr_min)\n",
        "            \n",
        "            if num_batches > 1 and batch_idx == 0:\n",
        "                print(f\"    Learning rate: {current_lr:.6f} (decaying linearly, progress: {words_processed_total/total_words_for_training*100:.1f}%)\")\n",
        "            \n",
        "            # Create batch arrays (slicing from CPU arrays)\n",
        "            batch_lens = lens[batch_start:batch_end]\n",
        "            batch_offs_local = offs[batch_start:batch_end] - batch_word_start  # Adjust offsets to start from 0\n",
        "            batch_inps_local = inps[batch_word_start:batch_word_end]\n",
        "            \n",
        "            # Transfer batch arrays to GPU\n",
        "            batch_lens_cuda = cuda.to_device(batch_lens)\n",
        "            batch_offs_cuda = cuda.to_device(batch_offs_local)\n",
        "            batch_inps_cuda = cuda.to_device(batch_inps_local)\n",
        "            \n",
        "            # Create calc_aux for this batch\n",
        "            batch_calc_aux = np.zeros((batch_sentence_count, embed_dim), dtype=np.float32)\n",
        "            batch_calc_aux_cuda = cuda.to_device(batch_calc_aux)\n",
        "            \n",
        "            # Create random states for this batch\n",
        "            batch_random_states_cuda = c_random.create_xoroshiro128p_states(\n",
        "                batch_sentence_count, seed=seed + epoch * 10000 + batch_idx * 100\n",
        "            )\n",
        "            \n",
        "            # Launch CUDA kernel for this batch with current learning rate\n",
        "            batch_blocks = math.ceil(batch_sentence_count / cuda_threads_per_block)\n",
        "            calc_skipgram[batch_blocks, cuda_threads_per_block](\n",
        "                batch_sentence_count, c, k, current_lr, w1_cuda, w2_cuda, batch_calc_aux_cuda, \n",
        "                batch_random_states_cuda, ssw_cuda, negs_cuda, batch_inps_cuda, \n",
        "                batch_offs_cuda, batch_lens_cuda,\n",
        "                use_hs, syn1_param, codes_param, points_param, lengths_param,\n",
        "                exp_table_cuda, EXP_TABLE_SIZE, MAX_EXP)\n",
        "            \n",
        "            # Update total words processed counter (as per word2vec.c)\n",
        "            # Note: Actual words processed may vary due to subsampling, but this is an approximation\n",
        "            # Use int64 to avoid overflow with large datasets and multiple epochs\n",
        "            words_processed_total = np.int64(words_processed_total) + np.int64(batch_word_count)\n",
        "            \n",
        "            # Free batch arrays from GPU memory\n",
        "            del batch_lens_cuda, batch_offs_cuda, batch_inps_cuda, batch_calc_aux_cuda, batch_random_states_cuda\n",
        "        \n",
        "        # Synchronize after all batches\n",
        "        sync_start = time.time()\n",
        "        cuda.synchronize()\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "        \n",
        "        # Final LR after epoch (using same formula as word2vec.c)\n",
        "        denominator = total_words_for_training + 1\n",
        "        final_lr = lr_max * (1.0 - words_processed_total / denominator) if denominator > 0 else lr_max\n",
        "        final_lr = max(final_lr, lr_max * 0.0001)\n",
        "        if epochs > 1:\n",
        "            final_lr = max(final_lr, lr_min)\n",
        "        \n",
        "        progress_percent = (words_processed_total / total_words_for_training * 100) if total_words_for_training > 0 else 0.0\n",
        "        print(f\"  Epoch {epoch+1} completed in {epoch_time:.2f}s (LR: {final_lr:.6f}, Progress: {progress_percent:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nSkip-gram training completed!\")\n",
        "    print(f\"Epoch times - Min: {min(epoch_times):.2f}s, Avg: {np.mean(epoch_times):.2f}s, Max: {max(epoch_times):.2f}s\")\n",
        "    print(f\"Total training time: {time.time()-calc_start:.2f}s\")\n",
        "    print(f\"Total time: {time.time()-start:.2f}s\")\n",
        "    \n",
        "    print_norms(w1_cuda)\n",
        "    \n",
        "    # Save results\n",
        "    stats[\"epoch_time_min_seconds\"] = min(epoch_times)\n",
        "    stats[\"epoch_time_avg_seconds\"] = np.mean(epoch_times)\n",
        "    stats[\"epoch_time_max_seconds\"] = max(epoch_times)\n",
        "    stats[\"epoch_time_total_seconds\"] = sum(epoch_times)\n",
        "    stats[\"epoch_times_all_seconds\"] = epoch_times\n",
        "    \n",
        "    print(f\"Saving Skip-gram vectors to: {out_file_path}\")\n",
        "    write_vectors(w1_cuda, vocab, out_file_path)\n",
        "    \n",
        "    print(f\"Saving parameters to: {params_path}\")\n",
        "    write_json(params, params_path)\n",
        "    \n",
        "    print(f\"Saving statistics to: {stats_path}\")\n",
        "    write_json(stats, stats_path)\n",
        "    \n",
        "    print(\"Skip-gram training completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzcl6Ua8nX70"
      },
      "source": [
        "# **CBOW Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4tu20GWnah1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import time\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "from numba import cuda\n",
        "from numba.cuda import random as c_random\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "\n",
        "@cuda.jit\n",
        "def calc_cbow(\n",
        "        rows: int,\n",
        "        c: int,\n",
        "        k: int,\n",
        "        learning_rate: float,\n",
        "        w1,\n",
        "        w2,\n",
        "        calc_aux,\n",
        "        random_states,\n",
        "        subsample_weights,\n",
        "        negsample_array,\n",
        "        inp,\n",
        "        offsets,\n",
        "        lengths,\n",
        "        use_hs,\n",
        "        syn1,\n",
        "        codes_array,\n",
        "        points_array,\n",
        "        code_lengths,\n",
        "        exp_table,\n",
        "        exp_table_size,\n",
        "        max_exp):\n",
        "    \"\"\"\n",
        "    CUDA kernel for CBOW training\n",
        "    Based on word2vec.c CBOW implementation from the original source code of the Word2Vec paper\n",
        "    Supports both Hierarchical Softmax and Negative Sampling\n",
        "    \"\"\"\n",
        "    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    if idx >= rows:\n",
        "        return\n",
        "    le = lengths[idx]\n",
        "    off = offsets[idx]\n",
        "    \n",
        "    for centre in range(0, le):\n",
        "        word_idx = inp[off + centre]\n",
        "        prob_to_reject = subsample_weights[word_idx]\n",
        "        rnd = c_random.xoroshiro128p_uniform_float32(random_states, idx)\n",
        "        \n",
        "        if rnd > prob_to_reject:\n",
        "            r_f = c_random.xoroshiro128p_uniform_float32(random_states, idx)\n",
        "            r: int = math.ceil(r_f * c)\n",
        "            \n",
        "            # Collect context words (before and after center word)\n",
        "            context_words = cuda.local.array(64, dtype=np.int32)  # Max 2*c context words\n",
        "            context_count = 0\n",
        "            \n",
        "            # Context before center word\n",
        "            for context_pre in range(max(0, centre-r), centre):\n",
        "                if context_count < 20:  # Prevent overflow\n",
        "                    context_words[context_count] = inp[off+context_pre]\n",
        "                    context_count += 1\n",
        "            \n",
        "            # Context after center word\n",
        "            for context_post in range(centre + 1, min(le, centre + 1 + r)):\n",
        "                if context_count < 20:  # Prevent overflow\n",
        "                    context_words[context_count] = inp[off+context_post]\n",
        "                    context_count += 1\n",
        "            \n",
        "            # Only proceed if we have context words\n",
        "            if context_count > 0:\n",
        "                step_cbow(idx, w1, w2, calc_aux, context_words, context_count,\n",
        "                         inp[off+centre], k, learning_rate, negsample_array, random_states,\n",
        "                         use_hs, syn1, codes_array, points_array, code_lengths,\n",
        "                         exp_table, exp_table_size, max_exp)\n",
        "\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def fast_sigmoid(f, exp_table, exp_table_size, max_exp):\n",
        "    \"\"\"\n",
        "    Fast sigmoid using precomputed exp table\n",
        "    Based on word2vec.c exp table lookup\n",
        "    \"\"\"\n",
        "    if f <= -max_exp:\n",
        "        return 0.0\n",
        "    elif f >= max_exp:\n",
        "        return 1.0\n",
        "    else:\n",
        "        idx = int((f + max_exp) * (exp_table_size / max_exp / 2.0))\n",
        "        if idx < 0:\n",
        "            idx = 0\n",
        "        if idx >= exp_table_size:\n",
        "            idx = exp_table_size - 1\n",
        "        return exp_table[idx]\n",
        "\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def step_cbow(thread_idx, w1, w2, calc_aux, context_words, context_count, \n",
        "              center_word, k, learning_rate, negsample_array, random_states,\n",
        "              use_hs, syn1, codes_array, points_array, code_lengths,\n",
        "              exp_table, exp_table_size, max_exp):\n",
        "    \"\"\"\n",
        "    Device function for CBOW gradient calculation\n",
        "    Based on word2vec.c CBOW implementation from the original source code of the Word2Vec paper\n",
        "    Supports both Hierarchical Softmax and Negative Sampling\n",
        "    \"\"\"\n",
        "    emb_dim = w1.shape[1]\n",
        "    negs_arr_len = len(negsample_array)\n",
        "    \n",
        "    # 1. Calculate neu1 = average of context word vectors\n",
        "    neu1 = cuda.local.array(1000, dtype=np.float32)  # Max embedding dimension\n",
        "    neu1e = cuda.local.array(1000, dtype=np.float32)  # Error accumulation\n",
        "    \n",
        "    # Initialize neu1 and neu1e\n",
        "    for i in range(emb_dim):\n",
        "        neu1[i] = 0.0\n",
        "        neu1e[i] = 0.0\n",
        "    \n",
        "    # Average context word vectors\n",
        "    for i in range(emb_dim):\n",
        "        for ctx_idx in range(context_count):\n",
        "            neu1[i] += w1[context_words[ctx_idx], i]\n",
        "        neu1[i] /= context_count\n",
        "    \n",
        "    # 2. Hierarchical Softmax (if enabled)\n",
        "    if use_hs:\n",
        "        codelen = code_lengths[center_word]\n",
        "        max_code_len = codes_array.shape[1]  # Get max code length from array shape\n",
        "        for d in range(codelen):\n",
        "            if d >= max_code_len:\n",
        "                break\n",
        "            node_idx = points_array[center_word, d]\n",
        "            if node_idx < 0:\n",
        "                continue\n",
        "            \n",
        "            # Calculate dot product: neu1 • syn1[node]\n",
        "            f = 0.0\n",
        "            for i in range(emb_dim):\n",
        "                f += neu1[i] * syn1[node_idx, i]\n",
        "            \n",
        "            # Early skip if f is outside range (same as original code)\n",
        "            # This prevents unnecessary updates when sigmoid is saturated\n",
        "            if f <= -max_exp:\n",
        "                continue\n",
        "            if f >= max_exp:\n",
        "                continue\n",
        "            \n",
        "            # Get sigmoid from exp table (only if in range)\n",
        "            sigmoid_val = fast_sigmoid(f, exp_table, exp_table_size, max_exp)\n",
        "            \n",
        "            # Get code bit (0 or 1)\n",
        "            code_bit = codes_array[center_word, d]\n",
        "            if code_bit < 0:\n",
        "                continue\n",
        "            \n",
        "            # Calculate gradient: g = (1 - code_bit - sigmoid) * learning_rate\n",
        "            g = (1.0 - float(code_bit) - sigmoid_val) * learning_rate\n",
        "            \n",
        "            # Propagate errors output -> hidden\n",
        "            for i in range(emb_dim):\n",
        "                neu1e[i] += g * syn1[node_idx, i]\n",
        "            \n",
        "            # Learn weights hidden -> output\n",
        "            for i in range(emb_dim):\n",
        "                syn1[node_idx, i] += g * neu1[i]\n",
        "    \n",
        "    # 3. Negative Sampling (if enabled)\n",
        "    if k > 0:\n",
        "        # Positive sample: predict center_word\n",
        "        dot_xy = 0.0\n",
        "        for i in range(emb_dim):\n",
        "            dot_xy += neu1[i] * w2[center_word, i]\n",
        "        s_xdy_m1 = fast_sigmoid(dot_xy, exp_table, exp_table_size, max_exp) - 1.0\n",
        "        \n",
        "        # Update w2[center_word] and accumulate neu1e\n",
        "        for i in range(emb_dim):\n",
        "            neu1e[i] += -learning_rate * s_xdy_m1 * w2[center_word, i]\n",
        "            w2[center_word, i] -= learning_rate * s_xdy_m1 * neu1[i]\n",
        "        \n",
        "        # Negative samples\n",
        "        for neg_sample in range(0, k):\n",
        "            rnd = c_random.xoroshiro128p_uniform_float32(random_states, thread_idx)\n",
        "            q_idx: int = int(math.floor(negs_arr_len * rnd))\n",
        "            neg = negsample_array[q_idx]\n",
        "            dot_xq = 0.0\n",
        "            for i in range(emb_dim):\n",
        "                dot_xq += neu1[i] * w2[neg, i]\n",
        "            s_dxq = fast_sigmoid(dot_xq, exp_table, exp_table_size, max_exp)\n",
        "            \n",
        "            # Update w2[neg] and accumulate neu1e\n",
        "            for i in range(emb_dim):\n",
        "                neu1e[i] -= learning_rate * s_dxq * w2[neg, i]\n",
        "                w2[neg, i] -= learning_rate * s_dxq * neu1[i]\n",
        "    \n",
        "    # 4. Backprop neu1e to all context words\n",
        "    # Note: Original code does NOT use gradient clipping, only early skip\n",
        "    # Gradient clipping may reduce training effectiveness\n",
        "    # Update context word vectors (same as original code)\n",
        "    for ctx_idx in range(context_count):\n",
        "        for i in range(emb_dim):\n",
        "            w1[context_words[ctx_idx], i] += neu1e[i]\n",
        "\n",
        "\n",
        "def train_cbow(\n",
        "        data_path: str,\n",
        "        out_file_path: str,\n",
        "        epochs: int,\n",
        "        embed_dim: int = 100,\n",
        "        min_occurs: int = 3,\n",
        "        c: int = 5,\n",
        "        k: int = 5,\n",
        "        t: float = 1e-5,\n",
        "        vocab_freq_exponent: float = 0.75,\n",
        "        lr_max: float = 0.025,\n",
        "        lr_min: float = 0.0025,\n",
        "        cuda_threads_per_block: int = 32,\n",
        "        hs: int = 0,\n",
        "        max_memory_gb: float = 70.0,\n",
        "        max_words: int = None,\n",
        "        vocab: list = None,\n",
        "        w_to_i: dict = None,\n",
        "        word_counts: list = None,\n",
        "        ssw: np.ndarray = None,\n",
        "        negs: np.ndarray = None):\n",
        "    \"\"\"\n",
        "    Train CBOW model\n",
        "    Based on word2vec.c CBOW implementation from the original source code of the Word2Vec paper\n",
        "    \n",
        "    Args:\n",
        "        hs: Hierarchical Softmax flag (0=NS only, 1=HS only). Cannot combine with k>0\n",
        "        k: Negative sampling count (0=HS only, >0=NS only). Cannot combine with hs=1\n",
        "        max_memory_gb: Maximum GPU memory usage in GB. If estimated memory exceeds this,\n",
        "                       the dataset will be automatically split into batches for processing\n",
        "                       Default: 70.0 GB (safe for A100 80GB GPU)\n",
        "    \n",
        "    Raises:\n",
        "        ValueError: If both hs=1 and k>0 are specified (HS and NS cannot be combined)\n",
        "    \"\"\"\n",
        "    # Validate: HS and NS cannot be used together\n",
        "    if hs == 1 and k > 0:\n",
        "        raise ValueError(\n",
        "            \"Error: Cannot use HS (hs=1) and Negative Sampling (k>0) together. \"\n",
        "            \"Please choose either HS only (hs=1, k=0) or NS only (hs=0, k>0)\"\n",
        "        )\n",
        "    \n",
        "    params = {\n",
        "        \"model_type\": \"cbow\",\n",
        "        \"w2v_version\": W2V_VERSION,\n",
        "        \"data_path\": data_path,\n",
        "        \"out_file_path\": out_file_path,\n",
        "        \"epochs\": epochs,\n",
        "        \"embed_dim\": embed_dim,\n",
        "        \"min_occurs\": min_occurs,\n",
        "        \"c\": c,\n",
        "        \"k\": k,\n",
        "        \"t\": t,\n",
        "        \"vocab_freq_exponent\": vocab_freq_exponent,\n",
        "        \"lr_max\": lr_max,\n",
        "        \"lr_min\": lr_min,\n",
        "        \"cuda_threads_per_block\": cuda_threads_per_block,\n",
        "        \"hs\": hs\n",
        "    }\n",
        "    stats = {}\n",
        "    params_path = out_file_path + \"_params.json\"\n",
        "    stats_path = out_file_path + \"_stats.json\"\n",
        "\n",
        "    seed = 12345\n",
        "    \n",
        "    # Adjust learning rate based on training method\n",
        "    original_lr_max = lr_max\n",
        "    original_lr_min = lr_min\n",
        "    \n",
        "    # Learning rate handling: HS only and NS only use the same learning rate\n",
        "    # (as per word2vec.c original implementation)\n",
        "    # No special adjustment needed for either method\n",
        "    \n",
        "    # Learning rate schedule\n",
        "    # For multiple epochs: decrease between epochs\n",
        "    # For all epochs: decrease LINEARLY within epoch (as per word2vec.c)\n",
        "    if epochs > 1:\n",
        "        lr_step = (lr_max - lr_min) / (epochs - 1)\n",
        "    else:\n",
        "        lr_step = 0.0  # Not used for single epoch (LR decays within epoch)\n",
        "\n",
        "    print(f\"CBOW Training Parameters:\")\n",
        "    print(f\"Seed: {seed}\")\n",
        "    print(f\"Window size: {c}\")\n",
        "    if hs == 1:\n",
        "        print(f\"Hierarchical Softmax: Enabled\")\n",
        "    if k > 0:\n",
        "        print(f\"Negative samples: {k}\")\n",
        "    if original_lr_max != lr_max:\n",
        "        print(f\"Learning rate adjusted: {original_lr_max} -> {lr_max} (reduced for stability)\")\n",
        "    if epochs == 1:\n",
        "        print(f\"Learning rate: {lr_max} -> ~0 (will decrease linearly within epoch, as per word2vec.c)\")\n",
        "    else:\n",
        "        print(f\"Learning rate: {lr_max} -> {lr_min} (step: {lr_step:.6f} between epochs, also decreases linearly within each epoch)\")\n",
        "    print(f\"Embedding dimension: {embed_dim}\")\n",
        "    print(f\"Min word count: {min_occurs}\")\n",
        "\n",
        "    # Start timing for total execution\n",
        "    start = time.time()\n",
        "\n",
        "    # Build vocabulary if not provided (for reuse when training both models)\n",
        "    if vocab is None or w_to_i is None or word_counts is None:\n",
        "        print(f\"\\nBuilding vocabulary from: {data_path}\")\n",
        "        vocab_start = time.time()\n",
        "        vocab, w_to_i, word_counts = handle_vocab(data_path, min_occurs, freq_exponent=vocab_freq_exponent, use_cache=True)\n",
        "        vocab_size = len(vocab)\n",
        "        build_time = time.time() - vocab_start\n",
        "        print(f\"Vocabulary {'loaded from cache' if build_time < 1.0 else 'built'} in {build_time:.2f}s. Vocab size: {vocab_size:,}\")\n",
        "    else:\n",
        "        vocab_size = len(vocab)\n",
        "        print(f\"\\nUsing pre-built vocabulary. Vocab size: {vocab_size:,}\")\n",
        "    \n",
        "    # Build subsampling weights and negative sampling array if not provided\n",
        "    if ssw is None or negs is None:\n",
        "        ssw, negs = get_subsampling_weights_and_negative_sampling_array(vocab, t=t)\n",
        "    \n",
        "    # Create exp table\n",
        "    print(\"Creating exp table for fast sigmoid...\")\n",
        "    exp_table = create_exp_table(EXP_TABLE_SIZE, MAX_EXP)\n",
        "    \n",
        "    # Setup Hierarchical Softmax if enabled\n",
        "    use_hs = (hs == 1)\n",
        "    syn1_cuda = None\n",
        "    codes_array_cuda = None\n",
        "    points_array_cuda = None\n",
        "    code_lengths_cuda = None\n",
        "    \n",
        "    if use_hs:\n",
        "        print(\"Creating Huffman tree for Hierarchical Softmax...\")\n",
        "        hs_start = time.time()\n",
        "        codes_array, points_array, code_lengths = create_huffman_tree(word_counts, MAX_CODE_LENGTH)\n",
        "        syn1 = init_hs_weight_matrix(vocab_size, embed_dim)\n",
        "        print(f\"Huffman tree created in {time.time() - hs_start:.2f}s\")\n",
        "        print(f\" -Codes array shape: {codes_array.shape}\")\n",
        "        print(f\" -Points array shape: {points_array.shape}\")\n",
        "        print(f\" -Syn1 matrix shape: {syn1.shape}\")\n",
        "\n",
        "    data_files = get_data_file_names(data_path, seed=seed)\n",
        "    print(f\"Processing {len(data_files)} data files...\")\n",
        "    if max_words is not None:\n",
        "        print(f\"⚠️ Limiting to {max_words:,} total words (will stop early if reached)\")\n",
        "    inps_, offs_, lens_ = read_all_data_files_ever(data_path, data_files, w_to_i, max_words=max_words)\n",
        "    inps, offs, lens = (np.asarray(inps_, dtype=np.int32), \n",
        "                       np.asarray(offs_, dtype=np.int32), \n",
        "                       np.asarray(lens_, dtype=np.int32))\n",
        "    sentence_count = len(lens)\n",
        "    total_words = len(inps)  # Total words for LR decay calculation\n",
        "    \n",
        "    print(f\"Data loaded: {sentence_count:,} sentences, {total_words:,} total words\")\n",
        "\n",
        "    # Initialize weight matrices\n",
        "    data_init_start = time.time()\n",
        "    w1, w2 = init_weight_matrices(vocab_size, embed_dim, seed=seed)\n",
        "    data_size_weights = 4 * (w1.size + w2.size)\n",
        "    data_size_inputs = 4 * (inps.size + offs.size + lens.size + ssw.size + negs.size)\n",
        "    \n",
        "    # Calculate memory usage and determine batch size\n",
        "    weights_gb = data_size_weights / (1024**3)\n",
        "    inputs_gb = data_size_inputs / (1024**3)\n",
        "    \n",
        "    # Estimate calc_aux memory for full dataset\n",
        "    calc_aux_size_full = sentence_count * embed_dim * 4\n",
        "    calc_aux_gb_full = calc_aux_size_full / (1024**3)\n",
        "    total_memory_gb = weights_gb + inputs_gb + calc_aux_gb_full\n",
        "    \n",
        "    # Determine if batch processing is needed\n",
        "    use_batch_processing = (total_memory_gb > max_memory_gb)\n",
        "    \n",
        "    if use_batch_processing:\n",
        "        # Calculate batch size based on available memory\n",
        "        available_memory_gb = max_memory_gb - weights_gb - inputs_gb\n",
        "        # Reserve 5GB for overhead\n",
        "        available_memory_gb = max(1.0, available_memory_gb - 5.0)\n",
        "        \n",
        "        # Calculate max sentences per batch\n",
        "        bytes_per_sentence = embed_dim * 4  # float32\n",
        "        max_batch_sentences = int((available_memory_gb * 1024**3) / bytes_per_sentence)\n",
        "        \n",
        "        # Round down to nice numbers for better performance\n",
        "        if max_batch_sentences >= 10_000_000:\n",
        "            batch_size = 10_000_000\n",
        "        elif max_batch_sentences >= 5_000_000:\n",
        "            batch_size = 5_000_000\n",
        "        elif max_batch_sentences >= 2_000_000:\n",
        "            batch_size = 2_000_000\n",
        "        elif max_batch_sentences >= 1_000_000:\n",
        "            batch_size = 1_000_000\n",
        "        else:\n",
        "            batch_size = max(100_000, max_batch_sentences)\n",
        "        \n",
        "        num_batches = math.ceil(sentence_count / batch_size)\n",
        "        batch_aux_gb = (batch_size * embed_dim * 4) / (1024**3)\n",
        "        batch_total_gb = weights_gb + inputs_gb + batch_aux_gb\n",
        "        \n",
        "        print(f\"\\n⚠️ Memory usage would be {total_memory_gb:.1f} GB (exceeds {max_memory_gb} GB limit)\")\n",
        "        print(f\"Using batch processing: {num_batches} batches, {batch_size:,} sentences/batch\")\n",
        "        print(f\"Memory per batch: {batch_total_gb:.1f} GB (calc_aux: {batch_aux_gb:.1f} GB)\")\n",
        "    else:\n",
        "        batch_size = sentence_count\n",
        "        num_batches = 1\n",
        "        print(f\"\\n✅ Memory usage: {total_memory_gb:.1f} GB (within {max_memory_gb} GB limit)\")\n",
        "        print(f\"Processing all {sentence_count:,} sentences in one batch\")\n",
        "    \n",
        "    blocks: int = math.ceil(batch_size / cuda_threads_per_block)\n",
        "    print(f\"CUDA config: {cuda_threads_per_block} threads/block, {blocks} blocks per batch\")\n",
        "\n",
        "    # Transfer to GPU - Transfer weights and vocab arrays (these are shared across batches)\n",
        "    print(\"Transferring data to GPU...\")\n",
        "    data_transfer_start = time.time()\n",
        "    ssw_cuda, negs_cuda = cuda.to_device(ssw), cuda.to_device(negs)\n",
        "    w1_cuda, w2_cuda = cuda.to_device(w1), cuda.to_device(w2)\n",
        "    exp_table_cuda = cuda.to_device(exp_table)\n",
        "    \n",
        "    # Keep input arrays on CPU - will slice and transfer per batch\n",
        "    # This saves GPU memory\n",
        "    \n",
        "    if use_hs:\n",
        "        syn1_cuda = cuda.to_device(syn1)\n",
        "        codes_array_cuda = cuda.to_device(codes_array)\n",
        "        points_array_cuda = cuda.to_device(points_array)\n",
        "        code_lengths_cuda = cuda.to_device(code_lengths)\n",
        "    \n",
        "    print(f\"Data transfer completed in {time.time()-data_transfer_start:.2f}s\")\n",
        "\n",
        "    stats[\"sentence_count\"] = len(lens)\n",
        "    stats[\"word_count\"] = len(inps)\n",
        "    stats[\"vocab_size\"] = vocab_size\n",
        "    stats[\"approx_data_size_weights\"] = data_size_weights\n",
        "    stats[\"approx_data_size_inputs\"] = data_size_inputs\n",
        "    stats[\"use_batch_processing\"] = use_batch_processing\n",
        "    if use_batch_processing:\n",
        "        stats[\"batch_size\"] = batch_size\n",
        "        stats[\"num_batches\"] = num_batches\n",
        "        batch_aux_size = batch_size * embed_dim * 4\n",
        "        stats[\"approx_data_size_aux_per_batch\"] = batch_aux_size\n",
        "        stats[\"approx_data_size_total\"] = data_size_weights + data_size_inputs + batch_aux_size\n",
        "    else:\n",
        "        data_size_aux = 4 * (sentence_count * embed_dim)\n",
        "        stats[\"approx_data_size_aux\"] = data_size_aux\n",
        "        stats[\"approx_data_size_total\"] = data_size_weights + data_size_inputs + data_size_aux\n",
        "\n",
        "    # Prepare HS parameters (use dummy arrays if HS disabled)\n",
        "    if not use_hs:\n",
        "        # Create dummy arrays for HS (will not be used, but needed for kernel signature)\n",
        "        dummy_syn1 = cuda.device_array((1, embed_dim), dtype=np.float32)\n",
        "        dummy_codes = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)\n",
        "        dummy_points = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)\n",
        "        dummy_lengths = cuda.device_array(vocab_size, dtype=np.int32)\n",
        "        syn1_param = dummy_syn1\n",
        "        codes_param = dummy_codes\n",
        "        points_param = dummy_points\n",
        "        lengths_param = dummy_lengths\n",
        "    else:\n",
        "        syn1_param = syn1_cuda\n",
        "        codes_param = codes_array_cuda\n",
        "        points_param = points_array_cuda\n",
        "        lengths_param = code_lengths_cuda\n",
        "    \n",
        "    print_norms(w1_cuda)\n",
        "    print(f\"\\nStarting CBOW training - {epochs} epochs...\")\n",
        "    epoch_times = []\n",
        "    calc_start = time.time()\n",
        "    \n",
        "    # Track total words processed across all epochs (as per word2vec.c)\n",
        "    # Learning rate decays based on total words processed, not per epoch\n",
        "    # Use int64 to avoid overflow with large datasets and multiple epochs\n",
        "    words_processed_total = np.int64(0)\n",
        "    total_words_for_training = np.int64(epochs) * np.int64(total_words)\n",
        "    \n",
        "    for epoch in range(0, epochs):\n",
        "        epoch_start = time.time()\n",
        "        \n",
        "        # Process each batch\n",
        "        for batch_idx in range(num_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = min((batch_idx + 1) * batch_size, sentence_count)\n",
        "            batch_sentence_count = batch_end - batch_start\n",
        "            \n",
        "            if num_batches > 1:\n",
        "                print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{num_batches}: sentences {batch_start:,}-{batch_end:,}\")\n",
        "            \n",
        "            # Calculate word offset for this batch (offsets are cumulative)\n",
        "            batch_word_start = offs[batch_start] if batch_start < len(offs) else 0\n",
        "            batch_word_end = offs[batch_end] if batch_end < len(offs) else len(inps)\n",
        "            batch_word_count = batch_word_end - batch_word_start\n",
        "            \n",
        "            # Calculate learning rate for this batch (linear decay as per word2vec.c)\n",
        "            # Formula from word2vec.c: alpha = starting_alpha * (1 - word_count_actual / (iter * train_words + 1))\n",
        "            # word_count_actual is total words processed across all epochs\n",
        "            # This ensures LR decreases linearly from lr_max to ~0 over entire training\n",
        "            denominator = total_words_for_training + 1\n",
        "            current_lr = lr_max * (1.0 - words_processed_total / denominator) if denominator > 0 else lr_max\n",
        "            \n",
        "            # Apply minimum threshold (as per word2vec.c: min = starting_alpha * 0.0001)\n",
        "            min_lr_threshold = lr_max * 0.0001\n",
        "            current_lr = max(current_lr, min_lr_threshold)\n",
        "            \n",
        "            # Also apply lr_min as additional constraint (for multi-epoch training)\n",
        "            if epochs > 1:\n",
        "                current_lr = max(current_lr, lr_min)\n",
        "            \n",
        "            if num_batches > 1 and batch_idx == 0:\n",
        "                print(f\"    Learning rate: {current_lr:.6f} (decaying linearly, progress: {words_processed_total/total_words_for_training*100:.1f}%)\")\n",
        "            \n",
        "            # Create batch arrays (slicing from CPU arrays)\n",
        "            batch_lens = lens[batch_start:batch_end]\n",
        "            batch_offs_local = offs[batch_start:batch_end] - batch_word_start  # Adjust offsets to start from 0\n",
        "            batch_inps_local = inps[batch_word_start:batch_word_end]\n",
        "            \n",
        "            # Transfer batch arrays to GPU\n",
        "            batch_lens_cuda = cuda.to_device(batch_lens)\n",
        "            batch_offs_cuda = cuda.to_device(batch_offs_local)\n",
        "            batch_inps_cuda = cuda.to_device(batch_inps_local)\n",
        "            \n",
        "            # Create calc_aux for this batch\n",
        "            batch_calc_aux = np.zeros((batch_sentence_count, embed_dim), dtype=np.float32)\n",
        "            batch_calc_aux_cuda = cuda.to_device(batch_calc_aux)\n",
        "            \n",
        "            # Create random states for this batch\n",
        "            batch_random_states_cuda = c_random.create_xoroshiro128p_states(\n",
        "                batch_sentence_count, seed=seed + epoch * 10000 + batch_idx * 100\n",
        "            )\n",
        "            \n",
        "            # Launch CUDA kernel for this batch with current learning rate\n",
        "            batch_blocks = math.ceil(batch_sentence_count / cuda_threads_per_block)\n",
        "            calc_cbow[batch_blocks, cuda_threads_per_block](\n",
        "                batch_sentence_count, c, k, current_lr, w1_cuda, w2_cuda, batch_calc_aux_cuda, \n",
        "                batch_random_states_cuda, ssw_cuda, negs_cuda, batch_inps_cuda, \n",
        "                batch_offs_cuda, batch_lens_cuda,\n",
        "                use_hs, syn1_param, codes_param, points_param, lengths_param,\n",
        "                exp_table_cuda, EXP_TABLE_SIZE, MAX_EXP)\n",
        "            \n",
        "            # Update total words processed counter (as per word2vec.c)\n",
        "            # Note: Actual words processed may vary due to subsampling, but this is an approximation\n",
        "            # Use int64 to avoid overflow with large datasets and multiple epochs\n",
        "            words_processed_total = np.int64(words_processed_total) + np.int64(batch_word_count)\n",
        "            \n",
        "            # Free batch arrays from GPU memory\n",
        "            del batch_lens_cuda, batch_offs_cuda, batch_inps_cuda, batch_calc_aux_cuda, batch_random_states_cuda\n",
        "        \n",
        "        # Synchronize after all batches\n",
        "        sync_start = time.time()\n",
        "        cuda.synchronize()\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "        \n",
        "        # Final LR after epoch (using same formula as word2vec.c)\n",
        "        denominator = total_words_for_training + 1\n",
        "        final_lr = lr_max * (1.0 - words_processed_total / denominator) if denominator > 0 else lr_max\n",
        "        final_lr = max(final_lr, lr_max * 0.0001)\n",
        "        if epochs > 1:\n",
        "            final_lr = max(final_lr, lr_min)\n",
        "        \n",
        "        progress_percent = (words_processed_total / total_words_for_training * 100) if total_words_for_training > 0 else 0.0\n",
        "        print(f\"  Epoch {epoch+1} completed in {epoch_time:.2f}s (LR: {final_lr:.6f}, Progress: {progress_percent:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nCBOW training completed!\")\n",
        "    print(f\"Epoch times - Min: {min(epoch_times):.2f}s, Avg: {np.mean(epoch_times):.2f}s, Max: {max(epoch_times):.2f}s\")\n",
        "    print(f\"Total training time: {time.time()-calc_start:.2f}s\")\n",
        "    print(f\"Total time: {time.time()-start:.2f}s\")\n",
        "    \n",
        "    print_norms(w1_cuda)\n",
        "    \n",
        "    # Save results\n",
        "    stats[\"epoch_time_min_seconds\"] = min(epoch_times)\n",
        "    stats[\"epoch_time_avg_seconds\"] = np.mean(epoch_times)\n",
        "    stats[\"epoch_time_max_seconds\"] = max(epoch_times)\n",
        "    stats[\"epoch_time_total_seconds\"] = sum(epoch_times)\n",
        "    stats[\"epoch_times_all_seconds\"] = epoch_times\n",
        "    \n",
        "    print(f\"Saving CBOW vectors to: {out_file_path}\")\n",
        "    write_vectors(w1_cuda, vocab, out_file_path)\n",
        "    \n",
        "    print(f\"Saving parameters to: {params_path}\")\n",
        "    write_json(params, params_path)\n",
        "    \n",
        "    print(f\"Saving statistics to: {stats_path}\")\n",
        "    write_json(stats, stats_path)\n",
        "    \n",
        "    print(\"CBOW training completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZWO2rTrnozl"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oEp5R7Gnufc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import requests\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "\n",
        "def download_questions_words(output_path: str = \"./data/questions-words.txt\") -> str:\n",
        "    \"\"\"\n",
        "    Download questions-words.txt for word analogy test\n",
        "    \"\"\"\n",
        "    if os.path.isfile(output_path):\n",
        "        print(f\"Questions-words.txt already exists at: {output_path}\")\n",
        "        return output_path\n",
        "    \n",
        "    url = \"https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt\"\n",
        "    print(f\"Downloading questions-words.txt from {url}...\")\n",
        "    \n",
        "    with requests.get(url, stream=True) as response:\n",
        "        response.raise_for_status()\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "    \n",
        "    print(f\"Questions-words.txt downloaded to: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def word_analogy_test(vectors_path: str, questions_path: str = None) -> Tuple[dict, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Run word analogy test on trained vectors\n",
        "    Returns dictionary containing: \n",
        "        - semantic_accuracy\n",
        "        - syntactic_accuracy\n",
        "        - total_accuracy\n",
        "    And details_by_category (list)\n",
        "    \"\"\"\n",
        "\n",
        "    if questions_path is None:\n",
        "        questions_path = download_questions_words()\n",
        "\n",
        "    print(f\"Loading vectors from: {vectors_path}\")\n",
        "    start = time.time()\n",
        "    vecs = KeyedVectors.load_word2vec_format(vectors_path, binary=False)\n",
        "    print(f\"Vectors loaded in {time.time() - start:.2f}s\")\n",
        "\n",
        "    print(f\"Running word analogy test with: {questions_path}\")\n",
        "    eval_start = time.time()\n",
        "    overall_acc, details = vecs.evaluate_word_analogies(questions_path, case_insensitive=True)\n",
        "    print(f\"Word analogy test completed in {time.time() - eval_start:.2f}s\")\n",
        "\n",
        "    semantic_correct = 0\n",
        "    semantic_total = 0\n",
        "\n",
        "    syntactic_correct = 0\n",
        "    syntactic_total = 0\n",
        "\n",
        "    for cat in details:\n",
        "        correct = len(cat[\"correct\"])\n",
        "        total = correct + len(cat[\"incorrect\"])\n",
        "\n",
        "        # Classify semantic vs syntactic based on categories in questions-words.txt\n",
        "        # Semantic (5 categories): capital-common-countries, capital-world, currency, city-in-state, family\n",
        "        # Syntactic (9 categories): gram1-9 (adjective-to-adverb, opposite, comparative, superlative, etc.)\n",
        "        section = cat[\"section\"].lower()\n",
        "\n",
        "        # Semantic categories keywords (5 categories from questions-words.txt)\n",
        "        # 1. capital-common-countries, capital-world -> \"capital\"\n",
        "        # 2. currency -> \"currency\"\n",
        "        # 3. city-in-state -> \"city-in-state\"\n",
        "        # 4. family -> \"family\"\n",
        "        semantic_keywords = [\"capital\", \"currency\", \"family\", \"city-in-state\"]\n",
        "        is_semantic = any(keyword in section for keyword in semantic_keywords)\n",
        "        \n",
        "        if is_semantic:\n",
        "            semantic_correct += correct\n",
        "            semantic_total += total\n",
        "        else:\n",
        "            # Syntactic categories (all remaining categories, usually starting with \"gram\")\n",
        "            syntactic_correct += correct\n",
        "            syntactic_total += total\n",
        "\n",
        "    semantic_acc = semantic_correct / semantic_total if semantic_total > 0 else 0\n",
        "    syntactic_acc = syntactic_correct / syntactic_total if syntactic_total > 0 else 0\n",
        "\n",
        "    # Total overall accuracy\n",
        "    total_acc = (\n",
        "        (semantic_correct + syntactic_correct) /\n",
        "        (semantic_total + syntactic_total)\n",
        "        if (semantic_total + syntactic_total) > 0 else 0\n",
        "    )\n",
        "    return (\n",
        "        {\n",
        "            \"semantic_accuracy\": semantic_acc,\n",
        "            \"syntactic_accuracy\": syntactic_acc,\n",
        "            \"total_accuracy\": total_acc\n",
        "        },\n",
        "        details\n",
        "    )\n",
        "\n",
        "def similarity_test(vectors_path: str, test_words: List[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Test word similarity and find most similar words\n",
        "    \"\"\"\n",
        "    if test_words is None:\n",
        "        test_words = [\"king\", \"queen\", \"man\", \"woman\", \"computer\", \"science\", \"university\", \"student\"]\n",
        "    \n",
        "    print(f\"Loading vectors for similarity test: {vectors_path}\")\n",
        "    vecs = KeyedVectors.load_word2vec_format(vectors_path, binary=False)\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    print(\"\\nMost similar words:\")\n",
        "    for word in test_words:\n",
        "        if word in vecs:\n",
        "            similar = vecs.most_similar(word, topn=5)\n",
        "            results[word] = similar\n",
        "            print(f\"\\n{word}:\")\n",
        "            for sim_word, score in similar:\n",
        "                print(f\"  {sim_word}: {score:.4f}\")\n",
        "        else:\n",
        "            print(f\"Word '{word}' not found in vocabulary\")\n",
        "            results[word] = []\n",
        "    \n",
        "    # Test some word pairs for similarity\n",
        "    word_pairs = [\n",
        "        (\"king\", \"queen\"),\n",
        "        (\"man\", \"woman\"),\n",
        "        (\"computer\", \"science\"),\n",
        "        (\"university\", \"student\"),\n",
        "        (\"good\", \"bad\"),\n",
        "        (\"big\", \"small\")\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nWord pair similarities:\")\n",
        "    pair_similarities = {}\n",
        "    for word1, word2 in word_pairs:\n",
        "        if word1 in vecs and word2 in vecs:\n",
        "            similarity = vecs.similarity(word1, word2)\n",
        "            pair_similarities[f\"{word1}-{word2}\"] = similarity\n",
        "            print(f\"  {word1} - {word2}: {similarity:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {word1} - {word2}: One or both words not found\")\n",
        "            pair_similarities[f\"{word1}-{word2}\"] = None\n",
        "    \n",
        "    results[\"pair_similarities\"] = pair_similarities\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_evaluation_results(results: Dict[str, Any], output_path: str):\n",
        "    \"\"\"\n",
        "    Save evaluation results to JSON file\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    \n",
        "    def convert_numpy_types(obj):\n",
        "        \"\"\"\n",
        "        Convert numpy types to Python native types for JSON serialization\n",
        "        \"\"\"\n",
        "        if isinstance(obj, np.float32):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.float64):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.int32):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.int64):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [convert_numpy_types(item) for item in obj]\n",
        "        else:\n",
        "            return obj\n",
        "    \n",
        "    # Convert numpy types to Python native types\n",
        "    results_converted = convert_numpy_types(results)\n",
        "    \n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results_converted, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Evaluation results saved to: {output_path}\")\n",
        "\n",
        "\n",
        "def compare_models(skipgram_path: str, cbow_path: str, output_path: str = \"./output/model_comparison.json\",\n",
        "                   sg_acc: float = None, sg_details: List[Dict] = None,\n",
        "                   cbow_acc: float = None, cbow_details: List[Dict] = None):\n",
        "    \"\"\"\n",
        "    Compare Skip-gram and CBOW models\n",
        "    \n",
        "    Args:\n",
        "        skipgram_path: Path to Skip-gram vectors\n",
        "        cbow_path: Path to CBOW vectors\n",
        "        output_path: Output path for comparison JSON\n",
        "        sg_acc: Pre-computed Skip-gram accuracy (optional, will compute if None)\n",
        "        sg_details: Pre-computed Skip-gram evaluation details (optional)\n",
        "        cbow_acc: Pre-computed CBOW accuracy (optional, will compute if None)\n",
        "        cbow_details: Pre-computed CBOW evaluation details (optional)\n",
        "    \"\"\"\n",
        "    print(\"Comparing Skip-gram vs CBOW models...\")\n",
        "    \n",
        "    # Evaluate both models only if not provided\n",
        "    if sg_acc is None or sg_details is None:\n",
        "        print(\"Evaluating Skip-gram model...\")\n",
        "        sg_result, sg_details = word_analogy_test(skipgram_path)\n",
        "        sg_acc = sg_result[\"total_accuracy\"]  # Extract total accuracy from dict\n",
        "    else:\n",
        "        print(\"Using pre-computed Skip-gram accuracy...\")\n",
        "    \n",
        "    if cbow_acc is None or cbow_details is None:\n",
        "        print(\"Evaluating CBOW model...\")\n",
        "        cbow_result, cbow_details = word_analogy_test(cbow_path)\n",
        "        cbow_acc = cbow_result[\"total_accuracy\"]  # Extract total accuracy from dict\n",
        "    else:\n",
        "        print(\"Using pre-computed CBOW accuracy...\")\n",
        "    \n",
        "    # Load statistics\n",
        "    sg_stats_path = skipgram_path + \"_stats.json\"\n",
        "    cbow_stats_path = cbow_path + \"_stats.json\"\n",
        "    \n",
        "    sg_stats = {}\n",
        "    cbow_stats = {}\n",
        "    \n",
        "    if os.path.isfile(sg_stats_path):\n",
        "        with open(sg_stats_path, \"r\") as f:\n",
        "            sg_stats = json.load(f)\n",
        "    \n",
        "    if os.path.isfile(cbow_stats_path):\n",
        "        with open(cbow_stats_path, \"r\") as f:\n",
        "            cbow_stats = json.load(f)\n",
        "    \n",
        "    comparison = {\n",
        "        \"models\": {\n",
        "            \"skipgram\": {\n",
        "                \"accuracy\": sg_acc,\n",
        "                \"details\": sg_details,\n",
        "                \"stats\": sg_stats\n",
        "            },\n",
        "            \"cbow\": {\n",
        "                \"accuracy\": cbow_acc,\n",
        "                \"details\": cbow_details,\n",
        "                \"stats\": cbow_stats\n",
        "            }\n",
        "        },\n",
        "        \"summary\": {\n",
        "            \"skipgram_accuracy\": sg_acc,\n",
        "            \"cbow_accuracy\": cbow_acc,\n",
        "            \"accuracy_difference\": sg_acc - cbow_acc,\n",
        "            \"skipgram_training_time\": sg_stats.get(\"epoch_time_total_seconds\", 0),\n",
        "            \"cbow_training_time\": cbow_stats.get(\"epoch_time_total_seconds\", 0),\n",
        "            \"time_difference\": sg_stats.get(\"epoch_time_total_seconds\", 0) - cbow_stats.get(\"epoch_time_total_seconds\", 0)\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    save_evaluation_results(comparison, output_path)\n",
        "    \n",
        "    print(f\"\\nModel Comparison Summary:\")\n",
        "    print(f\"Skip-gram accuracy: {sg_acc:.4f} ({sg_acc*100:.2f}%)\")\n",
        "    print(f\"CBOW accuracy: {cbow_acc:.4f} ({cbow_acc*100:.2f}%)\")\n",
        "    print(f\"Difference: {sg_acc - cbow_acc:.4f} ({(sg_acc - cbow_acc)*100:.2f}%)\")\n",
        "    \n",
        "    if sg_stats and cbow_stats:\n",
        "        sg_time = sg_stats.get(\"epoch_time_total_seconds\", 0)\n",
        "        cbow_time = cbow_stats.get(\"epoch_time_total_seconds\", 0)\n",
        "        print(f\"Skip-gram training time: {sg_time:.2f}s\")\n",
        "        print(f\"CBOW training time: {cbow_time:.2f}s\")\n",
        "        print(f\"Time difference: {sg_time - cbow_time:.2f}s\")\n",
        "    \n",
        "    return comparison\n",
        "\n",
        "\n",
        "def train_gensim_models(data_path: str, output_dir: str = \"./output/gensim\",\n",
        "                        epochs: int = 10, embed_dim: int = 100, min_count: int = 5,\n",
        "                        window: int = 5, negative: int = 5, hs: int = 0,\n",
        "                        alpha: float = 0.025, min_alpha: float = 0.0001,\n",
        "                        sample: float = 1e-5, workers: int = 4) -> Tuple[str, str, float, float]:\n",
        "    \"\"\"\n",
        "    Train Skip-gram and CBOW models using Gensim library\n",
        "    \n",
        "    Args:\n",
        "        data_path: Path to preprocessed data directory\n",
        "        output_dir: Output directory for Gensim models\n",
        "        epochs: Number of training epochs\n",
        "        embed_dim: Embedding dimension\n",
        "        min_count: Minimum word count\n",
        "        window: Window size\n",
        "        negative: Number of negative samples (if hs=0)\n",
        "        hs: Hierarchical Softmax (1=HS, 0=NS)\n",
        "        alpha: Initial learning rate\n",
        "        min_alpha: Minimum learning rate\n",
        "        sample: Subsampling threshold\n",
        "        workers: Number of worker threads\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (skipgram_path, cbow_path, skipgram_time, cbow_time)\n",
        "    \"\"\"\n",
        "    import glob\n",
        "    from data_handler import get_data_file_names\n",
        "    \n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    skipgram_path = os.path.join(output_dir, \"vectors_skipgram_gensim\")\n",
        "    cbow_path = os.path.join(output_dir, \"vectors_cbow_gensim\")\n",
        "    \n",
        "    # Read all sentences from data files\n",
        "    print(\"Reading data files...\")\n",
        "    data_files = get_data_file_names(data_path, seed=12345)\n",
        "    sentences = []\n",
        "    \n",
        "    start = time.time()\n",
        "    for filename in data_files:\n",
        "        filepath = os.path.join(data_path, filename)\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    words = [word for word in re.split(r\"[ .]+\", line) if word]\n",
        "                    if len(words) >= 2:\n",
        "                        sentences.append(words)\n",
        "    \n",
        "    print(f\"Read {len(sentences):,} sentences in {time.time() - start:.2f}s\")\n",
        "    \n",
        "    # Common parameters (Gensim 4.0+ uses vector_size instead of size)\n",
        "    common_params = {\n",
        "        \"vector_size\": embed_dim,  # Changed from \"size\" in Gensim 4.0+\n",
        "        \"window\": window,\n",
        "        \"min_count\": min_count,\n",
        "        \"hs\": hs,\n",
        "        \"negative\": 0 if hs == 1 else negative,\n",
        "        \"alpha\": alpha,\n",
        "        \"min_alpha\": min_alpha,\n",
        "        \"sample\": sample,\n",
        "        \"workers\": workers,\n",
        "        \"sg\": 0  # Will be set per model\n",
        "    }\n",
        "    \n",
        "    # Train Skip-gram model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"  TRAINING GENSIM SKIP-GRAM MODEL\")\n",
        "    print(\"=\"*60)\n",
        "    skipgram_params = common_params.copy()\n",
        "    skipgram_params[\"sg\"] = 1  # Skip-gram\n",
        "    \n",
        "    print(f\"Parameters: {skipgram_params}\")\n",
        "    print(\"Building vocabulary...\")\n",
        "    sg_model = Word2Vec(**skipgram_params)\n",
        "    sg_model.build_vocab(sentences)\n",
        "    vocab_size = len(sg_model.wv.vocab) if hasattr(sg_model.wv, 'vocab') else len(sg_model.wv.key_to_index)\n",
        "    print(f\"Vocabulary size: {vocab_size:,}\")\n",
        "    \n",
        "    print(f\"Training Skip-gram model for {epochs} epochs...\")\n",
        "    sg_start = time.time()\n",
        "    sg_model.train(sentences, total_examples=sg_model.corpus_count, epochs=epochs)\n",
        "    sg_time = time.time() - sg_start\n",
        "    \n",
        "    print(f\"Skip-gram training completed in {sg_time:.2f}s\")\n",
        "    print(f\"Saving Skip-gram vectors to: {skipgram_path}\")\n",
        "    sg_model.wv.save_word2vec_format(skipgram_path, binary=False)\n",
        "    \n",
        "    # Train CBOW model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"  TRAINING GENSIM CBOW MODEL\")\n",
        "    print(\"=\"*60)\n",
        "    cbow_params = common_params.copy()\n",
        "    cbow_params[\"sg\"] = 0  # CBOW\n",
        "    cbow_params[\"alpha\"] = alpha * 2  # CBOW typically uses higher LR\n",
        "    \n",
        "    print(f\"Parameters: {cbow_params}\")\n",
        "    print(\"Building vocabulary...\")\n",
        "    cbow_model = Word2Vec(**cbow_params)\n",
        "    cbow_model.build_vocab(sentences)\n",
        "    vocab_size = len(cbow_model.wv.vocab) if hasattr(cbow_model.wv, 'vocab') else len(cbow_model.wv.key_to_index)\n",
        "    print(f\"Vocabulary size: {vocab_size:,}\")\n",
        "    \n",
        "    print(f\"Training CBOW model for {epochs} epochs...\")\n",
        "    cbow_start = time.time()\n",
        "    cbow_model.train(sentences, total_examples=cbow_model.corpus_count, epochs=epochs)\n",
        "    cbow_time = time.time() - cbow_start\n",
        "    \n",
        "    print(f\"CBOW training completed in {cbow_time:.2f}s\")\n",
        "    print(f\"Saving CBOW vectors to: {cbow_path}\")\n",
        "    cbow_model.wv.save_word2vec_format(cbow_path, binary=False)\n",
        "    \n",
        "    return skipgram_path, cbow_path, sg_time, cbow_time\n",
        "\n",
        "\n",
        "def evaluate_gensim_models(skipgram_path: str, cbow_path: str, \n",
        "                           output_dir: str = \"./output/gensim\",\n",
        "                           questions_path: str = None) -> Tuple[float, List[Dict], float, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Evaluate Gensim-trained Skip-gram and CBOW models.\n",
        "    \n",
        "    Args:\n",
        "        skipgram_path: Path to Skip-gram vectors\n",
        "        cbow_path: Path to CBOW vectors\n",
        "        output_dir: Output directory for evaluation results\n",
        "        questions_path: Path to questions-words.txt file\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (skipgram_acc, skipgram_details, cbow_acc, cbow_details)\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    if questions_path is None:\n",
        "        questions_path = download_questions_words()\n",
        "    \n",
        "    # Evaluate Skip-gram\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"  EVALUATING GENSIM SKIP-GRAM MODEL\")\n",
        "    print(\"=\"*60)\n",
        "    sg_acc, sg_details = word_analogy_test(skipgram_path, questions_path)\n",
        "    sg_sim = similarity_test(skipgram_path)\n",
        "    \n",
        "    sg_eval_path = os.path.join(output_dir, \"skipgram_eval.json\")\n",
        "    save_evaluation_results({\n",
        "        \"accuracy\": sg_acc,\n",
        "        \"details\": sg_details,\n",
        "        \"similarity_test\": sg_sim\n",
        "    }, sg_eval_path)\n",
        "    \n",
        "    # Evaluate CBOW\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"  EVALUATING GENSIM CBOW MODEL\")\n",
        "    print(\"=\"*60)\n",
        "    cbow_acc, cbow_details = word_analogy_test(cbow_path, questions_path)\n",
        "    cbow_sim = similarity_test(cbow_path)\n",
        "    \n",
        "    cbow_eval_path = os.path.join(output_dir, \"cbow_eval.json\")\n",
        "    save_evaluation_results({\n",
        "        \"accuracy\": cbow_acc,\n",
        "        \"details\": cbow_details,\n",
        "        \"similarity_test\": cbow_sim\n",
        "    }, cbow_eval_path)\n",
        "    \n",
        "    return sg_acc, sg_details, cbow_acc, cbow_details\n",
        "\n",
        "\n",
        "def compare_with_gensim(custom_sg_path: str, custom_cbow_path: str,\n",
        "                        gensim_sg_path: str, gensim_cbow_path: str,\n",
        "                        gensim_sg_time: float, gensim_cbow_time: float,\n",
        "                        gensim_sg_acc: float, gensim_sg_details: List[Dict],\n",
        "                        gensim_cbow_acc: float, gensim_cbow_details: List[Dict],\n",
        "                        custom_sg_time: float = None, custom_cbow_time: float = None,\n",
        "                        output_path: str = \"./output/gensim_comparison.json\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compare custom implementation with Gensim models.\n",
        "    \n",
        "    Args:\n",
        "        custom_sg_path: Path to custom Skip-gram vectors\n",
        "        custom_cbow_path: Path to custom CBOW vectors\n",
        "        gensim_sg_path: Path to Gensim Skip-gram vectors\n",
        "        gensim_cbow_path: Path to Gensim CBOW vectors\n",
        "        gensim_sg_time: Gensim Skip-gram training time\n",
        "        gensim_cbow_time: Gensim CBOW training time\n",
        "        gensim_sg_acc: Gensim Skip-gram accuracy\n",
        "        gensim_sg_details: Gensim Skip-gram evaluation details\n",
        "        gensim_cbow_acc: Gensim CBOW accuracy\n",
        "        gensim_cbow_details: Gensim CBOW evaluation details\n",
        "        custom_sg_time: Custom Skip-gram training time (optional)\n",
        "        custom_cbow_time: Custom CBOW training time (optional)\n",
        "        output_path: Output path for comparison JSON\n",
        "    \n",
        "    Returns:\n",
        "        Comparison dictionary\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"  COMPARING CUSTOM vs GENSIM MODELS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Evaluate custom models\n",
        "    print(\"Evaluating custom models...\")\n",
        "    custom_sg_acc, custom_sg_details = word_analogy_test(custom_sg_path)\n",
        "    custom_cbow_acc, custom_cbow_details = word_analogy_test(custom_cbow_path)\n",
        "    \n",
        "    # Load custom model statistics if available\n",
        "    custom_sg_stats = {}\n",
        "    custom_cbow_stats = {}\n",
        "    \n",
        "    custom_sg_stats_path = custom_sg_path + \"_stats.json\"\n",
        "    custom_cbow_stats_path = custom_cbow_path + \"_stats.json\"\n",
        "    \n",
        "    if os.path.isfile(custom_sg_stats_path):\n",
        "        with open(custom_sg_stats_path, \"r\") as f:\n",
        "            custom_sg_stats = json.load(f)\n",
        "            if custom_sg_time is None:\n",
        "                custom_sg_time = custom_sg_stats.get(\"epoch_time_total_seconds\", 0)\n",
        "    \n",
        "    if os.path.isfile(custom_cbow_stats_path):\n",
        "        with open(custom_cbow_stats_path, \"r\") as f:\n",
        "            custom_cbow_stats = json.load(f)\n",
        "            if custom_cbow_time is None:\n",
        "                custom_cbow_time = custom_cbow_stats.get(\"epoch_time_total_seconds\", 0)\n",
        "    \n",
        "    # Create comparison\n",
        "    comparison = {\n",
        "        \"skipgram\": {\n",
        "            \"custom\": {\n",
        "                \"accuracy\": custom_sg_acc,\n",
        "                \"training_time\": custom_sg_time,\n",
        "                \"details\": custom_sg_details\n",
        "            },\n",
        "            \"gensim\": {\n",
        "                \"accuracy\": gensim_sg_acc,\n",
        "                \"training_time\": gensim_sg_time,\n",
        "                \"details\": gensim_sg_details\n",
        "            },\n",
        "            \"accuracy_difference\": custom_sg_acc - gensim_sg_acc,\n",
        "            \"time_difference\": (custom_sg_time or 0) - gensim_sg_time,\n",
        "            \"speedup\": gensim_sg_time / (custom_sg_time or 1) if custom_sg_time else None\n",
        "        },\n",
        "        \"cbow\": {\n",
        "            \"custom\": {\n",
        "                \"accuracy\": custom_cbow_acc,\n",
        "                \"training_time\": custom_cbow_time,\n",
        "                \"details\": custom_cbow_details\n",
        "            },\n",
        "            \"gensim\": {\n",
        "                \"accuracy\": gensim_cbow_acc,\n",
        "                \"training_time\": gensim_cbow_time,\n",
        "                \"details\": gensim_cbow_details\n",
        "            },\n",
        "            \"accuracy_difference\": custom_cbow_acc - gensim_cbow_acc,\n",
        "            \"time_difference\": (custom_cbow_time or 0) - gensim_cbow_time,\n",
        "            \"speedup\": gensim_cbow_time / (custom_cbow_time or 1) if custom_cbow_time else None\n",
        "        },\n",
        "        \"summary\": {\n",
        "            \"custom_skipgram_accuracy\": custom_sg_acc,\n",
        "            \"gensim_skipgram_accuracy\": gensim_sg_acc,\n",
        "            \"custom_cbow_accuracy\": custom_cbow_acc,\n",
        "            \"gensim_cbow_accuracy\": gensim_cbow_acc,\n",
        "            \"skipgram_accuracy_diff\": custom_sg_acc - gensim_sg_acc,\n",
        "            \"cbow_accuracy_diff\": custom_cbow_acc - gensim_cbow_acc\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    save_evaluation_results(comparison, output_path)\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"  COMPARISON SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nSkip-gram:\")\n",
        "    print(f\"  Custom:  {custom_sg_acc:.4f} ({custom_sg_acc*100:.2f}%) - {custom_sg_time:.2f}s\" if custom_sg_time else f\"  Custom:  {custom_sg_acc:.4f} ({custom_sg_acc*100:.2f}%)\")\n",
        "    print(f\"  Gensim:  {gensim_sg_acc:.4f} ({gensim_sg_acc*100:.2f}%) - {gensim_sg_time:.2f}s\")\n",
        "    print(f\"  Diff:    {custom_sg_acc - gensim_sg_acc:+.4f} ({(custom_sg_acc - gensim_sg_acc)*100:+.2f}%)\")\n",
        "    if custom_sg_time:\n",
        "        print(f\"  Speedup: {gensim_sg_time / custom_sg_time:.2f}x {'(Gensim faster)' if gensim_sg_time < custom_sg_time else '(Custom faster)'}\")\n",
        "    \n",
        "    print(\"\\nCBOW:\")\n",
        "    print(f\"  Custom:  {custom_cbow_acc:.4f} ({custom_cbow_acc*100:.2f}%) - {custom_cbow_time:.2f}s\" if custom_cbow_time else f\"  Custom:  {custom_cbow_acc:.4f} ({custom_cbow_acc*100:.2f}%)\")\n",
        "    print(f\"  Gensim:  {gensim_cbow_acc:.4f} ({gensim_cbow_acc*100:.2f}%) - {gensim_cbow_time:.2f}s\")\n",
        "    print(f\"  Diff:    {custom_cbow_acc - gensim_cbow_acc:+.4f} ({(custom_cbow_acc - gensim_cbow_acc)*100:+.2f}%)\")\n",
        "    if custom_cbow_time:\n",
        "        print(f\"  Speedup: {gensim_cbow_time / custom_cbow_time:.2f}x {'(Gensim faster)' if gensim_cbow_time < custom_cbow_time else '(Custom faster)'}\")\n",
        "    \n",
        "    return comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i16qHfHqpBwr"
      },
      "source": [
        "# **Main Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSFSdgBDpEcb",
        "outputId": "d16a9e37-2323-4326-b34c-43e2180ee595"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ numba-cuda configuration set for CUDA PTX compatibility\n",
            "\n",
            "Dataset: Text8\n",
            "  - Text8 Wikipedia (17M words, ~100MB)\n",
            "  - Smaller, faster to download and process\n",
            "  🎯 Training: Negative Sampling ONLY (HS=0, k>0)\n",
            "\n",
            "============================================================\n",
            "  STEP 1: GPU AVAILABILITY CHECK\n",
            "============================================================\n",
            "✓ CUDA GPU available: b'NVIDIA A100-SXM4-40GB'\n",
            "  Memory: 40.0 GB\n",
            "\n",
            "============================================================\n",
            "  STEP 2: DOWNLOADING & PREPROCESSING TEXT8\n",
            "============================================================\n",
            "Downloading text8 from http://mattmahoney.net/dc/text8.zip...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 31.3M/31.3M [00:00<00:00, 73.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/text8/text8.zip...\n",
            "Text8 dataset ready at: ./data/text8/text8\n",
            "Preprocessing text8 file: ./data/text8/text8\n",
            "Output directory: ./data/text8_processed\n",
            "Words per sentence: 1000\n",
            "Total words: 17,005,207\n",
            "Created 17,006 sentences\n",
            "Wrote 17,006 sentences to 0000\n",
            "Preprocessing complete. Created 1 files in ./data/text8_processed\n",
            "\n",
            "============================================================\n",
            "  STEP 3: TRAINING SKIP-GRAM MODEL\n",
            "============================================================\n",
            "Skip-gram parameters:\n",
            "  epochs: 10\n",
            "  embed_dim: 100\n",
            "  min_occurs: 5\n",
            "  c: 5\n",
            "  k: 5\n",
            "  t: 1e-05\n",
            "  vocab_freq_exponent: 0.75\n",
            "  lr_max: 0.025\n",
            "  lr_min: 0.0001\n",
            "  cuda_threads_per_block: 32\n",
            "  hs: 0\n",
            "Skip-gram Training Parameters:\n",
            "Seed: 12345\n",
            "Window size: 5\n",
            "Negative samples: 5\n",
            "Learning rate: 0.025 → 0.0001 (step: 0.002767)\n",
            "Embedding dimension: 100\n",
            "Min word count: 5\n",
            "\n",
            "Building vocabulary from: ./data/text8_processed\n",
            "Creating negative sampling array with size 10,000,000 for vocab size 60,603\n",
            "⚠️  WARNING: 1 words have frequency too low and will be excluded from negative sampling\n",
            "   Consider increasing arr_len or reducing min_occurs threshold\n",
            "Negative sampling array created: 9,999,190 entries (10.00M)\n",
            "Vocabulary built in 9.14s. Vocab size: 60,603\n",
            "Creating exp table for fast sigmoid...\n",
            "Processing 1 data files...\n",
            "read_all_data_files_ever() STATS: defaultdict(<class 'int'>, {'file_read_lines_ok': 17006, 'one_word_sentence_lines_which_were_ignored': 0})\n",
            "read_all_data_files_ever() Total time 8.274356365203857 s for 1 files (avg 8.274356365203857 s/file)\n",
            "Data loaded: 17,006 sentences, 16,640,699 total words\n",
            "CUDA config: 32 threads/block, 532 blocks\n",
            "Weight matrices initialized in 0.19s\n",
            "Memory usage: 48,482,400 weights + 106,938,016 inputs + 6,802,400 aux = 162,222,816 bytes\n",
            "Transferring data to GPU...\n",
            "Data transfer completed in 0.04s\n",
            "Initializing CUDA random states for 17,006 threads...\n",
            "CUDA random states initialized in 1.17s\n",
            "Vector norms (count 60603) 2.5% median mean 97.5%: 0.8608  0.9968  0.9973  1.1382\n",
            "\n",
            "Starting Skip-gram training - 10 epochs...\n",
            "  Epoch 1 kernel launched in 2.19s (LR: 0.025000)\n",
            "  Synchronized in 2.38s\n",
            "  → Epoch 1 completed in 4.56s\n",
            "  Epoch 2 kernel launched in 0.00s (LR: 0.022233)\n",
            "  Synchronized in 2.31s\n",
            "  → Epoch 2 completed in 2.31s\n",
            "  Epoch 3 kernel launched in 0.00s (LR: 0.019467)\n",
            "  Synchronized in 2.30s\n",
            "  → Epoch 3 completed in 2.31s\n",
            "  Epoch 4 kernel launched in 0.00s (LR: 0.016700)\n",
            "  Synchronized in 2.32s\n",
            "  → Epoch 4 completed in 2.32s\n",
            "  Epoch 5 kernel launched in 0.00s (LR: 0.013933)\n",
            "  Synchronized in 2.31s\n",
            "  → Epoch 5 completed in 2.31s\n",
            "  Epoch 6 kernel launched in 0.00s (LR: 0.011167)\n",
            "  Synchronized in 2.30s\n",
            "  → Epoch 6 completed in 2.30s\n",
            "  Epoch 7 kernel launched in 0.00s (LR: 0.008400)\n",
            "  Synchronized in 2.32s\n",
            "  → Epoch 7 completed in 2.32s\n",
            "  Epoch 8 kernel launched in 0.00s (LR: 0.005633)\n",
            "  Synchronized in 2.32s\n",
            "  → Epoch 8 completed in 2.32s\n",
            "  Epoch 9 kernel launched in 0.00s (LR: 0.002867)\n",
            "  Synchronized in 2.31s\n",
            "  → Epoch 9 completed in 2.31s\n",
            "  Epoch 10 kernel launched in 0.00s (LR: 0.000100)\n",
            "  Synchronized in 2.32s\n",
            "  → Epoch 10 completed in 2.32s\n",
            "\n",
            "Skip-gram training completed!\n",
            "Epoch times - Min: 2.30s, Avg: 2.54s, Max: 4.56s\n",
            "Total training time: 25.39s\n",
            "Total time: 45.28s\n",
            "Vector norms (count 60603) 2.5% median mean 97.5%: 1.9919  2.9273  2.9314  4.0074\n",
            "Saving Skip-gram vectors to: ./output/vectors_skipgram\n",
            "Saving parameters to: ./output/vectors_skipgram_params.json\n",
            "Saving statistics to: ./output/vectors_skipgram_stats.json\n",
            "Skip-gram training completed successfully!\n",
            "\n",
            "============================================================\n",
            "  STEP 4: TRAINING CBOW MODEL\n",
            "============================================================\n",
            "CBOW parameters:\n",
            "  epochs: 10\n",
            "  embed_dim: 100\n",
            "  min_occurs: 5\n",
            "  c: 5\n",
            "  k: 5\n",
            "  t: 1e-05\n",
            "  vocab_freq_exponent: 0.75\n",
            "  lr_max: 0.05\n",
            "  lr_min: 0.0001\n",
            "  cuda_threads_per_block: 32\n",
            "  hs: 0\n",
            "CBOW Training Parameters:\n",
            "Seed: 12345\n",
            "Window size: 5\n",
            "Negative samples: 5\n",
            "Learning rate: 0.05 → 0.0001 (step: 0.005544)\n",
            "Embedding dimension: 100\n",
            "Min word count: 5\n",
            "\n",
            "Building vocabulary from: ./data/text8_processed\n",
            "Creating negative sampling array with size 10,000,000 for vocab size 60,603\n",
            "⚠️  WARNING: 1 words have frequency too low and will be excluded from negative sampling\n",
            "   Consider increasing arr_len or reducing min_occurs threshold\n",
            "Negative sampling array created: 9,999,190 entries (10.00M)\n",
            "Vocabulary built in 9.17s. Vocab size: 60,603\n",
            "Creating exp table for fast sigmoid...\n",
            "Processing 1 data files...\n",
            "read_all_data_files_ever() STATS: defaultdict(<class 'int'>, {'file_read_lines_ok': 17006, 'one_word_sentence_lines_which_were_ignored': 0})\n",
            "read_all_data_files_ever() Total time 8.423961162567139 s for 1 files (avg 8.423961162567139 s/file)\n",
            "Data loaded: 17,006 sentences, 16,640,699 total words\n",
            "CUDA config: 32 threads/block, 532 blocks\n",
            "Weight matrices initialized in 0.18s\n",
            "Memory usage: 48,482,400 weights + 106,938,016 inputs + 6,802,400 aux = 162,222,816 bytes\n",
            "Transferring data to GPU...\n",
            "Data transfer completed in 0.04s\n",
            "Initializing CUDA random states for 17,006 threads...\n",
            "CUDA random states initialized in 0.00s\n",
            "Vector norms (count 60603) 2.5% median mean 97.5%: 0.8608  0.9968  0.9973  1.1382\n",
            "\n",
            "Starting CBOW training - 10 epochs...\n",
            "  Epoch 1 kernel launched in 1.80s (LR: 0.050000)\n",
            "  Synchronized in 0.76s\n",
            "  → Epoch 1 completed in 2.57s\n",
            "  Epoch 2 kernel launched in 0.00s (LR: 0.044456)\n",
            "  Synchronized in 0.71s\n",
            "  → Epoch 2 completed in 0.71s\n",
            "  Epoch 3 kernel launched in 0.00s (LR: 0.038911)\n",
            "  Synchronized in 0.71s\n",
            "  → Epoch 3 completed in 0.71s\n",
            "  Epoch 4 kernel launched in 0.00s (LR: 0.033367)\n",
            "  Synchronized in 0.71s\n",
            "  → Epoch 4 completed in 0.71s\n",
            "  Epoch 5 kernel launched in 0.00s (LR: 0.027822)\n",
            "  Synchronized in 0.71s\n",
            "  → Epoch 5 completed in 0.71s\n",
            "  Epoch 6 kernel launched in 0.00s (LR: 0.022278)\n",
            "  Synchronized in 0.71s\n",
            "  → Epoch 6 completed in 0.71s\n",
            "  Epoch 7 kernel launched in 0.00s (LR: 0.016733)\n",
            "  Synchronized in 0.71s\n",
            "  → Epoch 7 completed in 0.71s\n",
            "  Epoch 8 kernel launched in 0.00s (LR: 0.011189)\n",
            "  Synchronized in 0.71s\n",
            "  → Epoch 8 completed in 0.71s\n",
            "  Epoch 9 kernel launched in 0.00s (LR: 0.005644)\n",
            "  Synchronized in 0.71s\n",
            "  → Epoch 9 completed in 0.71s\n",
            "  Epoch 10 kernel launched in 0.00s (LR: 0.000100)\n",
            "  Synchronized in 0.71s\n",
            "  → Epoch 10 completed in 0.71s\n",
            "\n",
            "CBOW training completed!\n",
            "Epoch times - Min: 0.71s, Avg: 0.90s, Max: 2.57s\n",
            "Total training time: 8.97s\n",
            "Total time: 27.85s\n",
            "Vector norms (count 60603) 2.5% median mean 97.5%: 1.2163  2.4169  3.9555  14.4595\n",
            "Saving CBOW vectors to: ./output/vectors_cbow\n",
            "Saving parameters to: ./output/vectors_cbow_params.json\n",
            "Saving statistics to: ./output/vectors_cbow_stats.json\n",
            "CBOW training completed successfully!\n",
            "\n",
            "============================================================\n",
            "  STEP 5: EVALUATING SKIP-GRAM MODEL\n",
            "============================================================\n",
            "Downloading questions-words.txt from https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt...\n",
            "Questions-words.txt downloaded to: ./data/questions-words.txt\n",
            "Loading vectors from: ./output/vectors_skipgram\n",
            "Vectors loaded in 6.42s\n",
            "Running word analogy test with: ./data/questions-words.txt\n",
            "Word analogy test completed in 13.94s\n",
            "Overall accuracy: 0.2735 (27.35%)\n",
            "  capital-common-countries: 0.6028 (60.28%) - 305/506\n",
            "  capital-world: 0.2546 (25.46%) - 787/3091\n",
            "  currency: 0.1241 (12.41%) - 68/548\n",
            "  city-in-state: 0.1612 (16.12%) - 355/2202\n",
            "  family: 0.4857 (48.57%) - 204/420\n",
            "  gram1-adjective-to-adverb: 0.0917 (9.17%) - 91/992\n",
            "  gram2-opposite: 0.0688 (6.88%) - 52/756\n",
            "  gram3-comparative: 0.4437 (44.37%) - 591/1332\n",
            "  gram4-superlative: 0.1845 (18.45%) - 183/992\n",
            "  gram5-present-participle: 0.1970 (19.70%) - 208/1056\n",
            "  gram6-nationality-adjective: 0.5911 (59.11%) - 899/1521\n",
            "  gram7-past-tense: 0.2141 (21.41%) - 334/1560\n",
            "  gram8-plural: 0.3093 (30.93%) - 412/1332\n",
            "  gram9-plural-verbs: 0.2402 (24.02%) - 209/870\n",
            "  Total accuracy: 0.2735 (27.35%) - 4698/17178\n",
            "Loading vectors for similarity test: ./output/vectors_skipgram\n",
            "\n",
            "Most similar words:\n",
            "\n",
            "king:\n",
            "  prince: 0.8072\n",
            "  son: 0.8027\n",
            "  queen: 0.7914\n",
            "  theodoric: 0.7706\n",
            "  lothair: 0.7689\n",
            "\n",
            "queen:\n",
            "  king: 0.7914\n",
            "  prince: 0.7804\n",
            "  elizabeth: 0.7793\n",
            "  crown: 0.7631\n",
            "  victoria: 0.7550\n",
            "\n",
            "man:\n",
            "  woman: 0.7979\n",
            "  god: 0.7356\n",
            "  girl: 0.7332\n",
            "  thing: 0.7282\n",
            "  creature: 0.7259\n",
            "\n",
            "woman:\n",
            "  man: 0.7979\n",
            "  child: 0.7600\n",
            "  girl: 0.7441\n",
            "  person: 0.7179\n",
            "  prostitute: 0.7087\n",
            "\n",
            "computer:\n",
            "  computers: 0.8265\n",
            "  computing: 0.8100\n",
            "  hardware: 0.7886\n",
            "  software: 0.7651\n",
            "  networking: 0.7526\n",
            "\n",
            "science:\n",
            "  fiction: 0.7791\n",
            "  humanities: 0.7772\n",
            "  physics: 0.7506\n",
            "  society: 0.7491\n",
            "  sciences: 0.7444\n",
            "\n",
            "university:\n",
            "  college: 0.8687\n",
            "  institute: 0.8135\n",
            "  cambridge: 0.7834\n",
            "  polytechnic: 0.7753\n",
            "  school: 0.7750\n",
            "\n",
            "student:\n",
            "  school: 0.7619\n",
            "  graduate: 0.7431\n",
            "  teacher: 0.7342\n",
            "  students: 0.7329\n",
            "  faculty: 0.7219\n",
            "\n",
            "Word pair similarities:\n",
            "  king - queen: 0.7914\n",
            "  man - woman: 0.7979\n",
            "  computer - science: 0.7021\n",
            "  university - student: 0.6719\n",
            "  good - bad: 0.8029\n",
            "  big - small: 0.5281\n",
            "Evaluation results saved to: ./output/skipgram_eval.json\n",
            "\n",
            "============================================================\n",
            "  STEP 6: EVALUATING CBOW MODEL\n",
            "============================================================\n",
            "Questions-words.txt already exists at: ./data/questions-words.txt\n",
            "Loading vectors from: ./output/vectors_cbow\n",
            "Vectors loaded in 6.40s\n",
            "Running word analogy test with: ./data/questions-words.txt\n",
            "Word analogy test completed in 13.88s\n",
            "Overall accuracy: 0.2596 (25.96%)\n",
            "  capital-common-countries: 0.4802 (48.02%) - 243/506\n",
            "  capital-world: 0.2174 (21.74%) - 672/3091\n",
            "  currency: 0.0639 (6.39%) - 35/548\n",
            "  city-in-state: 0.1894 (18.94%) - 417/2202\n",
            "  family: 0.5333 (53.33%) - 224/420\n",
            "  gram1-adjective-to-adverb: 0.0746 (7.46%) - 74/992\n",
            "  gram2-opposite: 0.0384 (3.84%) - 29/756\n",
            "  gram3-comparative: 0.4535 (45.35%) - 604/1332\n",
            "  gram4-superlative: 0.1058 (10.58%) - 105/992\n",
            "  gram5-present-participle: 0.1676 (16.76%) - 177/1056\n",
            "  gram6-nationality-adjective: 0.5799 (57.99%) - 882/1521\n",
            "  gram7-past-tense: 0.2199 (21.99%) - 343/1560\n",
            "  gram8-plural: 0.3416 (34.16%) - 455/1332\n",
            "  gram9-plural-verbs: 0.2287 (22.87%) - 199/870\n",
            "  Total accuracy: 0.2596 (25.96%) - 4459/17178\n",
            "Loading vectors for similarity test: ./output/vectors_cbow\n",
            "\n",
            "Most similar words:\n",
            "\n",
            "king:\n",
            "  kings: 0.6773\n",
            "  throne: 0.6548\n",
            "  prince: 0.6243\n",
            "  macedon: 0.6151\n",
            "  aragon: 0.6151\n",
            "\n",
            "queen:\n",
            "  elizabeth: 0.7465\n",
            "  princess: 0.6968\n",
            "  consort: 0.6357\n",
            "  daughter: 0.6278\n",
            "  king: 0.6120\n",
            "\n",
            "man:\n",
            "  woman: 0.7060\n",
            "  girl: 0.5745\n",
            "  creature: 0.5651\n",
            "  men: 0.5472\n",
            "  person: 0.5391\n",
            "\n",
            "woman:\n",
            "  child: 0.7284\n",
            "  man: 0.7060\n",
            "  girl: 0.6794\n",
            "  person: 0.6658\n",
            "  spouse: 0.6631\n",
            "\n",
            "computer:\n",
            "  computers: 0.7413\n",
            "  hardware: 0.6640\n",
            "  graphics: 0.6536\n",
            "  computing: 0.6450\n",
            "  pc: 0.6144\n",
            "\n",
            "science:\n",
            "  psychology: 0.6545\n",
            "  anthropology: 0.6421\n",
            "  sociology: 0.6340\n",
            "  sciences: 0.6184\n",
            "  physics: 0.5820\n",
            "\n",
            "university:\n",
            "  univ: 0.7703\n",
            "  institute: 0.6626\n",
            "  bloomington: 0.6492\n",
            "  college: 0.6399\n",
            "  polytechnic: 0.6300\n",
            "\n",
            "student:\n",
            "  graduate: 0.7745\n",
            "  undergraduate: 0.7427\n",
            "  diploma: 0.7172\n",
            "  students: 0.7139\n",
            "  professors: 0.6890\n",
            "\n",
            "Word pair similarities:\n",
            "  king - queen: 0.6120\n",
            "  man - woman: 0.7060\n",
            "  computer - science: 0.4074\n",
            "  university - student: 0.4972\n",
            "  good - bad: 0.6929\n",
            "  big - small: 0.2419\n",
            "Evaluation results saved to: ./output/cbow_eval.json\n",
            "\n",
            "============================================================\n",
            "  STEP 7: COMPARING CUSTOM MODELS (Skip-gram vs CBOW)\n",
            "============================================================\n",
            "Comparing Skip-gram vs CBOW models...\n",
            "Using pre-computed Skip-gram accuracy...\n",
            "Using pre-computed CBOW accuracy...\n",
            "Evaluation results saved to: ./output/model_comparison.json\n",
            "\n",
            "Model Comparison Summary:\n",
            "Skip-gram accuracy: 0.2735 (27.35%)\n",
            "CBOW accuracy: 0.2596 (25.96%)\n",
            "Difference: 0.0139 (1.39%)\n",
            "Skip-gram training time: 25.39s\n",
            "CBOW training time: 8.96s\n",
            "Time difference: 16.42s\n",
            "\n",
            "============================================================\n",
            "  STEP 8: CREATING VISUALIZATIONS\n",
            "============================================================\n",
            "Creating t-SNE visualizations...\n",
            "Creating t-SNE visualization for: ./output/vectors_skipgram\n",
            "Parameters: n_words=500, perplexity=30\n",
            "Loaded 60602 word vectors\n",
            "Selected 500 words for visualization\n",
            "Applying t-SNE dimensionality reduction...\n",
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 500 samples in 0.000s...\n",
            "[t-SNE] Computed neighbors for 500 samples in 0.034s...\n",
            "[t-SNE] Computed conditional probabilities for sample 500 / 500\n",
            "[t-SNE] Mean sigma: 0.755674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 69.066818\n",
            "[t-SNE] KL divergence after 1000 iterations: 1.196429\n",
            "t-SNE plot saved to: ./output/skipgram_tsne.png\n",
            "Creating t-SNE visualization for: ./output/vectors_cbow\n",
            "Parameters: n_words=500, perplexity=30\n",
            "Loaded 60602 word vectors\n",
            "Selected 500 words for visualization\n",
            "Applying t-SNE dimensionality reduction...\n",
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 500 samples in 0.000s...\n",
            "[t-SNE] Computed neighbors for 500 samples in 0.008s...\n",
            "[t-SNE] Computed conditional probabilities for sample 500 / 500\n",
            "[t-SNE] Mean sigma: 5.649043\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 75.555008\n",
            "[t-SNE] KL divergence after 1000 iterations: 1.356079\n",
            "t-SNE plot saved to: ./output/cbow_tsne.png\n",
            "Creating similarity heatmaps...\n",
            "Creating similarity heatmap for: ./output/vectors_skipgram\n",
            "Words: ['king', 'queen', 'man', 'woman', 'computer', 'science', 'university', 'student']\n",
            "Similarity heatmap saved to: ./output/skipgram_heatmap.png\n",
            "Creating similarity heatmap for: ./output/vectors_cbow\n",
            "Words: ['king', 'queen', 'man', 'woman', 'computer', 'science', 'university', 'student']\n",
            "Similarity heatmap saved to: ./output/cbow_heatmap.png\n",
            "Creating training comparison plots...\n",
            "Creating training comparison plots...\n",
            "Training comparison plot saved to: ./output/training_comparison.png\n",
            "Creating accuracy comparison plot...\n",
            "Accuracy comparison plot saved to: ./output/accuracy_comparison.png\n",
            "\n",
            "============================================================\n",
            "  FINAL SUMMARY\n",
            "============================================================\n",
            "Model Performance:\n",
            "  Skip-gram accuracy: 0.2735 (27.35%)\n",
            "  CBOW accuracy: 0.2596 (25.96%)\n",
            "  Difference: 0.0139 (+1.39%)\n",
            "\n",
            "Training Times:\n",
            "  Skip-gram: 25.39s\n",
            "  CBOW: 8.96s\n",
            "  Difference: 16.42s\n",
            "\n",
            "Data Processed:\n",
            "  Words: 16,640,699\n",
            "  Sentences: 17,006\n",
            "  Vocabulary: 60,603\n",
            "\n",
            "Output Files:\n",
            "  Skip-gram vectors: ./output/vectors_skipgram\n",
            "  CBOW vectors: ./output/vectors_cbow\n",
            "  Visualizations: ./output/*.png\n",
            "  Evaluation results: ./output/*.json\n",
            "\n",
            "🎉 Word2Vec training and evaluation completed successfully!\n",
            "📁 Check the ./output/ directory for all results and visualizations.\n",
            "\n",
            "Dataset used: Text8\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# Copyright 2024 Word2Vec Implementation\n",
        "# Main script to run complete Word2Vec pipeline - NOTEBOOK VERSION\n",
        "# Modified to run in notebook (no interactive menu, use config variables)\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION - Change these values\n",
        "# ============================================\n",
        "# Dataset selection\n",
        "use_wmt14 = False      # True to use WMT14 News\n",
        "dataset_name = \"Text8\" # \"Text8\" or \"WMT14 News\"\n",
        "\n",
        "# Dataset size (only for WMT14)\n",
        "max_sentences = None   # None = full dataset, or number like 100000\n",
        "max_files = None       # None = all files, or number like 10\n",
        "max_words = None       # None = no limit, or number like 700000000 for 700M words\n",
        "\n",
        "# Training method\n",
        "use_hs_only = False    # True = HS only (HS=1, k=0)\n",
        "use_hs = False         # True = HS only (HS=1, k=0), cannot combine with k>0\n",
        "\n",
        "# Model selection\n",
        "should_train_skipgram = True  # True to train Skip-gram\n",
        "should_train_cbow = True      # True to train CBOW\n",
        "\n",
        "# Phrase detection\n",
        "use_phrases = False    # True to enable phrase detection\n",
        "\n",
        "# Gensim training\n",
        "use_gensim = False     # True to train Gensim models\n",
        "\n",
        "# ============================================\n",
        "# IMPORTS\n",
        "# ============================================\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure numba-cuda for CUDA PTX compatibility (Official Solution)\n",
        "# Based on: https://github.com/googlecolab/colabtools/issues/5081\n",
        "try:\n",
        "    from numba import config\n",
        "    config.CUDA_ENABLE_PYNVJITLINK = 1\n",
        "    config.CUDA_LOW_OCCUPANCY_WARNINGS = 0\n",
        "    print(\"✓ numba-cuda configuration set for CUDA PTX compatibility\")\n",
        "except ImportError:\n",
        "    print(\"⚠️  numba not available - CUDA configuration skipped\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️  Could not configure numba: {e}\")\n",
        "\n",
        "def check_gpu_availability():\n",
        "    \"\"\"Check if CUDA GPU is available.\"\"\"\n",
        "    try:\n",
        "        from numba import cuda\n",
        "        if cuda.is_available():\n",
        "            device = cuda.get_current_device()\n",
        "            print(f\"✓ CUDA GPU available: {device.name}\")\n",
        "            \n",
        "            # Try to get memory info using pynvml if available\n",
        "            try:\n",
        "                import pynvml\n",
        "                pynvml.nvmlInit()\n",
        "                handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "                total_memory = memory_info.total / 1024**3\n",
        "                print(f\"  Memory: {total_memory:.1f} GB\")\n",
        "            except (ImportError, Exception) as e:\n",
        "                # Fallback: just show device name without memory info\n",
        "                print(f\"  Device: {device.name}\")\n",
        "                print(f\"  (Memory info unavailable: {e})\")\n",
        "            \n",
        "            return True\n",
        "        else:\n",
        "            print(\"✗ CUDA GPU not available\")\n",
        "            return False\n",
        "    except ImportError:\n",
        "        print(\"✗ Numba CUDA not available\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def print_section_header(title: str):\n",
        "    \"\"\"Print formatted section header.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  {title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "def print_summary(sg_acc: float, cbow_acc: float, sg_stats: dict, cbow_stats: dict,\n",
        "                 sg_sem: float = None, sg_syn: float = None, \n",
        "                 cbow_sem: float = None, cbow_syn: float = None):\n",
        "    \"\"\"Print final summary of results.\"\"\"\n",
        "    print_section_header(\"FINAL SUMMARY\")\n",
        "    \n",
        "    print(f\"Model Performance:\")\n",
        "    if sg_acc is not None:\n",
        "        print(f\"  Skip-gram accuracy: {sg_acc:.4f} ({sg_acc*100:.2f}%)\")\n",
        "        if sg_sem is not None and sg_syn is not None:\n",
        "            print(f\"    - Semantic:  {sg_sem:.4f} ({sg_sem*100:.2f}%)\")\n",
        "            print(f\"    - Syntactic: {sg_syn:.4f} ({sg_syn*100:.2f}%)\")\n",
        "    if cbow_acc is not None:\n",
        "        print(f\"  CBOW accuracy: {cbow_acc:.4f} ({cbow_acc*100:.2f}%)\")\n",
        "        if cbow_sem is not None and cbow_syn is not None:\n",
        "            print(f\"    - Semantic:  {cbow_sem:.4f} ({cbow_sem*100:.2f}%)\")\n",
        "            print(f\"    - Syntactic: {cbow_syn:.4f} ({cbow_syn*100:.2f}%)\")\n",
        "    if sg_acc is not None and cbow_acc is not None:\n",
        "        print(f\"  Difference: {sg_acc - cbow_acc:.4f} ({(sg_acc - cbow_acc)*100:+.2f}%)\")\n",
        "    \n",
        "    has_stats = sg_stats or cbow_stats\n",
        "    if has_stats:\n",
        "        print(f\"\\nTraining Times:\")\n",
        "        if sg_stats:\n",
        "            sg_time = sg_stats.get('epoch_time_total_seconds', 0)\n",
        "            print(f\"  Skip-gram: {sg_time:.2f}s\")\n",
        "        if cbow_stats:\n",
        "            cbow_time = cbow_stats.get('epoch_time_total_seconds', 0)\n",
        "            print(f\"  CBOW: {cbow_time:.2f}s\")\n",
        "        if sg_stats and cbow_stats:\n",
        "            sg_time = sg_stats.get('epoch_time_total_seconds', 0)\n",
        "            cbow_time = cbow_stats.get('epoch_time_total_seconds', 0)\n",
        "            print(f\"  Difference: {sg_time - cbow_time:.2f}s\")\n",
        "        \n",
        "        print(f\"\\nData Processed:\")\n",
        "        stats = sg_stats if sg_stats else cbow_stats\n",
        "        if stats:\n",
        "            words = stats.get('word_count', 0)\n",
        "            print(f\"  Words: {words:,}\")\n",
        "            print(f\"  Sentences: {stats.get('sentence_count', 0):,}\")\n",
        "            print(f\"  Vocabulary: {stats.get('vocab_size', 0):,}\")\n",
        "    \n",
        "    print(f\"\\nOutput Files:\")\n",
        "    if sg_acc is not None:\n",
        "        print(f\"  Skip-gram vectors: ./output/vectors_skipgram\")\n",
        "        print(f\"  Skip-gram evaluation: ./output/skipgram_eval.json\")\n",
        "        print(f\"  Skip-gram statistics: ./output/vectors_skipgram_stats.json\")\n",
        "    if cbow_acc is not None:\n",
        "        print(f\"  CBOW vectors: ./output/vectors_cbow\")\n",
        "        print(f\"  CBOW evaluation: ./output/cbow_eval.json\")\n",
        "        print(f\"  CBOW statistics: ./output/vectors_cbow_stats.json\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main pipeline execution - NOTEBOOK VERSION.\"\"\"\n",
        "    # Config is set at the top of the file (no interactive menu needed)\n",
        "    \n",
        "    # Validate configuration\n",
        "    if use_hs_only and not use_hs:\n",
        "        use_hs = True  # Ensure consistency\n",
        "    \n",
        "    # Print configuration\n",
        "    print(f\"\\nDataset: {dataset_name}\")\n",
        "    if use_wmt14:\n",
        "        print(\"  - WMT14/WMT15 News Crawl (combines WMT14 2012-2013 + WMT15 2014)\")\n",
        "        print(\"  - Higher quality news articles\")\n",
        "        if max_words:\n",
        "            print(f\"  - Limited to {max_words:,} words ({max_words/1e6:.1f}M words)\")\n",
        "        elif max_sentences:\n",
        "            print(f\"  - Limited to {max_sentences:,} sentences\")\n",
        "    else:\n",
        "        print(\"  - Text8 (17M words, ~100MB)\")\n",
        "        print(\"  - Smaller, faster to download and process\")\n",
        "    \n",
        "    if use_hs_only:\n",
        "        print(\"  🎯 Training: Hierarchical Softmax ONLY (HS=1, k=0)\")\n",
        "    else:\n",
        "        print(\"  🎯 Training: Negative Sampling ONLY (HS=0, k=5)\")\n",
        "    \n",
        "    if not should_train_skipgram:\n",
        "        print(\"  ⏭️  Skip-gram training: Disabled\")\n",
        "    if not should_train_cbow:\n",
        "        print(\"  ⏭️  CBOW training: Disabled\")\n",
        "    \n",
        "    if use_phrases:\n",
        "        print(\"  🔗 Phrase detection: Enabled\")\n",
        "    \n",
        "    if use_gensim:\n",
        "        print(\"  📚 Gensim training: Enabled\")\n",
        "    \n",
        "    # 1. Setup & GPU Check\n",
        "    print_section_header(\"STEP 1: GPU AVAILABILITY CHECK\")\n",
        "    if not check_gpu_availability():\n",
        "        print(\"⚠️  Warning: No GPU detected. Training will be slow on CPU.\")\n",
        "        print(\"Continuing anyway...\")\n",
        "        # No input() in notebook - doesn't work well\n",
        "    \n",
        "    # 2. Download & Preprocess Data\n",
        "    print_section_header(f\"STEP 2: DOWNLOADING & PREPROCESSING {dataset_name.upper()}\")\n",
        "    data_dir = \"./data\"\n",
        "    \n",
        "    if use_phrases:\n",
        "        print(\"  🔗 Phrase detection: Enabled (will combine frequent bigrams)\")\n",
        "    \n",
        "    if use_wmt14:\n",
        "        news_file = download_wmt14_news(data_dir)\n",
        "        processed_dir = preprocess_wmt14_news(news_file, \"./data/wmt14_processed\", \n",
        "                                            max_sentences=max_sentences, max_files=max_files,\n",
        "                                            use_phrases=use_phrases)\n",
        "    else:\n",
        "        text8_file = download_text8(data_dir)\n",
        "        processed_dir = preprocess_text8(text8_file, \"./data/text8_processed\",\n",
        "                                        use_phrases=use_phrases)\n",
        "    \n",
        "    # Prepare training parameters (used by both models)\n",
        "    epochs_value = 1  # Set epochs here for consistency\n",
        "    base_params = {\n",
        "        \"epochs\": epochs_value,\n",
        "        \"embed_dim\": 600,\n",
        "        \"min_occurs\": 5,\n",
        "        \"c\": 5,\n",
        "        \"k\": 0 if use_hs_only else 5,\n",
        "        \"t\": 1e-5,\n",
        "        \"vocab_freq_exponent\": 0.75,\n",
        "        \"lr_max\": 0.025,\n",
        "        # For 1 epoch with large dataset, keep learning rate high (as in paper)\n",
        "        \"lr_min\": 0.025 if epochs_value == 1 else 0.0001,\n",
        "        \"cuda_threads_per_block\": 512,  # Optimized for A100 GPU\n",
        "        \"hs\": 1 if use_hs else 0,\n",
        "        \"max_words\": max_words  # Limit total words for training (None = no limit)\n",
        "    }\n",
        "    \n",
        "    # Build vocabulary once if training both models (to save time)\n",
        "    shared_vocab = None\n",
        "    shared_w_to_i = None\n",
        "    shared_word_counts = None\n",
        "    shared_ssw = None\n",
        "    shared_negs = None\n",
        "    \n",
        "    if should_train_skipgram and should_train_cbow:\n",
        "        print_section_header(\"STEP 3: BUILDING SHARED VOCABULARY\")\n",
        "        print(\"  ℹ️  Building vocabulary once for both Skip-gram and CBOW models\")\n",
        "        print(\"  ℹ️  Vocabulary will be cached for future runs (even with different epochs/dim)\")\n",
        "        from w2v_common import handle_vocab, get_subsampling_weights_and_negative_sampling_array\n",
        "        import time\n",
        "        start = time.time()\n",
        "        shared_vocab, shared_w_to_i, shared_word_counts = handle_vocab(\n",
        "            processed_dir, base_params[\"min_occurs\"], freq_exponent=base_params[\"vocab_freq_exponent\"], use_cache=True\n",
        "        )\n",
        "        shared_ssw, shared_negs = get_subsampling_weights_and_negative_sampling_array(shared_vocab, t=base_params[\"t\"])\n",
        "        vocab_size = len(shared_vocab)\n",
        "        build_time = time.time() - start\n",
        "        print(f\"  ✓ Vocabulary {'loaded from cache' if build_time < 1.0 else 'built'} in {build_time:.2f}s. Vocab size: {vocab_size:,}\")\n",
        "        print(f\"  ✓ Vocabulary will be reused for both models\\n\")\n",
        "    \n",
        "    # 3. Train Skip-gram (if selected)\n",
        "    if should_train_skipgram:\n",
        "        print_section_header(\"STEP 3: TRAINING SKIP-GRAM MODEL\")\n",
        "        skipgram_params = base_params.copy()\n",
        "        \n",
        "        if epochs_value == 1:\n",
        "            print(\"  ℹ️  Using 1 epoch: Learning rate will be kept constant at 0.025 (as per paper)\")\n",
        "        \n",
        "        print(\"Skip-gram parameters:\")\n",
        "        for key, value in skipgram_params.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "        \n",
        "        # Validate: HS and NS cannot be used together\n",
        "        if skipgram_params[\"hs\"] == 1 and skipgram_params[\"k\"] > 0:\n",
        "            raise ValueError(\"Error: Cannot use HS (hs=1) and Negative Sampling (k>0) together. Please choose either HS only (hs=1, k=0) or NS only (hs=0, k>0).\")\n",
        "        \n",
        "        # Pass shared vocabulary if available\n",
        "        if shared_vocab is not None:\n",
        "            train_skipgram(processed_dir, \"./output/vectors_skipgram\", \n",
        "                          vocab=shared_vocab, w_to_i=shared_w_to_i, word_counts=shared_word_counts,\n",
        "                          ssw=shared_ssw, negs=shared_negs, **skipgram_params)\n",
        "        else:\n",
        "            train_skipgram(processed_dir, \"./output/vectors_skipgram\", **skipgram_params)\n",
        "    else:\n",
        "        print_section_header(\"STEP 3: SKIPPING SKIP-GRAM TRAINING\")\n",
        "        print(\"  ⏭️  Skip-gram training skipped as requested\")\n",
        "        skipgram_params = base_params.copy()  # Still need params for CBOW if training both\n",
        "    \n",
        "    # 4. Train CBOW (if selected)\n",
        "    if should_train_cbow:\n",
        "        print_section_header(\"STEP 4: TRAINING CBOW MODEL\")\n",
        "        cbow_params = base_params.copy()\n",
        "        # CBOW uses same learning rate as Skip-gram (0.025) to prevent gradient explosion\n",
        "        cbow_params[\"lr_max\"] = 0.025\n",
        "        # For 1 epoch with large dataset, keep learning rate high (same as Skip-gram)\n",
        "        cbow_params[\"lr_min\"] = 0.025 if epochs_value == 1 else 0.0001\n",
        "        \n",
        "        if epochs_value == 1:\n",
        "            print(\"  ℹ️  Using 1 epoch: Learning rate will be kept constant at 0.025 (same as Skip-gram)\")\n",
        "        \n",
        "        print(\"CBOW parameters:\")\n",
        "        for key, value in cbow_params.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "        \n",
        "        # Validate: HS and NS cannot be used together\n",
        "        if cbow_params[\"hs\"] == 1 and cbow_params[\"k\"] > 0:\n",
        "            raise ValueError(\"Error: Cannot use HS (hs=1) and Negative Sampling (k>0) together. Please choose either HS only (hs=1, k=0) or NS only (hs=0, k>0).\")\n",
        "        \n",
        "        # Pass shared vocabulary if available\n",
        "        if shared_vocab is not None:\n",
        "            train_cbow(processed_dir, \"./output/vectors_cbow\",\n",
        "                      vocab=shared_vocab, w_to_i=shared_w_to_i, word_counts=shared_word_counts,\n",
        "                      ssw=shared_ssw, negs=shared_negs, **cbow_params)\n",
        "        else:\n",
        "            train_cbow(processed_dir, \"./output/vectors_cbow\", **cbow_params)\n",
        "    else:\n",
        "        print_section_header(\"STEP 4: SKIPPING CBOW TRAINING\")\n",
        "        print(\"  ⏭️  CBOW training skipped as requested\")\n",
        "    \n",
        "    # 5. Evaluate Skip-gram (if trained)\n",
        "    sg_result = None\n",
        "    sg_details = None\n",
        "    sg_sem = None\n",
        "    sg_syn = None\n",
        "    sg_total = None\n",
        "    sg_acc = None\n",
        "    sg_sim = None\n",
        "    \n",
        "    if should_train_skipgram:\n",
        "        print_section_header(\"STEP 5: EVALUATING SKIP-GRAM MODEL\")\n",
        "        sg_result, sg_details = word_analogy_test(\"./output/vectors_skipgram\")\n",
        "\n",
        "        sg_sem   = sg_result[\"semantic_accuracy\"]\n",
        "        sg_syn   = sg_result[\"syntactic_accuracy\"]\n",
        "        sg_total = sg_result[\"total_accuracy\"]\n",
        "        sg_acc   = sg_total  # Total accuracy for comparison functions\n",
        "\n",
        "        sg_sim = similarity_test(\"./output/vectors_skipgram\")\n",
        "\n",
        "        save_evaluation_results({\n",
        "            \"semantic_accuracy\": sg_sem,\n",
        "            \"syntactic_accuracy\": sg_syn,\n",
        "            \"total_accuracy\": sg_total,\n",
        "            \"details\": sg_details,\n",
        "            \"similarity_test\": sg_sim\n",
        "        }, \"./output/skipgram_eval.json\")\n",
        "    else:\n",
        "        print_section_header(\"STEP 5: SKIPPING SKIP-GRAM EVALUATION\")\n",
        "        print(\"  ⏭️  Skip-gram evaluation skipped (model not trained)\")\n",
        "\n",
        "    # 6. Evaluate CBOW (if trained)\n",
        "    cbow_result = None\n",
        "    cbow_details = None\n",
        "    cbow_sem = None\n",
        "    cbow_syn = None\n",
        "    cbow_total = None\n",
        "    cbow_acc = None\n",
        "    cbow_sim = None\n",
        "    \n",
        "    if should_train_cbow:\n",
        "        print_section_header(\"STEP 6: EVALUATING CBOW MODEL\")\n",
        "        cbow_result, cbow_details = word_analogy_test(\"./output/vectors_cbow\")\n",
        "\n",
        "        cbow_sem   = cbow_result[\"semantic_accuracy\"]\n",
        "        cbow_syn   = cbow_result[\"syntactic_accuracy\"]\n",
        "        cbow_total = cbow_result[\"total_accuracy\"]\n",
        "        cbow_acc   = cbow_total  # Total accuracy for comparison functions\n",
        "\n",
        "        cbow_sim = similarity_test(\"./output/vectors_cbow\")\n",
        "\n",
        "        save_evaluation_results({\n",
        "            \"semantic_accuracy\": cbow_sem,\n",
        "            \"syntactic_accuracy\": cbow_syn,\n",
        "            \"total_accuracy\": cbow_total,\n",
        "            \"details\": cbow_details,\n",
        "            \"similarity_test\": cbow_sim\n",
        "        }, \"./output/cbow_eval.json\")\n",
        "    else:\n",
        "        print_section_header(\"STEP 6: SKIPPING CBOW EVALUATION\")\n",
        "        print(\"  ⏭️  CBOW evaluation skipped (model not trained)\")\n",
        "    \n",
        "    # 7. Train Gensim Models (if enabled)\n",
        "    gensim_sg_path = None\n",
        "    gensim_cbow_path = None\n",
        "    gensim_sg_time = None\n",
        "    gensim_cbow_time = None\n",
        "    gensim_sg_acc = None\n",
        "    gensim_sg_details = None\n",
        "    gensim_cbow_acc = None\n",
        "    gensim_cbow_details = None\n",
        "    \n",
        "    if use_gensim:\n",
        "        print_section_header(\"STEP 7: TRAINING GENSIM MODELS\")\n",
        "        gensim_sg_path, gensim_cbow_path, gensim_sg_time, gensim_cbow_time = train_gensim_models(\n",
        "            processed_dir,\n",
        "            output_dir=\"./output/gensim\",\n",
        "            epochs=skipgram_params[\"epochs\"],\n",
        "            embed_dim=skipgram_params[\"embed_dim\"],\n",
        "            min_count=skipgram_params[\"min_occurs\"],\n",
        "            window=skipgram_params[\"c\"],\n",
        "            negative=skipgram_params[\"k\"],\n",
        "            hs=skipgram_params[\"hs\"],\n",
        "            alpha=skipgram_params[\"lr_max\"],\n",
        "            min_alpha=skipgram_params[\"lr_min\"]\n",
        "        )\n",
        "        \n",
        "        # 8. Evaluate Gensim Models\n",
        "        print_section_header(\"STEP 8: EVALUATING GENSIM MODELS\")\n",
        "        gensim_sg_acc, gensim_sg_details, gensim_cbow_acc, gensim_cbow_details = evaluate_gensim_models(\n",
        "            gensim_sg_path,\n",
        "            gensim_cbow_path,\n",
        "            output_dir=\"./output/gensim\"\n",
        "        )\n",
        "        \n",
        "        # 9. Compare with Gensim\n",
        "        print_section_header(\"STEP 9: COMPARING WITH GENSIM\")\n",
        "        # Load custom model statistics\n",
        "        custom_sg_time = None\n",
        "        custom_cbow_time = None\n",
        "        try:\n",
        "            import json\n",
        "            if should_train_skipgram:\n",
        "                try:\n",
        "                    with open(\"./output/vectors_skipgram_stats.json\", \"r\") as f:\n",
        "                        custom_sg_stats = json.load(f)\n",
        "                        custom_sg_time = custom_sg_stats.get(\"epoch_time_total_seconds\", None)\n",
        "                except FileNotFoundError:\n",
        "                    pass\n",
        "            if should_train_cbow:\n",
        "                try:\n",
        "                    with open(\"./output/vectors_cbow_stats.json\", \"r\") as f:\n",
        "                        custom_cbow_stats = json.load(f)\n",
        "                        custom_cbow_time = custom_cbow_stats.get(\"epoch_time_total_seconds\", None)\n",
        "                except FileNotFoundError:\n",
        "                    pass\n",
        "        except Exception:\n",
        "            pass\n",
        "        \n",
        "        gensim_comparison = compare_with_gensim(\n",
        "            \"./output/vectors_skipgram\" if should_train_skipgram else None,\n",
        "            \"./output/vectors_cbow\" if should_train_cbow else None,\n",
        "            gensim_sg_path,\n",
        "            gensim_cbow_path,\n",
        "            gensim_sg_time,\n",
        "            gensim_cbow_time,\n",
        "            gensim_sg_acc,\n",
        "            gensim_sg_details,\n",
        "            gensim_cbow_acc,\n",
        "            gensim_cbow_details,\n",
        "            custom_sg_time,\n",
        "            custom_cbow_time\n",
        "        )\n",
        "    \n",
        "    # 10. Model Comparison (Custom Skip-gram vs CBOW) - only if both trained\n",
        "    if should_train_skipgram and should_train_cbow:\n",
        "        step_num = 7 if not use_gensim else 10\n",
        "        print_section_header(f\"STEP {step_num}: COMPARING CUSTOM MODELS (Skip-gram vs CBOW)\")\n",
        "        # Pass pre-computed accuracy values to avoid re-evaluating\n",
        "        comparison = compare_models(\"./output/vectors_skipgram\", \"./output/vectors_cbow\",\n",
        "                                    sg_acc=sg_acc, sg_details=sg_details,\n",
        "                                    cbow_acc=cbow_acc, cbow_details=cbow_details)\n",
        "    else:\n",
        "        step_num = 7 if not use_gensim else 10\n",
        "        print_section_header(f\"STEP {step_num}: SKIPPING MODEL COMPARISON\")\n",
        "        if should_train_skipgram:\n",
        "            print(\"  ⏭️  Model comparison skipped (CBOW not trained)\")\n",
        "        elif should_train_cbow:\n",
        "            print(\"  ⏭️  Model comparison skipped (Skip-gram not trained)\")\n",
        "    \n",
        "    # Load statistics for summary\n",
        "    sg_stats = {}\n",
        "    cbow_stats = {}\n",
        "    \n",
        "    try:\n",
        "        import json\n",
        "        if should_train_skipgram:\n",
        "            try:\n",
        "                with open(\"./output/vectors_skipgram_stats.json\", \"r\") as f:\n",
        "                    sg_stats = json.load(f)\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "        if should_train_cbow:\n",
        "            try:\n",
        "                with open(\"./output/vectors_cbow_stats.json\", \"r\") as f:\n",
        "                    cbow_stats = json.load(f)\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "    except Exception:\n",
        "        print(\"Warning: Could not load statistics files\")\n",
        "    \n",
        "    # Final Summary\n",
        "    print_summary(sg_acc, cbow_acc, sg_stats, cbow_stats,\n",
        "                 sg_sem=sg_sem, sg_syn=sg_syn,\n",
        "                 cbow_sem=cbow_sem, cbow_syn=cbow_syn)\n",
        "    \n",
        "    print(f\"\\n🎉 Word2Vec training and evaluation completed successfully!\")\n",
        "    print(f\"📁 Check the ./output/ directory for all results.\")\n",
        "    \n",
        "    print(f\"\\nDataset used: {dataset_name}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # In notebook, don't use sys.exit() as it will cause SystemExit exception\n",
        "    try:\n",
        "        main()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n⚠️  Training interrupted by user.\")\n",
        "        raise  # Re-raise to show error in notebook\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        # Don't use sys.exit() in notebook - re-raise exception\n",
        "        raise  # Re-raise to show error in notebook\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
