{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FBpun1jxD1"
      },
      "source": [
        "# I. **Setup & Configuration**\n",
        "\n",
        "This cell performs the complete environment setup for the Word2Vec GPU-based implementation. The setup process consists of four main steps:\n",
        "\n",
        "## **Setup Steps**\n",
        "\n",
        "### **Step 1: Numba-CUDA Installation Check**\n",
        "- Verifies if `numba-cuda` (version 0.4.0) is already installed in the environment\n",
        "- If not installed, provides instructions for manual installation using `uv pip install`\n",
        "- This is a prerequisite for GPU-accelerated training\n",
        "\n",
        "### **Step 2: Numba-CUDA Configuration**\n",
        "- Configures numba-cuda settings for optimal performance in Google Colab environment\n",
        "- Enables `CUDA_ENABLE_PYNVJITLINK` to support CUDA 12.x compatibility\n",
        "- Disables low occupancy warnings for cleaner output\n",
        "- Performs a CUDA kernel test to verify functionality\n",
        "\n",
        "### **Step 3: Required Package Installation**\n",
        "Automatically installs all necessary Python packages for the Word2Vec implementation:\n",
        "\n",
        "- **`pynvjitlink-cu12`**: CUDA 12.x JIT linker support for numba-cuda\n",
        "- **`numpy>=1.20.0`**: Numerical computing library\n",
        "- **`gensim>=4.0.0`**: Word embedding evaluation and comparison tools\n",
        "- **`scikit-learn>=1.0.0`**: Machine learning utilities for evaluation\n",
        "- **`matplotlib>=3.5.0`**: Plotting and visualization library\n",
        "- **`seaborn>=0.11.0`**: Statistical data visualization\n",
        "- **`tqdm>=4.60.0`**: Progress bars for long-running operations\n",
        "- **`requests>=2.25.0`**: HTTP library for downloading datasets\n",
        "- **`pynvml>=11.0.0`**: NVIDIA Management Library for GPU monitoring\n",
        "\n",
        "The installation process uses `uv pip` (if available) or falls back to standard `pip`, with automatic error handling for each package.\n",
        "\n",
        "### **Step 4: GPU and CUDA Availability Check**\n",
        "- Verifies NVIDIA GPU presence using `nvidia-smi`\n",
        "- Checks CUDA availability through numba-cuda\n",
        "- Displays GPU device information and memory capacity\n",
        "- Provides clear status indicators for all checks\n",
        "\n",
        "## **Output**\n",
        "After execution, a summary report displays the status of all setup steps:\n",
        "- ✅ Green checkmarks indicate successful completion\n",
        "- ❌ Red X marks indicate failures or missing components\n",
        "- Warnings are provided for non-critical issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnZeOFcOd7FM",
        "outputId": "8f2d6a89-8f55-458d-af80-c78f8b6eb609"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package_with_uv(package: str, quiet: bool = True) -> bool:\n",
        "    try:\n",
        "        cmd = [\"uv\", \"pip\", \"install\"]\n",
        "        if quiet:\n",
        "            cmd.append(\"-q\")\n",
        "        cmd.extend([\"--system\", package])\n",
        "\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            capture_output=quiet,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "\n",
        "        if not quiet:\n",
        "            print(f\"✅ {package} installed successfully\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"❌ Failed to install {package}: {e}\")\n",
        "        if not quiet and e.stdout:\n",
        "            print(f\"stdout: {e.stdout}\")\n",
        "        if not quiet and e.stderr:\n",
        "            print(f\"stderr: {e.stderr}\")\n",
        "        return False\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"⚠️ uv not found, trying regular pip for {package}...\")\n",
        "        try:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
        "            if quiet:\n",
        "                cmd.append(\"-q\")\n",
        "            cmd.append(package)\n",
        "\n",
        "            subprocess.check_call(cmd)\n",
        "            if not quiet:\n",
        "                print(f\"✅ {package} installed successfully (via pip)\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"❌ Failed to install {package} with pip: {e2}\")\n",
        "            return False\n",
        "\n",
        "def check_numba_cuda_installed():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 1: Checking numba-cuda installation\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Checking if numba-cuda is installed...\")\n",
        "\n",
        "    try:\n",
        "        import numba\n",
        "        from numba import cuda\n",
        "        print(\"✅ numba-cuda is already installed\")\n",
        "\n",
        "        try:\n",
        "            import numba_cuda\n",
        "            print(f\"  numba version: {numba.__version__ if hasattr(numba, '__version__') else 'unknown'}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return True\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"❌ numba-cuda is NOT installed\")\n",
        "        print(\"⚠️ Please install manually first:\")\n",
        "        print(\"!uv pip install -q --system numba-cuda==0.4.0\")\n",
        "        return False\n",
        "\n",
        "def setup_numba_cuda_config():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 2: Configuring numba-cuda\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Setting up numba-cuda (Official Solution)\")\n",
        "    print(\"Based on: https://github.com/googlecolab/colabtools/issues/5081\")\n",
        "    print()\n",
        "\n",
        "    print(\"Configuring numba-cuda...\")\n",
        "    try:\n",
        "        from numba import config\n",
        "        config.CUDA_ENABLE_PYNVJITLINK = 1\n",
        "        config.CUDA_LOW_OCCUPANCY_WARNINGS = 0\n",
        "        print(\"✅ numba-cuda configuration set\")\n",
        "        print(\"  - CUDA_ENABLE_PYNVJITLINK = 1\")\n",
        "        print(\"  - CUDA_LOW_OCCUPANCY_WARNINGS = 0\")\n",
        "    except ImportError:\n",
        "        print(\"❌ numba not installed - cannot configure\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to configure numba-cuda: {e}\")\n",
        "        return False\n",
        "\n",
        "    print(\"\\nTesting CUDA functionality...\")\n",
        "    try:\n",
        "        from numba import cuda\n",
        "        import numpy as np\n",
        "\n",
        "        if cuda.is_available():\n",
        "            device = cuda.get_current_device()\n",
        "            print(f\"CUDA available: {device.name}\")\n",
        "\n",
        "            @cuda.jit\n",
        "            def increment_by_one(an_array):\n",
        "                pos = cuda.grid(1)\n",
        "                if pos < an_array.size:\n",
        "                    an_array[pos] += 1\n",
        "\n",
        "            test_array = np.zeros(10, dtype=np.float32)\n",
        "            increment_by_one[16, 16](test_array)\n",
        "\n",
        "            expected = np.ones(10, dtype=np.float32)\n",
        "            if np.allclose(test_array, expected):\n",
        "                print(\"✅ CUDA kernel test passed!\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"❌ CUDA kernel test failed\")\n",
        "                return False\n",
        "        else:\n",
        "            print(\"❌ CUDA not available\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CUDA test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def install_all_requirements():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 3: Installing all required packages\")\n",
        "    print(\"Installing required packages for Google Colab...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    packages = [\n",
        "        \"pynvjitlink-cu12\",\n",
        "        \"numpy>=1.20.0\",\n",
        "        \"gensim>=4.0.0\",\n",
        "        \"scikit-learn>=1.0.0\",\n",
        "        \"matplotlib>=3.5.0\",\n",
        "        \"seaborn>=0.11.0\",\n",
        "        \"tqdm>=4.60.0\",\n",
        "        \"requests>=2.25.0\",\n",
        "        \"pynvml>=11.0.0\"\n",
        "    ]\n",
        "\n",
        "    success_count = 0\n",
        "    failed_packages = []\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"Installing {package}...\", end=\" \", flush=True)\n",
        "        if install_package_with_uv(package, quiet=True):\n",
        "            print(\"✅\")\n",
        "            success_count += 1\n",
        "        else:\n",
        "            print(\"❌\")\n",
        "            failed_packages.append(package)\n",
        "\n",
        "    print(f\"\\nInstalled {success_count}/{len(packages)} packages successfully\")\n",
        "\n",
        "    if failed_packages:\n",
        "        print(f\"⚠️  Failed packages: {', '.join(failed_packages)}\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def check_gpu():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Checking GPU availability...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"✅ NVIDIA GPU detected:\")\n",
        "            print(result.stdout)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"❌ No NVIDIA GPU detected\")\n",
        "            return False\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"❌ nvidia-smi not found\")\n",
        "        return False\n",
        "\n",
        "def check_cuda():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Checking CUDA availability...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        from numba import cuda\n",
        "        if cuda.is_available():\n",
        "            device = cuda.get_current_device()\n",
        "            print(f\"✅ CUDA available: {device.name}\")\n",
        "\n",
        "            try:\n",
        "                import pynvml\n",
        "                pynvml.nvmlInit()\n",
        "                handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "                total_memory = memory_info.total / 1024**3\n",
        "                print(f\"  Memory: {total_memory:.1f} GB\")\n",
        "            except (ImportError, Exception) as e:\n",
        "                print(f\"  Device: {device.name}\")\n",
        "                print(f\"  (Memory info unavailable: {e})\")\n",
        "\n",
        "            return True\n",
        "        else:\n",
        "            print(\"❌ CUDA not available\")\n",
        "            return False\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"❌ Numba not installed\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"  Word2Vec Implementation - Complete Google Colab Setup\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nThis script combines all setup steps:\")\n",
        "    print(\"  1. Check numba-cuda installation\")\n",
        "    print(\"  2. Configure numba-cuda\")\n",
        "    print(\"  3. Install all required packages\")\n",
        "    print(\"  4. Check GPU and CUDA availability\")\n",
        "\n",
        "    results = {\n",
        "        \"numba_cuda_installed\": False,\n",
        "        \"numba_cuda_configured\": False,\n",
        "        \"requirements_installed\": False,\n",
        "        \"gpu_available\": False,\n",
        "        \"cuda_available\": False\n",
        "    }\n",
        "\n",
        "    results[\"numba_cuda_installed\"] = check_numba_cuda_installed()\n",
        "\n",
        "    if not results[\"numba_cuda_installed\"]:\n",
        "        print(\"\\n⚠️  Warning: numba-cuda is not installed. Please install it first:\")\n",
        "        print(\"   !uv pip install -q --system numba-cuda==0.4.0\")\n",
        "        print(\"   Continuing with other setup steps...\")\n",
        "\n",
        "    results[\"numba_cuda_configured\"] = setup_numba_cuda_config()\n",
        "\n",
        "    if not results[\"numba_cuda_configured\"]:\n",
        "        print(\"\\n⚠️  Warning: Failed to configure numba-cuda. Continuing anyway...\")\n",
        "\n",
        "    results[\"requirements_installed\"] = install_all_requirements()\n",
        "\n",
        "    results[\"gpu_available\"] = check_gpu()\n",
        "\n",
        "    results[\"cuda_available\"] = check_cuda()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"  SETUP SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  ✅ numba-cuda installed: {'✅' if results['numba_cuda_installed'] else '❌'}\")\n",
        "    print(f\"  ✅ numba-cuda configured: {'✅' if results['numba_cuda_configured'] else '❌'}\")\n",
        "    print(f\"  ✅ Requirements installed: {'✅' if results['requirements_installed'] else '❌'}\")\n",
        "    print(f\"  ✅ GPU available: {'✅' if results['gpu_available'] else '❌'}\")\n",
        "    print(f\"  ✅ CUDA available: {'✅' if results['cuda_available'] else '❌'}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if results['gpu_available'] and results['cuda_available']:\n",
        "        print(\"\\nSetup complete! Ready to run Word2Vec training\")\n",
        "        print(\"\\nTo run the full pipeline:\")\n",
        "        print(\"  !python run_all.py\")\n",
        "\n",
        "    elif results['numba_cuda_installed'] and results['numba_cuda_configured']:\n",
        "        print(\"\\nSetup completed successfully\")\n",
        "        print(\"⚠️ Note: GPU/CUDA may not be available, but CPU training is still possible\")\n",
        "        print(\"\\nTo run the full pipeline:\")\n",
        "        print(\"  !python run_all.py\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nSetup completed with some warnings\")\n",
        "        print(\"Some features may not work correctly\")\n",
        "        print(\"\\nTo run anyway:\")\n",
        "        print(\"  !python run_all.py\")\n",
        "\n",
        "    return 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZI-FINNkFma"
      },
      "source": [
        "# II. **Common Utilities**\n",
        "\n",
        "This cell contains core utility functions for Word2Vec implementation, handling vocabulary processing, data loading, weight initialization, and support structures for both Hierarchical Softmax and Negative Sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3lbJApckDPa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import time\n",
        "import hashlib\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "from numba import cuda\n",
        "import numpy as np\n",
        "from numpy import linalg, ndarray\n",
        "\n",
        "\n",
        "W2V_VERSION = \"1.0\"\n",
        "BLANK_TOKEN = \"<BLANK>\"\n",
        "\n",
        "# Constants for Hierarchical Softmax and Exp Table\n",
        "EXP_TABLE_SIZE = 1000\n",
        "MAX_EXP = 6\n",
        "MAX_CODE_LENGTH = 40\n",
        "\n",
        "\n",
        "def build_vocab(data_path: str) -> List[Tuple[str, int, int]]:\n",
        "    \"\"\"\n",
        "    Build vocabulary from data files -> Returns list of (word, total_count, sentence_count)\n",
        "    \"\"\"\n",
        "    files = [fn for fn in os.listdir(data_path) if fn.startswith(\"0\")]\n",
        "    sentences_per_word = defaultdict(int)\n",
        "    totals_per_word = defaultdict(int)\n",
        "\n",
        "    for file in files:\n",
        "        with open(os.path.join(data_path, file), encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                less_spacey = re.sub(r\"[ ]{2,}\", \" \", line.strip())\n",
        "                words = less_spacey.split(\" \")\n",
        "                if len(words) > 1:\n",
        "                    uniques = set()\n",
        "                    for word in words:\n",
        "                        uniques.add(word)\n",
        "                        totals_per_word[word] += 1\n",
        "                    for deduped in uniques:\n",
        "                        sentences_per_word[deduped] += 1\n",
        "\n",
        "    r = []\n",
        "    for word, total in totals_per_word.items():\n",
        "        sent = sentences_per_word[word]\n",
        "        r.append((word, total, sent))\n",
        "    return r\n",
        "\n",
        "\n",
        "def sort_vocab(my_vocab: List[Tuple[str, int, int]]) -> List[Tuple[str, int, int]]:\n",
        "    \"\"\"\n",
        "    Sort vocabulary by frequency (descending) then alphabetically\n",
        "    \"\"\"\n",
        "    vs = [(BLANK_TOKEN, 0, 0)] + sorted(my_vocab, key=lambda t: (-t[1], t[0]))\n",
        "    return vs\n",
        "\n",
        "\n",
        "def prune_vocab(min_occrs: int, my_vocab: List[Tuple[str, int, int]]) -> List[Tuple[str, int]]:\n",
        "    \"\"\"\n",
        "    Prune vocabulary based on minimum sentence occurrences -> Returns only total counts\n",
        "    \"\"\"\n",
        "    if min_occrs > 1:\n",
        "        totals = [(wrd, total_count) for wrd, total_count, sentence_count in my_vocab\n",
        "                 if sentence_count >= min_occrs or wrd == BLANK_TOKEN]\n",
        "        return totals\n",
        "    else:\n",
        "        return [(word, total) for word, total, _ in my_vocab]\n",
        "\n",
        "\n",
        "def bias_freq_counts(vocab: List[Tuple[str, int]], exponent: float) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Apply frequency biasing with given exponent for negative sampling\n",
        "    \"\"\"\n",
        "    totalsson = sum(count for _, count in vocab)\n",
        "    plain = [(word, count / totalsson) for word, count in vocab]\n",
        "\n",
        "    if exponent == 1.0:\n",
        "        return plain\n",
        "\n",
        "    exped = [(word, math.pow(count, exponent)) for word, count in plain]\n",
        "    sum_exped = sum([q for _, q in exped])\n",
        "    jooh = [(word, f/sum_exped) for word, f in exped]\n",
        "    return jooh\n",
        "\n",
        "\n",
        "def _get_vocab_cache_key(data_path: str, min_occurs_by_sentence: int, freq_exponent: float) -> str:\n",
        "    \"\"\"\n",
        "    Generate cache key based on vocabulary parameters\n",
        "    \"\"\"\n",
        "    key_string = f\"{data_path}_{min_occurs_by_sentence}_{freq_exponent}\"\n",
        "    return hashlib.md5(key_string.encode()).hexdigest()\n",
        "\n",
        "\n",
        "def _get_vocab_cache_path(cache_key: str) -> str:\n",
        "    \"\"\"\n",
        "    Get path to vocabulary cache file\n",
        "    \"\"\"\n",
        "    cache_dir = \"./output/vocab_cache\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    return os.path.join(cache_dir, f\"vocab_{cache_key}.pkl\")\n",
        "\n",
        "\n",
        "def _save_vocab_cache(vocab: List[Tuple[str, float]], w_to_i: Dict[str, int], word_counts: List[int], cache_path: str):\n",
        "    \"\"\"\n",
        "    Save vocabulary to cache file\n",
        "    \"\"\"\n",
        "    cache_data = {\n",
        "        'vocab': vocab,\n",
        "        'w_to_i': w_to_i,\n",
        "        'word_counts': word_counts\n",
        "    }\n",
        "    with open(cache_path, 'wb') as f:\n",
        "        pickle.dump(cache_data, f)\n",
        "\n",
        "\n",
        "def _load_vocab_cache(cache_path: str) -> Optional[Tuple[List[Tuple[str, float]], Dict[str, int], List[int]]]:\n",
        "    \"\"\"\n",
        "    Load vocabulary from cache file -> Returns None if cache doesn't exist or is invalid\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(cache_path):\n",
        "            return None\n",
        "        with open(cache_path, 'rb') as f:\n",
        "            cache_data = pickle.load(f)\n",
        "        return (cache_data['vocab'], cache_data['w_to_i'], cache_data['word_counts'])\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def handle_vocab(data_path: str, min_occurs_by_sentence: int, freq_exponent: float, use_cache: bool = True):\n",
        "    \"\"\"\n",
        "    Complete vocabulary handling pipeline with optional caching -> Returns: (biased_vocab, w_to_i, word_counts)\n",
        "    - biased_vocab: List of (word, frequency) for negative sampling\n",
        "    - w_to_i: Dictionary mapping word to index\n",
        "    - word_counts: List of word counts (for Huffman tree construction)\n",
        "\n",
        "    Args:\n",
        "        use_cache: If True, try to load from cache or save to cache after building\n",
        "                   Cache is based on data_path, min_occurs_by_sentence, and freq_exponent\n",
        "                   Changing epochs or embed_dim will not invalidate the cache\n",
        "    \"\"\"\n",
        "    # Try to load from cache\n",
        "    if use_cache:\n",
        "        cache_key = _get_vocab_cache_key(data_path, min_occurs_by_sentence, freq_exponent)\n",
        "        cache_path = _get_vocab_cache_path(cache_key)\n",
        "        cached_vocab = _load_vocab_cache(cache_path)\n",
        "        if cached_vocab is not None:\n",
        "            return cached_vocab\n",
        "\n",
        "    # Build vocabulary\n",
        "    vocab: List[Tuple[str, int, int]] = build_vocab(data_path)\n",
        "    sorted_vocab: List[Tuple[str, int, int]] = sort_vocab(vocab)\n",
        "    pruned_vocab: List[Tuple[str, int]] = prune_vocab(min_occurs_by_sentence, sorted_vocab)\n",
        "\n",
        "    # Store word counts before biasing\n",
        "    word_counts = [count for _, count in pruned_vocab]\n",
        "    biased_vocab: List[Tuple[str, float]] = bias_freq_counts(pruned_vocab, freq_exponent)\n",
        "    w_to_i: Dict[str, int] = {word: idx for idx, (word, _) in enumerate(biased_vocab)}\n",
        "\n",
        "    # Save to cache\n",
        "    if use_cache:\n",
        "        cache_key = _get_vocab_cache_key(data_path, min_occurs_by_sentence, freq_exponent)\n",
        "        cache_path = _get_vocab_cache_path(cache_key)\n",
        "        _save_vocab_cache(biased_vocab, w_to_i, word_counts, cache_path)\n",
        "\n",
        "    return biased_vocab, w_to_i, word_counts\n",
        "\n",
        "\n",
        "def get_subsampling_weights_and_negative_sampling_array(vocab: List[Tuple[str, float]], t: float) -> Tuple[ndarray, ndarray]:\n",
        "    \"\"\"\n",
        "    Calculate subsampling weights and create negative sampling array\n",
        "\n",
        "    Negative sampling array size is dynamically adjusted based on vocabulary size:\n",
        "    - For small vocabs (< 10k): uses 1M (original default)\n",
        "    - For medium vocabs (10k-100k): uses 10M\n",
        "    - For large vocabs (> 100k): uses 100M (same as word2vec.c original)\n",
        "\n",
        "    This ensures all words appear in the array and maintains distribution accuracy\n",
        "    \"\"\"\n",
        "    # Subsampling weights\n",
        "    tot_wgt: int = sum([c for _, c in vocab])\n",
        "    freqs: List[float] = [c/tot_wgt for _, c in vocab]\n",
        "\n",
        "    # Clamp negative probabilities to zero\n",
        "    probs: List[float] = [max(0.0, 1-math.sqrt(t/freq)) if freq > 0 else 0.0 for freq in freqs]\n",
        "\n",
        "    # Negative sampling array - precompute for efficient sampling\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Dynamically adjust arr_len based on vocabulary size\n",
        "    # Original source code of the Word2Vec paper uses 1e8 (100M), we scale based on vocab size\n",
        "    if vocab_size < 10000:\n",
        "        arr_len = 1000000  # 1M for small vocabs\n",
        "    elif vocab_size < 100000:\n",
        "        arr_len = 10000000  # 10M for medium vocabs\n",
        "    else:\n",
        "        arr_len = 100000000  # 100M for large vocabs (same as word2vec.c in original source code)\n",
        "\n",
        "    print(f\"Creating negative sampling array with size {arr_len:,} for vocab size {vocab_size:,}\")\n",
        "\n",
        "    w2 = [round(f*arr_len) for f in freqs]\n",
        "\n",
        "    # Check if any words would be excluded (rounded to 0)\n",
        "    excluded_count = sum(1 for scaled in w2 if scaled == 0)\n",
        "    if excluded_count > 0:\n",
        "        print(f\"⚠️ WARNING: {excluded_count} words have frequency too low and will be excluded from negative sampling\")\n",
        "        print(f\"⚠️ Consider increasing arr_len or reducing min_occurs threshold\")\n",
        "\n",
        "    neg_arr = []\n",
        "    for i, scaled in enumerate(w2):\n",
        "        if scaled > 0:  # Only add words that appear at least once\n",
        "            neg_arr.extend([i]*scaled)\n",
        "\n",
        "    actual_arr_size = len(neg_arr)\n",
        "    print(f\"Negative sampling array created: {actual_arr_size:,} entries ({actual_arr_size/1e6:.2f}M)\")\n",
        "\n",
        "    return np.asarray(probs, dtype=np.float32), np.asarray(neg_arr, dtype=np.int32)\n",
        "\n",
        "\n",
        "def get_data_file_names(path: str, seed: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Get shuffled list of data file names\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    qq = [fn for fn in os.listdir(path) if fn.startswith(\"0\")]\n",
        "    data_files = sorted(qq)\n",
        "    rng.shuffle(data_files)\n",
        "    return data_files\n",
        "\n",
        "\n",
        "def read_all_data_files_ever(dat_path: str, file_names: List[str], w_to_i: Dict[str, int],\n",
        "                             max_words: int = None) -> Tuple[List[int], List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Read all data files and convert to indices\n",
        "\n",
        "    Args:\n",
        "        dat_path: Path to data directory\n",
        "        file_names: List of file names to read\n",
        "        w_to_i: Word to index mapping\n",
        "        max_words: Maximum number of words to read (None = all). If specified, will stop reading when total words reach this limit.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (inps, offs, lens) where:\n",
        "        - inps: List of word indices\n",
        "        - offs: List of offsets for each sentence\n",
        "        - lens: List of sentence lengths\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    inps, offs, lens = [], [], []\n",
        "    offset_total = 0\n",
        "    stats = defaultdict(int)\n",
        "    total_words_read = 0\n",
        "    stopped_early = False\n",
        "\n",
        "    for fn in file_names:\n",
        "        fp = os.path.join(dat_path, fn)\n",
        "        ok_lines = 0\n",
        "        too_short_lines = 0\n",
        "        with open(fp, encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                # Check if we've reached max_words limit\n",
        "                if max_words is not None and total_words_read >= max_words:\n",
        "                    stopped_early = True\n",
        "                    break\n",
        "\n",
        "                words = [word for word in re.split(r\"[ .]+\", line.strip()) if word]\n",
        "                if len(words) < 2:\n",
        "                    too_short_lines += 1\n",
        "                    continue\n",
        "\n",
        "                idcs = [w_to_i[w] for w in words if w in w_to_i]\n",
        "                le = len(idcs)\n",
        "\n",
        "                # Check if adding this sentence would exceed max_words\n",
        "                if max_words is not None and total_words_read + le > max_words:\n",
        "                    # Only add words up to the limit\n",
        "                    remaining_words = max_words - total_words_read\n",
        "                    if remaining_words > 0:\n",
        "                        idcs = idcs[:remaining_words]\n",
        "                        le = len(idcs)\n",
        "                    else:\n",
        "                        stopped_early = True\n",
        "                        break\n",
        "\n",
        "                ok_lines += 1\n",
        "                offs.append(offset_total)\n",
        "                lens.append(le)\n",
        "                inps.extend(idcs)\n",
        "                offset_total += le\n",
        "                total_words_read += le\n",
        "\n",
        "                # Break if we've reached the limit exactly\n",
        "                if max_words is not None and total_words_read >= max_words:\n",
        "                    stopped_early = True\n",
        "                    break\n",
        "\n",
        "        stats[\"file_read_lines_ok\"] += ok_lines\n",
        "        stats[\"one_word_sentence_lines_which_were_ignored\"] += too_short_lines\n",
        "\n",
        "        # Break outer loop if we've reached the limit\n",
        "        if stopped_early:\n",
        "            break\n",
        "\n",
        "    print(f\"read_all_data_files_ever() STATS: {stats}\")\n",
        "    if max_words is not None and stopped_early:\n",
        "        print(f\"⚠️ Stopped early: reached max_words limit of {max_words:,} words\")\n",
        "    tot_tm = time.time()-start\n",
        "    print(f\"read_all_data_files_ever() Total time {tot_tm} s for {len(file_names)} files (avg {tot_tm/len(file_names)} s/file)\")\n",
        "    return inps, offs, lens\n",
        "\n",
        "\n",
        "def init_weight_matrices(vocab_size: int, embed_dim: int, seed: int) -> Tuple[ndarray, ndarray]:\n",
        "    \"\"\"\n",
        "    Initialize weight matrices with Gaussian distribution\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    rows, cols = vocab_size, embed_dim\n",
        "    sigma: float = math.sqrt(1.0/cols)\n",
        "    zs = rng.standard_normal(size=(rows, cols), dtype=np.float32)\n",
        "    xs = sigma * zs\n",
        "    # First row all zero since it represents the blank token\n",
        "    xs[0, :] = 0.0\n",
        "    zs2 = rng.standard_normal(size=(rows, cols), dtype=np.float32)\n",
        "    xs2 = sigma * zs2\n",
        "    xs2[0, :] = 0.0\n",
        "    return xs, xs2\n",
        "\n",
        "\n",
        "def print_norms(weights_cuda):\n",
        "    \"\"\"\n",
        "    Print statistics about vector norms\n",
        "    \"\"\"\n",
        "    w = weights_cuda.copy_to_host()\n",
        "    norms = [linalg.norm(v) for v in w]\n",
        "    a, med, b = np.percentile(norms, [2.5, 50, 97.5])\n",
        "    avg = float(sum(norms) / len(norms))\n",
        "    print(f\"Vector norms (count {len(norms)}) 2.5% median mean 97.5%: {a:0.4f}  {med:0.4f}  {avg:0.4f}  {b:0.4f}\")\n",
        "\n",
        "\n",
        "def write_vectors(weights_cuda, vocab: List[Tuple[str, float]], out_path: str):\n",
        "    \"\"\"\n",
        "    Write vectors to file in word2vec format\n",
        "    \"\"\"\n",
        "    w = weights_cuda.copy_to_host()\n",
        "    pathlib.Path(os.path.dirname(out_path)).mkdir(parents=True, exist_ok=True)\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        # len-1: skip first which is the blank token & all zero\n",
        "        f.write(f\"{len(w)-1} {len(w[0])}\\n\")\n",
        "        for i, v in enumerate(w):\n",
        "            # skip first which is the blank token & all zero\n",
        "            if i == 0:\n",
        "                continue\n",
        "            v_str = \" \".join([str(f) for f in v])\n",
        "            word, _ = vocab[i]\n",
        "            f.write(f\"{word} {v_str}\\n\")\n",
        "\n",
        "\n",
        "def write_json(to_jsonify: Dict[str, Any], json_path: str):\n",
        "    \"\"\"\n",
        "    Write dictionary to JSON file\n",
        "    \"\"\"\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(to_jsonify))\n",
        "        f.write(\"\\n\")\n",
        "        f.flush()\n",
        "\n",
        "\n",
        "def create_exp_table(exp_table_size: int = EXP_TABLE_SIZE, max_exp: float = MAX_EXP) -> ndarray:\n",
        "    \"\"\"\n",
        "    Create precomputed exp table for fast sigmoid calculation\n",
        "    Based on word2vec.c from the original source code of the Word2Vec paper\n",
        "\n",
        "    Args:\n",
        "        exp_table_size: Size of the exp table (default: 1000)\n",
        "        max_exp: Maximum exponent value (default: 6)\n",
        "\n",
        "    Returns:\n",
        "        numpy array of precomputed sigmoid values\n",
        "    \"\"\"\n",
        "    exp_table = np.zeros(exp_table_size, dtype=np.float32)\n",
        "    for i in range(exp_table_size):\n",
        "        # Precompute exp((i / exp_table_size * 2 - 1) * max_exp)\n",
        "        exp_value = math.exp((i / exp_table_size * 2 - 1) * max_exp)\n",
        "        # Precompute sigmoid: exp(x) / (exp(x) + 1)\n",
        "        exp_table[i] = exp_value / (exp_value + 1)\n",
        "    return exp_table\n",
        "\n",
        "\n",
        "def init_hs_weight_matrix(vocab_size: int, embed_dim: int) -> ndarray:\n",
        "    \"\"\"\n",
        "    Initialize Hierarchical Softmax weight matrix (syn1)\n",
        "    Based on word2vec.c from the original source code of the Word2Vec paper\n",
        "\n",
        "    Args:\n",
        "        vocab_size: Vocabulary size\n",
        "        embed_dim: Embedding dimension\n",
        "\n",
        "    Returns:\n",
        "        Weight matrix for internal nodes: (vocab_size - 1, embed_dim)\n",
        "        Initialized with zeros\n",
        "    \"\"\"\n",
        "    # Internal nodes: vocab_size - 1\n",
        "    syn1 = np.zeros((vocab_size - 1, embed_dim), dtype=np.float32)\n",
        "    return syn1\n",
        "\n",
        "\n",
        "def create_huffman_tree(word_counts: List[int], max_code_length: int = MAX_CODE_LENGTH) -> Tuple[ndarray, ndarray, ndarray]:\n",
        "    \"\"\"\n",
        "    Create binary Huffman tree from word counts\n",
        "    Based on word2vec.c from the original source code of the Word2Vec paper\n",
        "    Frequent words will have short unique binary codes\n",
        "\n",
        "    Args:\n",
        "        word_counts: List of word counts (frequencies)\n",
        "        max_code_length: Maximum code length (default: 40)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (codes_array, points_array, code_lengths):\n",
        "        - codes_array: (vocab_size, max_code_length) binary codes, padded with -1\n",
        "        - points_array: (vocab_size, max_code_length) node indices in path, padded with -1\n",
        "        - code_lengths: (vocab_size,) code length for each word\n",
        "    \"\"\"\n",
        "    vocab_size = len(word_counts)\n",
        "\n",
        "    # Initialize arrays\n",
        "    count = np.zeros(vocab_size * 2 + 1, dtype=np.int64)\n",
        "    binary = np.zeros(vocab_size * 2 + 1, dtype=np.int32)\n",
        "    parent_node = np.zeros(vocab_size * 2 + 1, dtype=np.int64)\n",
        "\n",
        "    # Set initial counts\n",
        "    for a in range(vocab_size):\n",
        "        count[a] = word_counts[a]\n",
        "    for a in range(vocab_size, vocab_size * 2):\n",
        "        count[a] = int(1e15)  # Large value for internal nodes\n",
        "\n",
        "    # Build Huffman tree\n",
        "    pos1 = vocab_size - 1\n",
        "    pos2 = vocab_size\n",
        "\n",
        "    for a in range(vocab_size - 1):\n",
        "        # Find two smallest nodes\n",
        "        if pos1 >= 0:\n",
        "            if count[pos1] < count[pos2]:\n",
        "                min1i = pos1\n",
        "                pos1 -= 1\n",
        "            else:\n",
        "                min1i = pos2\n",
        "                pos2 += 1\n",
        "        else:\n",
        "            min1i = pos2\n",
        "            pos2 += 1\n",
        "\n",
        "        if pos1 >= 0:\n",
        "            if count[pos1] < count[pos2]:\n",
        "                min2i = pos1\n",
        "                pos1 -= 1\n",
        "            else:\n",
        "                min2i = pos2\n",
        "                pos2 += 1\n",
        "        else:\n",
        "            min2i = pos2\n",
        "            pos2 += 1\n",
        "\n",
        "        count[vocab_size + a] = count[min1i] + count[min2i]\n",
        "        parent_node[min1i] = vocab_size + a\n",
        "        parent_node[min2i] = vocab_size + a\n",
        "        binary[min2i] = 1\n",
        "\n",
        "    # Assign binary codes to each word\n",
        "    codes_array = np.full((vocab_size, max_code_length), -1, dtype=np.int32)\n",
        "    points_array = np.full((vocab_size, max_code_length), -1, dtype=np.int32)\n",
        "    code_lengths = np.zeros(vocab_size, dtype=np.int32)\n",
        "\n",
        "    for a in range(vocab_size):\n",
        "        b = a\n",
        "        i = 0\n",
        "        code = np.zeros(max_code_length, dtype=np.int32)\n",
        "        point = np.zeros(max_code_length, dtype=np.int64)\n",
        "\n",
        "        # Traverse from leaf to root\n",
        "        while True:\n",
        "            code[i] = binary[b]\n",
        "            point[i] = b\n",
        "            i += 1\n",
        "            b = parent_node[b]\n",
        "            if b == vocab_size * 2 - 2:\n",
        "                break\n",
        "            if i >= max_code_length:\n",
        "                break  # Safety check\n",
        "\n",
        "        code_lengths[a] = i\n",
        "        # Store code and point arrays (reversed)\n",
        "        points_array[a, 0] = vocab_size - 2  # Root node\n",
        "        for b_idx in range(i):\n",
        "            codes_array[a, i - b_idx - 1] = code[b_idx]\n",
        "            if b_idx < i - 1:\n",
        "                points_array[a, i - b_idx] = int(point[b_idx] - vocab_size)\n",
        "\n",
        "    return codes_array, points_array, code_lengths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5KqsC97lmeI"
      },
      "source": [
        "# III. **Data Handler**\n",
        "\n",
        "This cell contains functions for downloading, preprocessing, and loading training datasets. It handles text cleaning, phrase detection, and data format conversion for Word2Vec training.\n",
        "\n",
        "## **Dataset Download**\n",
        "\n",
        "- **`download_wmt14_news()`**: Downloads and combines WMT14 (2012) and WMT15 (2014) News Crawl datasets from https://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2012.en.shuffled.gz https://www.statmt.org/wmt15/training-monolingual-news-crawl/news.2014.en.shuffled.gz\n",
        "  - Downloads compressed files, extracts, and combines into single file\n",
        "  - Skips download if files already exist\n",
        "  - Returns path to combined file\n",
        "- **`download_text8()`**: Downloads text8 dataset from http://mattmahoney.net/dc/text8.zip\n",
        "  - Extracts and returns path to text8 file\n",
        "\n",
        "## **Data Preprocessing**\n",
        "\n",
        "- **`preprocess_wmt14_news()`**: Preprocesses WMT14 news file into sentence files\n",
        "  - Removes punctuation, normalizes text\n",
        "  - Splits into sentences (default: 1000 words per sentence)\n",
        "  - Saves to numbered files (100K sentences per file)\n",
        "  - Optional phrase detection (2-pass)\n",
        "  - Supports `max_sentences` and `max_files` limits\n",
        "- **`preprocess_text8()`**: Preprocesses text8 file into sentence files\n",
        "  - Similar to WMT14 preprocessing\n",
        "  - Optional phrase detection\n",
        "\n",
        "## **Data Loading**\n",
        "\n",
        "- **`get_data_file_names()`**: Retrieves and shuffles data file names (seeded for reproducibility)\n",
        "- **`read_all_data_files()`**: Reads data files and converts words to integer indices\n",
        "  - Returns: `(inps, offs, lens)` - word indices, sentence offsets, sentence lengths\n",
        "  - Filters out sentences with < 2 words\n",
        "  - Provides statistics on processed lines\n",
        "\n",
        "## **Data Preparation Pipeline**\n",
        "\n",
        "1. **Download**: Use `download_wmt14_news()` or `download_text8()` to get raw data\n",
        "2. **Preprocess**: Use `preprocess_wmt14_news()` or `preprocess_text8()` to create sentence files\n",
        "3. **Load**: Use `get_data_file_names()` and `read_all_data_files()` to load for training\n",
        "\n",
        "## **Notes**\n",
        "\n",
        "- All preprocessing removes punctuation (commas, periods, etc.) for consistency\n",
        "- Phrase detection is optional but improves quality for multi-word expressions\n",
        "- Processed files are cached - existing files are detected and skipped\n",
        "- Data files are named with 4-digit numbers (e.g., \"0000\", \"0001\") starting with \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgycGerWlp4P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import time\n",
        "import zipfile\n",
        "import gzip\n",
        "import json\n",
        "from typing import List, Tuple, Dict\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "\n",
        "def clean_text_remove_punctuation(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean text by removing punctuation and normalizing whitespace\n",
        "\n",
        "    Args:\n",
        "        text: Input text line\n",
        "\n",
        "    Returns:\n",
        "        Cleaned text with only lowercase letters and spaces\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Replace tabs and newlines with spaces\n",
        "    text = re.sub(r'[\\t\\n]', ' ', text)\n",
        "\n",
        "    # Normalize multiple spaces to single space\n",
        "    text = re.sub(r'[ ]{2,}', ' ', text)\n",
        "\n",
        "    # Remove all punctuation, keep only letters and spaces\n",
        "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "\n",
        "    # Convert to lowercase and strip\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def detect_phrases(text: str, word_counts: Dict[str, int], bigram_counts: Dict[Tuple[str, str], int],\n",
        "                   train_words: int, min_count: int = 5, threshold: float = 100.0) -> str:\n",
        "    \"\"\"\n",
        "    Detect and combine phrases in text based on bigram scores\n",
        "    Based on word2phrase.c TrainModel() function from the original source code of the Word2Vec paper\n",
        "\n",
        "    Args:\n",
        "        text: Input text (space-separated words)\n",
        "        word_counts: Dictionary mapping words to their counts\n",
        "        bigram_counts: Dictionary mapping (word1, word2) tuples to bigram counts\n",
        "        train_words: Total number of words in training data\n",
        "        min_count: Minimum word count threshold\n",
        "        threshold: Score threshold for phrase formation (higher = fewer phrases)\n",
        "\n",
        "    Returns:\n",
        "        Text with phrases combined (e.g., \"new york\" -> \"new_york\")\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) < 2:\n",
        "        return text\n",
        "\n",
        "    result = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        if i == len(words) - 1:\n",
        "            # Last word, no bigram possible\n",
        "            result.append(words[i])\n",
        "            break\n",
        "\n",
        "        word1 = words[i]\n",
        "        word2 = words[i + 1]\n",
        "\n",
        "        # Check if both words meet min_count\n",
        "        count1 = word_counts.get(word1, 0)\n",
        "        count2 = word_counts.get(word2, 0)\n",
        "\n",
        "        if count1 < min_count or count2 < min_count:\n",
        "            # One word doesn't meet threshold, keep as separate\n",
        "            result.append(word1)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        bigram = (word1, word2)\n",
        "        count_bigram = bigram_counts.get(bigram, 0)\n",
        "\n",
        "        if count_bigram == 0:\n",
        "            # Bigram not found, keep as separate\n",
        "            result.append(word1)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # score = (pab - min_count) / pa / pb * train_words (Score formula from word2phrase.c)\n",
        "        score = (count_bigram - min_count) / count1 / count2 * train_words\n",
        "\n",
        "        if score > threshold:\n",
        "            # Combine into phrase\n",
        "            result.append(f\"{word1}_{word2}\")\n",
        "            i += 2  # Skip both words\n",
        "        else:\n",
        "            # Keep as separate\n",
        "            result.append(word1)\n",
        "            i += 1\n",
        "\n",
        "    return \" \".join(result)\n",
        "\n",
        "\n",
        "def learn_phrase_vocab(data_path: str, min_count: int = 5) -> Tuple[Dict[str, int], Dict[Tuple[str, str], int], int]:\n",
        "    \"\"\"\n",
        "    Learn vocabulary and bigram counts from training data\n",
        "    Based on word2phrase.c LearnVocabFromTrainFile() function from the original source code of the Word2Vec paper\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to training data directory\n",
        "        min_count: Minimum word count threshold\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (word_counts, bigram_counts, total_words)\n",
        "    \"\"\"\n",
        "    word_counts = defaultdict(int)\n",
        "    bigram_counts = defaultdict(int)\n",
        "    total_words = 0\n",
        "\n",
        "    data_files = [f for f in os.listdir(data_path) if f.startswith(\"0\")]\n",
        "    data_files.sort()\n",
        "\n",
        "    print(f\"Learning phrase vocabulary from {len(data_files)} files...\")\n",
        "\n",
        "    for file_idx, filename in enumerate(data_files):\n",
        "        filepath = os.path.join(data_path, filename)\n",
        "        last_word = None\n",
        "        start = True\n",
        "\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    start = True\n",
        "                    last_word = None\n",
        "                    continue\n",
        "\n",
        "                words = line.split()\n",
        "                for word in words:\n",
        "                    word = word.lower().strip()\n",
        "                    if not word:\n",
        "                        continue\n",
        "\n",
        "                    total_words += 1\n",
        "\n",
        "                    # Count unigram\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "                    # Count bigram (if not at start of sentence)\n",
        "                    if not start and last_word:\n",
        "                        bigram = (last_word, word)\n",
        "                        bigram_counts[bigram] += 1\n",
        "\n",
        "                    last_word = word\n",
        "                    start = False\n",
        "\n",
        "                # Reset at end of line\n",
        "                start = True\n",
        "                last_word = None\n",
        "\n",
        "        if (file_idx + 1) % 10 == 0:\n",
        "            print(f\"  Processed {file_idx + 1}/{len(data_files)} files...\")\n",
        "\n",
        "    filtered_word_counts = {w: c for w, c in word_counts.items() if c >= min_count}\n",
        "\n",
        "    print(f\"Vocabulary: {len(filtered_word_counts):,} words (min_count={min_count})\")\n",
        "    print(f\"Bigrams: {len(bigram_counts):,} unique bigrams\")\n",
        "    print(f\"Total words: {total_words:,}\")\n",
        "\n",
        "    return filtered_word_counts, bigram_counts, total_words\n",
        "\n",
        "\n",
        "def apply_phrases_to_data(data_path: str, output_path: str, word_counts: Dict[str, int],\n",
        "                          bigram_counts: Dict[Tuple[str, str], int], train_words: int,\n",
        "                          min_count: int = 5, threshold: float = 100.0) -> str:\n",
        "    \"\"\"\n",
        "    Apply phrase detection to all data files\n",
        "\n",
        "    Args:\n",
        "        data_path: Input data directory\n",
        "        output_path: Output data directory\n",
        "        word_counts: Word count dictionary\n",
        "        bigram_counts: Bigram count dictionary\n",
        "        train_words: Total number of words\n",
        "        min_count: Minimum word count\n",
        "        threshold: Phrase score threshold\n",
        "\n",
        "    Returns:\n",
        "        Path to output directory\n",
        "    \"\"\"\n",
        "    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    data_files = [f for f in os.listdir(data_path) if f.startswith(\"0\")]\n",
        "    data_files.sort()\n",
        "\n",
        "    print(f\"Applying phrase detection (threshold={threshold}) to {len(data_files)} files...\")\n",
        "\n",
        "    for file_idx, filename in enumerate(data_files):\n",
        "        input_filepath = os.path.join(data_path, filename)\n",
        "        output_filepath = os.path.join(output_path, filename)\n",
        "\n",
        "        with open(input_filepath, 'r', encoding='utf-8') as fin, \\\n",
        "             open(output_filepath, 'w', encoding='utf-8') as fout:\n",
        "\n",
        "            for line in fin:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    fout.write('\\n')\n",
        "                    continue\n",
        "\n",
        "                # Apply phrase detection\n",
        "                processed_line = detect_phrases(line, word_counts, bigram_counts,\n",
        "                                                train_words, min_count, threshold)\n",
        "                fout.write(processed_line + '\\n')\n",
        "\n",
        "        if (file_idx + 1) % 10 == 0:\n",
        "            print(f\"  Processed {file_idx + 1}/{len(data_files)} files...\")\n",
        "\n",
        "    print(f\"Phrase detection complete. Output: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def preprocess_with_phrases(data_path: str, output_path: str, min_count: int = 5,\n",
        "                            threshold1: float = 200.0, threshold2: float = 100.0) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess data with phrase detection (2 passes, like word2phrase.c from the original source code)\n",
        "\n",
        "    Args:\n",
        "        data_path: Input data directory\n",
        "        output_path: Final output directory\n",
        "        min_count: Minimum word count\n",
        "        threshold1: First pass threshold (higher, fewer phrases)\n",
        "        threshold2: Second pass threshold (lower, more phrases)\n",
        "\n",
        "    Returns:\n",
        "        Path to final output directory\n",
        "    \"\"\"\n",
        "    print(f\"Preprocessing with phrase detection...\")\n",
        "    print(f\" -Input: {data_path}\")\n",
        "    print(f\" -Output: {output_path}\")\n",
        "    print(f\" -Threshold 1: {threshold1} (first pass)\")\n",
        "    print(f\" -Threshold 2: {threshold2} (second pass)\")\n",
        "\n",
        "    print(\"\\nStep 1: Learning vocabulary and bigram counts...\")\n",
        "    word_counts, bigram_counts, train_words = learn_phrase_vocab(data_path, min_count)\n",
        "\n",
        "    print(f\"\\nStep 2: First pass phrase detection (threshold={threshold1})...\")\n",
        "    temp_path1 = output_path + \"_phrase1\"\n",
        "    apply_phrases_to_data(data_path, temp_path1, word_counts, bigram_counts,\n",
        "                          train_words, min_count, threshold1)\n",
        "\n",
        "    print(\"\\nStep 3: Relearning vocabulary from first pass...\")\n",
        "    word_counts2, bigram_counts2, train_words2 = learn_phrase_vocab(temp_path1, min_count)\n",
        "\n",
        "    print(f\"\\nStep 4: Second pass phrase detection (threshold={threshold2})...\")\n",
        "    apply_phrases_to_data(temp_path1, output_path, word_counts2, bigram_counts2,\n",
        "                          train_words2, min_count, threshold2)\n",
        "\n",
        "    import shutil\n",
        "    if os.path.exists(temp_path1):\n",
        "        shutil.rmtree(temp_path1)\n",
        "        print(f\"Cleaned up temporary directory: {temp_path1}\")\n",
        "\n",
        "    print(f\"\\nPhrase preprocessing complete: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def download_wmt14_news(output_dir: str = \"./data\") -> str:\n",
        "    \"\"\"\n",
        "    Download and combine multiple years of WMT14 and WMT15 News Crawl datasets\n",
        "    Downloads WMT14 year 2012 and WMT15 year 2014, combines them into a single file\n",
        "    Returns path to combined news file\n",
        "    \"\"\"\n",
        "    datasets = [\n",
        "        (\"WMT14\", 2012, \"http://www.statmt.org/wmt14/training-monolingual-news-crawl\"),\n",
        "        (\"WMT15\", 2014, \"https://www.statmt.org/wmt15/training-monolingual-news-crawl\"),\n",
        "    ]\n",
        "\n",
        "    output_path = os.path.join(output_dir, \"wmt14\")\n",
        "    combined_file = os.path.join(output_path, \"news.combined.en.shuffled\")\n",
        "\n",
        "    # Create output directory\n",
        "    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Check if combined file already exists\n",
        "    if os.path.isfile(combined_file):\n",
        "        print(f\"WMT14/WMT15 News combined file already exists at: {combined_file}\")\n",
        "        return combined_file\n",
        "\n",
        "    # Download and extract each dataset\n",
        "    downloaded_files = []\n",
        "    for wmt_version, year, base_url in datasets:\n",
        "        train_file = f\"news.{year}.en.shuffled\"\n",
        "        train_gz = f\"{train_file}.gz\"\n",
        "        train_url = f\"{base_url}/{train_gz}\"\n",
        "        news_file = os.path.join(output_path, train_file)\n",
        "        gz_path = os.path.join(output_path, train_gz)\n",
        "\n",
        "        # Check if already extracted\n",
        "        if os.path.isfile(news_file):\n",
        "            print(f\"{wmt_version} News {year} already exists at: {news_file}\")\n",
        "            downloaded_files.append(news_file)\n",
        "            continue\n",
        "\n",
        "        # Download if missing\n",
        "        if not os.path.isfile(gz_path):\n",
        "            print(f\"Downloading {wmt_version} News {year} ({train_gz})...\")\n",
        "            try:\n",
        "                with requests.get(train_url, stream=True, timeout=30) as response:\n",
        "                    response.raise_for_status()\n",
        "                    total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "                    with open(gz_path, 'wb') as f:\n",
        "                        with tqdm.tqdm(total=total_size, unit='B', unit_scale=True,\n",
        "                                     desc=f\"Downloading {year}\") as pbar:\n",
        "                            for chunk in response.iter_content(chunk_size=8192):\n",
        "                                if chunk:\n",
        "                                    f.write(chunk)\n",
        "                                    pbar.update(len(chunk))\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"⚠️ Warning: Could not download {train_url}: {e}\")\n",
        "                print(f\"⚠️ Skipping {wmt_version} year {year}\")\n",
        "                continue\n",
        "\n",
        "        # Extract if needed\n",
        "        if os.path.isfile(gz_path) and not os.path.isfile(news_file):\n",
        "            print(f\"Extracting {gz_path}...\")\n",
        "            try:\n",
        "                with gzip.open(gz_path, \"rb\") as source, open(news_file, \"wb\") as target:\n",
        "                    target.write(source.read())\n",
        "                downloaded_files.append(news_file)\n",
        "                print(f\"✅ Extracted {train_file}\")\n",
        "                # Remove gz file to save space\n",
        "                os.remove(gz_path)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error extracting {gz_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "    if not downloaded_files:\n",
        "        raise FileNotFoundError(\"No WMT14/WMT15 News files were successfully downloaded\")\n",
        "\n",
        "    # Combine all downloaded files into one\n",
        "    print(f\"\\nCombining {len(downloaded_files)} WMT14/WMT15 News files into: {combined_file}\")\n",
        "    total_lines = 0\n",
        "\n",
        "    with open(combined_file, 'w', encoding='utf-8') as outfile:\n",
        "        for i, news_file in enumerate(downloaded_files):\n",
        "            if not os.path.isfile(news_file):\n",
        "                print(f\"⚠️ Warning: {news_file} not found, skipping\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Adding file {i+1}/{len(downloaded_files)}: {os.path.basename(news_file)}\")\n",
        "            line_count = 0\n",
        "\n",
        "            with open(news_file, 'r', encoding='utf-8') as infile:\n",
        "                for line in infile:\n",
        "                    cleaned = line.strip()\n",
        "                    if cleaned:  # Skip empty lines\n",
        "                        outfile.write(cleaned + '\\n')\n",
        "                        line_count += 1\n",
        "                        total_lines += 1\n",
        "\n",
        "            print(f\"Added {line_count:,} lines\")\n",
        "\n",
        "    # Get file size\n",
        "    file_size = os.path.getsize(combined_file) / (1024**3)  # GB\n",
        "\n",
        "    print(f\"\\n✅ Combined WMT14/WMT15 News dataset created:\")\n",
        "    print(f\" -File: {combined_file}\")\n",
        "    print(f\" -Total lines: {total_lines:,}\")\n",
        "    print(f\" -Size: {file_size:.2f} GB\")\n",
        "    print(f\" -Estimated words: ~{total_lines * 20:,} (assuming ~20 words/line)\")\n",
        "\n",
        "    return combined_file\n",
        "\n",
        "\n",
        "def download_text8(output_dir: str = \"./data\") -> str:\n",
        "    \"\"\"\n",
        "    Download text8 dataset from http://mattmahoney.net/dc/text8.zip\n",
        "    Returns path to downloaded text8 file\n",
        "    \"\"\"\n",
        "    url = \"http://mattmahoney.net/dc/text8.zip\"\n",
        "    output_path = os.path.join(output_dir, \"text8\")\n",
        "    text8_file = os.path.join(output_path, \"text8\")\n",
        "\n",
        "    # Create output directory\n",
        "    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Check if already exists\n",
        "    if os.path.isfile(text8_file):\n",
        "        print(f\"Text8 file already exists at: {text8_file}\")\n",
        "        return text8_file\n",
        "\n",
        "    zip_path = os.path.join(output_path, \"text8.zip\")\n",
        "\n",
        "    print(f\"Downloading text8 from {url}...\")\n",
        "    with requests.get(url, stream=True) as response:\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            with tqdm.tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "                        pbar.update(len(chunk))\n",
        "\n",
        "    print(f\"Extracting {zip_path}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_path)\n",
        "\n",
        "    # Remove zip file to save space\n",
        "    os.remove(zip_path)\n",
        "\n",
        "    print(f\"Text8 dataset ready at: {text8_file}\")\n",
        "    return text8_file\n",
        "\n",
        "\n",
        "def preprocess_wmt14_news(news_file_path: str, output_dir: str, words_per_sentence: int = 1000,\n",
        "                        max_sentences: int = None, max_files: int = None, use_phrases: bool = False,\n",
        "                        phrase_threshold1: float = 200.0, phrase_threshold2: float = 100.0) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess WMT14 news file into sentence files\n",
        "\n",
        "    Args:\n",
        "        news_file_path: Path to WMT14 news file\n",
        "        output_dir: Output directory for processed files\n",
        "        words_per_sentence: Number of words per sentence (default: 1000)\n",
        "        max_sentences: Maximum number of sentences to process (None = all)\n",
        "        max_files: Maximum number of files to create (None = all)\n",
        "        use_phrases: Whether to apply phrase detection (default: False)\n",
        "        phrase_threshold1: First pass phrase threshold (default: 200.0)\n",
        "        phrase_threshold2: Second pass phrase threshold (default: 100.0)\n",
        "\n",
        "    Returns:\n",
        "        Path to output directory\n",
        "    \"\"\"\n",
        "    print(f\"Preprocessing WMT14 news file: {news_file_path}\")\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "    print(f\"Words per sentence: {words_per_sentence}\")\n",
        "    print(\"Note: Punctuation will be removed from text (commas, periods, etc.)\")\n",
        "    if max_sentences:\n",
        "        print(f\"Max sentences: {max_sentences:,}\")\n",
        "    if max_files:\n",
        "        print(f\"Max files: {max_files}\")\n",
        "    if use_phrases:\n",
        "        print(f\"Phrase detection: Enabled (threshold1={phrase_threshold1}, threshold2={phrase_threshold2})\")\n",
        "\n",
        "    # Create output directory\n",
        "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Check if already processed\n",
        "    existing_files = [f for f in os.listdir(output_dir) if f.startswith(\"0\")]\n",
        "    if existing_files:\n",
        "        print(f\"Found {len(existing_files)} existing processed files. Skipping preprocessing.\")\n",
        "        print(\"⚠️ WARNING: If these files contain punctuation, delete them and reprocess to apply cleaning.\")\n",
        "        return output_dir\n",
        "\n",
        "    # Step 1: Basic preprocessing\n",
        "    temp_dir = output_dir + \"_temp\"\n",
        "    pathlib.Path(temp_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Read news file (one sentence per line)\n",
        "    sentences = []\n",
        "    sentence_count = 0\n",
        "\n",
        "    with open(news_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            # Clean text: remove punctuation and normalize\n",
        "            cleaned_line = clean_text_remove_punctuation(line)\n",
        "            if cleaned_line:  # Skip empty lines after cleaning\n",
        "                # Split into words and group into chunks\n",
        "                words = cleaned_line.split()\n",
        "                for i in range(0, len(words), words_per_sentence):\n",
        "                    sentence_words = words[i:i + words_per_sentence]\n",
        "                    if len(sentence_words) >= 2:  # Skip very short sentences\n",
        "                        sentences.append(\" \".join(sentence_words))\n",
        "                        sentence_count += 1\n",
        "\n",
        "                        # Stop if we've reached max_sentences\n",
        "                        if max_sentences and sentence_count >= max_sentences:\n",
        "                            print(f\"Reached max_sentences limit: {max_sentences:,}\")\n",
        "                            break\n",
        "\n",
        "                # Break outer loop if we've reached max_sentences\n",
        "                if max_sentences and sentence_count >= max_sentences:\n",
        "                    break\n",
        "\n",
        "    print(f\"Total sentences: {len(sentences):,}\")\n",
        "\n",
        "    # Save to temporary files\n",
        "    sentences_per_file = 100000\n",
        "    file_count = 0\n",
        "    current_file_sentences = []\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        current_file_sentences.append(sentence)\n",
        "\n",
        "        # Write file when it reaches sentences_per_file or we're at the end\n",
        "        if len(current_file_sentences) >= sentences_per_file or i == len(sentences) - 1:\n",
        "            filename = f\"{file_count:04d}\"\n",
        "            filepath = os.path.join(temp_dir, filename)\n",
        "\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                for sent in current_file_sentences:\n",
        "                    f.write(sent + '\\n')\n",
        "\n",
        "            print(f\"Wrote {len(current_file_sentences):,} sentences to {filename}\")\n",
        "            file_count += 1\n",
        "            current_file_sentences = []\n",
        "\n",
        "            # Stop if we've reached max_files\n",
        "            if max_files and file_count >= max_files:\n",
        "                print(f\"Reached max_files limit: {max_files}\")\n",
        "                break\n",
        "\n",
        "    # Step 2: Apply phrase detection if enabled\n",
        "    if use_phrases:\n",
        "        print(\"\\nApplying phrase detection...\")\n",
        "        preprocess_with_phrases(temp_dir, output_dir, min_count=5,\n",
        "                               threshold1=phrase_threshold1, threshold2=phrase_threshold2)\n",
        "        # Cleanup temp directory\n",
        "        import shutil\n",
        "        if os.path.exists(temp_dir):\n",
        "            shutil.rmtree(temp_dir)\n",
        "    else:\n",
        "        # Just move files from temp to output\n",
        "        import shutil\n",
        "        if os.path.exists(output_dir):\n",
        "            shutil.rmtree(output_dir)\n",
        "        shutil.move(temp_dir, output_dir)\n",
        "\n",
        "    print(f\"Preprocessing complete. Created {file_count} files in {output_dir}\")\n",
        "    return output_dir\n",
        "\n",
        "\n",
        "def preprocess_text8(text8_file_path: str, output_dir: str, words_per_sentence: int = 1000,\n",
        "                    use_phrases: bool = False, phrase_threshold1: float = 200.0,\n",
        "                    phrase_threshold2: float = 100.0) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess text8 file into sentence files.\n",
        "\n",
        "    Args:\n",
        "        text8_file_path: Path to text8 file\n",
        "        output_dir: Output directory for processed files\n",
        "        words_per_sentence: Number of words per sentence (default: 1000)\n",
        "        use_phrases: Whether to apply phrase detection (default: False)\n",
        "        phrase_threshold1: First pass phrase threshold (default: 200.0)\n",
        "        phrase_threshold2: Second pass phrase threshold (default: 100.0)\n",
        "\n",
        "    Returns:\n",
        "        Path to output directory\n",
        "    \"\"\"\n",
        "    print(f\"Preprocessing text8 file: {text8_file_path}\")\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "    print(f\"Words per sentence: {words_per_sentence}\")\n",
        "    if use_phrases:\n",
        "        print(f\"Phrase detection: Enabled (threshold1={phrase_threshold1}, threshold2={phrase_threshold2})\")\n",
        "\n",
        "    # Create output directory\n",
        "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Check if already processed\n",
        "    existing_files = [f for f in os.listdir(output_dir) if f.startswith(\"0\")]\n",
        "    if existing_files:\n",
        "        print(f\"Found {len(existing_files)} existing processed files. Skipping preprocessing.\")\n",
        "        return output_dir\n",
        "\n",
        "    # Step 1: Basic preprocessing\n",
        "    temp_dir = output_dir + \"_temp\"\n",
        "    pathlib.Path(temp_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Read text8 file (single long line)\n",
        "    with open(text8_file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read().strip()\n",
        "\n",
        "    # Split into words\n",
        "    words = text.split()\n",
        "    print(f\"Total words: {len(words):,}\")\n",
        "\n",
        "    # Group into sentences\n",
        "    sentences = []\n",
        "    for i in range(0, len(words), words_per_sentence):\n",
        "        sentence_words = words[i:i + words_per_sentence]\n",
        "        if len(sentence_words) >= 2:  # Skip very short sentences\n",
        "            sentences.append(\" \".join(sentence_words))\n",
        "\n",
        "    print(f\"Created {len(sentences):,} sentences\")\n",
        "\n",
        "    # Save to temporary files (similar to myw2v format)\n",
        "    sentences_per_file = 100000\n",
        "    file_count = 0\n",
        "    current_file_sentences = []\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        current_file_sentences.append(sentence)\n",
        "\n",
        "        # Write file when it reaches sentences_per_file or we're at the end\n",
        "        if len(current_file_sentences) >= sentences_per_file or i == len(sentences) - 1:\n",
        "            filename = f\"{file_count:04d}\"\n",
        "            filepath = os.path.join(temp_dir, filename)\n",
        "\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                for sent in current_file_sentences:\n",
        "                    f.write(sent + '\\n')\n",
        "\n",
        "            print(f\"Wrote {len(current_file_sentences):,} sentences to {filename}\")\n",
        "            file_count += 1\n",
        "            current_file_sentences = []\n",
        "\n",
        "    # Step 2: Apply phrase detection if enabled\n",
        "    if use_phrases:\n",
        "        print(\"\\nApplying phrase detection...\")\n",
        "        preprocess_with_phrases(temp_dir, output_dir, min_count=5,\n",
        "                               threshold1=phrase_threshold1, threshold2=phrase_threshold2)\n",
        "        # Cleanup temp directory\n",
        "        import shutil\n",
        "        if os.path.exists(temp_dir):\n",
        "            shutil.rmtree(temp_dir)\n",
        "    else:\n",
        "        # Just move files from temp to output\n",
        "        import shutil\n",
        "        if os.path.exists(output_dir):\n",
        "            shutil.rmtree(output_dir)\n",
        "        shutil.move(temp_dir, output_dir)\n",
        "\n",
        "    print(f\"Preprocessing complete. Created {file_count} files in {output_dir}\")\n",
        "    return output_dir\n",
        "\n",
        "\n",
        "def get_data_file_names(path: str, seed: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Get shuffled list of data file names\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    qq = [fn for fn in os.listdir(path) if fn.startswith(\"0\")]\n",
        "    # Sort first to ensure consistent shuffling\n",
        "    data_files = sorted(qq)\n",
        "    rng.shuffle(data_files)\n",
        "    return data_files\n",
        "\n",
        "\n",
        "def read_all_data_files(data_path: str, file_names: List[str], word_to_idx: dict) -> Tuple[List[int], List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Read all data files and convert words to indices\n",
        "    Returns (inputs, offsets, lengths)\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "\n",
        "    start = time.time()\n",
        "    inps, offs, lens = [], [], []\n",
        "    offset_total = 0\n",
        "    stats = defaultdict(int)\n",
        "\n",
        "    for fn in file_names:\n",
        "        fp = os.path.join(data_path, fn)\n",
        "        ok_lines = 0\n",
        "        too_short_lines = 0\n",
        "        with open(fp, encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                words = [word for word in re.split(r\"[ .]+\", line.strip()) if word]\n",
        "                if len(words) < 2:\n",
        "                    too_short_lines += 1\n",
        "                    continue\n",
        "                idcs = [word_to_idx[w] for w in words if w in word_to_idx]\n",
        "                le = len(idcs)\n",
        "                ok_lines += 1\n",
        "                offs.append(offset_total)\n",
        "                lens.append(le)\n",
        "                inps.extend(idcs)\n",
        "                offset_total += le\n",
        "        stats[\"file_read_lines_ok\"] += ok_lines\n",
        "        stats[\"one_word_sentence_lines_which_were_ignored\"] += too_short_lines\n",
        "\n",
        "    print(f\"read_all_data_files() STATS: {stats}\")\n",
        "    tot_tm = time.time()-start\n",
        "    print(f\"read_all_data_files() Total time {tot_tm} s for {len(file_names)} files (avg {tot_tm/len(file_names)} s/file)\")\n",
        "    return inps, offs, lens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy1oEFb5mTRG"
      },
      "source": [
        "# IV. **Skip-gram Implementation**\n",
        "\n",
        "This cell implements the Skip-gram architecture for Word2Vec training using GPU-accelerated CUDA kernels. Skip-gram predicts surrounding context words from a center word, making it effective for learning word representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNat0U4dmWye"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import time\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "from numba import cuda\n",
        "from numba.cuda import random as c_random\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "\n",
        "@cuda.jit\n",
        "def calc_skipgram(\n",
        "        rows: int,\n",
        "        c: int,\n",
        "        k: int,\n",
        "        learning_rate: float,\n",
        "        w1,\n",
        "        w2,\n",
        "        calc_aux,\n",
        "        random_states,\n",
        "        subsample_weights,\n",
        "        negsample_array,\n",
        "        inp,\n",
        "        offsets,\n",
        "        lengths,\n",
        "        use_hs,\n",
        "        syn1,\n",
        "        codes_array,\n",
        "        points_array,\n",
        "        code_lengths,\n",
        "        exp_table,\n",
        "        exp_table_size,\n",
        "        max_exp):\n",
        "    \"\"\"\n",
        "    CUDA kernel for Skip-gram training\n",
        "    Based on word2vec.c Skip-gram implementation from the original source code of the Word2Vec paper\n",
        "    Supports both Hierarchical Softmax and Negative Sampling\n",
        "    \"\"\"\n",
        "    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    if idx >= rows:\n",
        "        return\n",
        "    le = lengths[idx]\n",
        "    off = offsets[idx]\n",
        "\n",
        "    for centre in range(0, le):\n",
        "        word_idx = inp[off + centre]\n",
        "        prob_to_reject = subsample_weights[word_idx]\n",
        "        rnd = c_random.xoroshiro128p_uniform_float32(random_states, idx)\n",
        "\n",
        "        if rnd > prob_to_reject:\n",
        "            r_f = c_random.xoroshiro128p_uniform_float32(random_states, idx)\n",
        "            r: int = math.ceil(r_f * c)\n",
        "\n",
        "            # Context before center word\n",
        "            for context_pre in range(max(0, centre-r), centre):\n",
        "                step_skipgram(idx, w1, w2, calc_aux, inp[off+centre], inp[off+context_pre],\n",
        "                             k, learning_rate, negsample_array, random_states,\n",
        "                             use_hs, syn1, codes_array, points_array, code_lengths,\n",
        "                             exp_table, exp_table_size, max_exp)\n",
        "\n",
        "            # Context after center word\n",
        "            for context_post in range(centre + 1, min(le, centre + 1 + r)):\n",
        "                step_skipgram(idx, w1, w2, calc_aux, inp[off+centre], inp[off+context_post],\n",
        "                             k, learning_rate, negsample_array, random_states,\n",
        "                             use_hs, syn1, codes_array, points_array, code_lengths,\n",
        "                             exp_table, exp_table_size, max_exp)\n",
        "\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def fast_sigmoid(f, exp_table, exp_table_size, max_exp):\n",
        "    \"\"\"\n",
        "    Fast sigmoid using precomputed exp table\n",
        "    Based on word2vec.c exp table lookup from the original source code of the Word2Vec paper\n",
        "    \"\"\"\n",
        "    if f <= -max_exp:\n",
        "        return 0.0\n",
        "    elif f >= max_exp:\n",
        "        return 1.0\n",
        "    else:\n",
        "        idx = int((f + max_exp) * (exp_table_size / max_exp / 2.0))\n",
        "        if idx < 0:\n",
        "            idx = 0\n",
        "        if idx >= exp_table_size:\n",
        "            idx = exp_table_size - 1\n",
        "        return exp_table[idx]\n",
        "\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def step_skipgram(thread_idx, w1, w2, calc_aux, x, y, k, learning_rate, negsample_array, random_states,\n",
        "                  use_hs, syn1, codes_array, points_array, code_lengths,\n",
        "                  exp_table, exp_table_size, max_exp):\n",
        "    \"\"\"\n",
        "    Device function for Skip-gram gradient calculation\n",
        "    Based on word2vec.c Skip-gram implementation from the original source code of the Word2Vec paper\n",
        "    Supports both Hierarchical Softmax and Negative Sampling\n",
        "    \"\"\"\n",
        "    emb_dim = w1.shape[1]\n",
        "    negs_arr_len = len(negsample_array)\n",
        "\n",
        "    # Initialize error accumulator\n",
        "    for i in range(emb_dim):\n",
        "        calc_aux[thread_idx, i] = 0.0\n",
        "\n",
        "    # Hierarchical Softmax (if enabled) - traverse tree for context word y\n",
        "    if use_hs:\n",
        "        codelen = code_lengths[y]\n",
        "        max_code_len = codes_array.shape[1]\n",
        "        for d in range(codelen):\n",
        "            if d >= max_code_len:\n",
        "                break\n",
        "            node_idx = points_array[y, d]\n",
        "            if node_idx < 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate dot product: w1[x] • syn1[node]\n",
        "            f = 0.0\n",
        "            for i in range(emb_dim):\n",
        "                f += w1[x, i] * syn1[node_idx, i]\n",
        "\n",
        "            # Early skip if f is outside range (same as original code)\n",
        "            # This prevents unnecessary updates when sigmoid is saturated\n",
        "            if f <= -max_exp:\n",
        "                continue\n",
        "            if f >= max_exp:\n",
        "                continue\n",
        "\n",
        "            # Get sigmoid from exp table (only if in range)\n",
        "            sigmoid_val = fast_sigmoid(f, exp_table, exp_table_size, max_exp)\n",
        "\n",
        "            # Get code bit (0 or 1)\n",
        "            code_bit = codes_array[y, d]\n",
        "            if code_bit < 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate gradient: g = (1 - code_bit - sigmoid) * learning_rate\n",
        "            g = (1.0 - float(code_bit) - sigmoid_val) * learning_rate\n",
        "\n",
        "            # Propagate errors output -> hidden\n",
        "            for i in range(emb_dim):\n",
        "                calc_aux[thread_idx, i] += g * syn1[node_idx, i]\n",
        "\n",
        "            # Learn weights hidden -> output\n",
        "            for i in range(emb_dim):\n",
        "                syn1[node_idx, i] += g * w1[x, i]\n",
        "\n",
        "    # Negative Sampling (if enabled)\n",
        "    if k > 0:\n",
        "        # Positive sample: predict context word y\n",
        "        dot_xy = 0.0\n",
        "        for i in range(emb_dim):\n",
        "            dot_xy += w1[x, i] * w2[y, i]\n",
        "        s_xdy_m1 = fast_sigmoid(dot_xy, exp_table, exp_table_size, max_exp) - 1.0\n",
        "\n",
        "        # Positive sample gradients\n",
        "        for i in range(emb_dim):\n",
        "            calc_aux[thread_idx, i] += -learning_rate * s_xdy_m1 * w2[y, i]\n",
        "            w2[y, i] -= learning_rate * s_xdy_m1 * w1[x, i]\n",
        "\n",
        "        # Negative samples\n",
        "        for neg_sample in range(0, k):\n",
        "            rnd = c_random.xoroshiro128p_uniform_float32(random_states, thread_idx)\n",
        "            q_idx: int = int(math.floor(negs_arr_len * rnd))\n",
        "            neg = negsample_array[q_idx]\n",
        "            dot_xq = 0.0\n",
        "            for i in range(emb_dim):\n",
        "                dot_xq += w1[x, i] * w2[neg, i]\n",
        "            s_dxq = fast_sigmoid(dot_xq, exp_table, exp_table_size, max_exp)\n",
        "\n",
        "            # Negative sample gradients\n",
        "            for i in range(emb_dim):\n",
        "                calc_aux[thread_idx, i] -= learning_rate * s_dxq * w2[neg, i]\n",
        "                w2[neg, i] -= learning_rate * s_dxq * w1[x, i]\n",
        "\n",
        "    # Note: Original code does NOT use gradient clipping, only early skip\n",
        "    # Gradient clipping may reduce training effectiveness\n",
        "    # Update center word vector (same as original code)\n",
        "    for i in range(emb_dim):\n",
        "        w1[x, i] += calc_aux[thread_idx, i]\n",
        "\n",
        "\n",
        "def train_skipgram(\n",
        "        data_path: str,\n",
        "        out_file_path: str,\n",
        "        epochs: int,\n",
        "        embed_dim: int = 100,\n",
        "        min_occurs: int = 3,\n",
        "        c: int = 5,\n",
        "        k: int = 5,\n",
        "        t: float = 1e-5,\n",
        "        vocab_freq_exponent: float = 0.75,\n",
        "        lr_max: float = 0.025,\n",
        "        lr_min: float = 0.0025,\n",
        "        cuda_threads_per_block: int = 32,\n",
        "        hs: int = 0,\n",
        "        max_memory_gb: float = 70.0,\n",
        "        max_words: int = None,\n",
        "        vocab: list = None,\n",
        "        w_to_i: dict = None,\n",
        "        word_counts: list = None,\n",
        "        ssw: np.ndarray = None,\n",
        "        negs: np.ndarray = None):\n",
        "    \"\"\"\n",
        "    Train Skip-gram model\n",
        "    Based on word2vec.c Skip-gram implementation from the original source code of the Word2Vec paper\n",
        "\n",
        "    Args:\n",
        "        hs: Hierarchical Softmax flag (0=NS only, 1=HS only). Cannot combine with k>0\n",
        "        k: Negative sampling count (0=HS only, >0=NS only). Cannot combine with hs=1\n",
        "        max_memory_gb: Maximum GPU memory usage in GB. If estimated memory exceeds this,\n",
        "                       the dataset will be automatically split into batches for processing\n",
        "                       Default: 70.0 GB (safe for A100 80GB GPU)\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If both hs=1 and k>0 are specified (HS and NS cannot be combined)\n",
        "    \"\"\"\n",
        "    # Validate: HS and NS cannot be used together\n",
        "    if hs == 1 and k > 0:\n",
        "        raise ValueError(\n",
        "            \"Error: Cannot use HS (hs=1) and Negative Sampling (k>0) together. \"\n",
        "            \"Please choose either HS only (hs=1, k=0) or NS only (hs=0, k>0)\"\n",
        "        )\n",
        "\n",
        "    params = {\n",
        "        \"model_type\": \"skipgram\",\n",
        "        \"w2v_version\": W2V_VERSION,\n",
        "        \"data_path\": data_path,\n",
        "        \"out_file_path\": out_file_path,\n",
        "        \"epochs\": epochs,\n",
        "        \"embed_dim\": embed_dim,\n",
        "        \"min_occurs\": min_occurs,\n",
        "        \"c\": c,\n",
        "        \"k\": k,\n",
        "        \"t\": t,\n",
        "        \"vocab_freq_exponent\": vocab_freq_exponent,\n",
        "        \"lr_max\": lr_max,\n",
        "        \"lr_min\": lr_min,\n",
        "        \"cuda_threads_per_block\": cuda_threads_per_block,\n",
        "        \"hs\": hs\n",
        "    }\n",
        "    stats = {}\n",
        "    params_path = out_file_path + \"_params.json\"\n",
        "    stats_path = out_file_path + \"_stats.json\"\n",
        "\n",
        "    seed = 12345\n",
        "\n",
        "    # Adjust learning rate based on training method\n",
        "    original_lr_max = lr_max\n",
        "    original_lr_min = lr_min\n",
        "\n",
        "    # Learning rate handling: HS only and NS only use the same learning rate\n",
        "    # (as per word2vec.c original implementation)\n",
        "    # No special adjustment needed for either method\n",
        "\n",
        "    # Learning rate schedule\n",
        "    # For multiple epochs: decrease between epochs\n",
        "    # For all epochs: decrease linearly within epoch (as per word2vec.c)\n",
        "    if epochs > 1:\n",
        "        lr_step = (lr_max - lr_min) / (epochs - 1)\n",
        "    else:\n",
        "        lr_step = 0.0  # Not used for single epoch (LR decays within epoch)\n",
        "\n",
        "    print(f\"Skip-gram Training Parameters:\")\n",
        "    print(f\"Seed: {seed}\")\n",
        "    print(f\"Window size: {c}\")\n",
        "    if hs == 1:\n",
        "        print(f\"Hierarchical Softmax: Enabled\")\n",
        "    if k > 0:\n",
        "        print(f\"Negative samples: {k}\")\n",
        "    if original_lr_max != lr_max:\n",
        "        print(f\"Learning rate adjusted: {original_lr_max} -> {lr_max} (reduced for stability)\")\n",
        "    if epochs == 1:\n",
        "        print(f\"Learning rate: {lr_max} -> ~0 (will decrease linearly within epoch, as per word2vec.c)\")\n",
        "    else:\n",
        "        print(f\"Learning rate: {lr_max} -> {lr_min} (step: {lr_step:.6f} between epochs, also decreases linearly within each epoch)\")\n",
        "    print(f\"Embedding dimension: {embed_dim}\")\n",
        "    print(f\"Min word count: {min_occurs}\")\n",
        "\n",
        "    # Start timing for total execution\n",
        "    start = time.time()\n",
        "\n",
        "    # Build vocabulary if not provided (for reuse when training both models)\n",
        "    if vocab is None or w_to_i is None or word_counts is None:\n",
        "        print(f\"\\nBuilding vocabulary from: {data_path}\")\n",
        "        vocab_start = time.time()\n",
        "        vocab, w_to_i, word_counts = handle_vocab(data_path, min_occurs, freq_exponent=vocab_freq_exponent, use_cache=True)\n",
        "        vocab_size = len(vocab)\n",
        "        build_time = time.time() - vocab_start\n",
        "        print(f\"Vocabulary {'loaded from cache' if build_time < 1.0 else 'built'} in {build_time:.2f}s. Vocab size: {vocab_size:,}\")\n",
        "    else:\n",
        "        vocab_size = len(vocab)\n",
        "        print(f\"\\nUsing pre-built vocabulary. Vocab size: {vocab_size:,}\")\n",
        "\n",
        "    # Build subsampling weights and negative sampling array if not provided\n",
        "    if ssw is None or negs is None:\n",
        "        ssw, negs = get_subsampling_weights_and_negative_sampling_array(vocab, t=t)\n",
        "\n",
        "    # Create exp table\n",
        "    print(\"Creating exp table for fast sigmoid...\")\n",
        "    exp_table = create_exp_table(EXP_TABLE_SIZE, MAX_EXP)\n",
        "\n",
        "    # Setup Hierarchical Softmax if enabled\n",
        "    use_hs = (hs == 1)\n",
        "    syn1_cuda = None\n",
        "    codes_array_cuda = None\n",
        "    points_array_cuda = None\n",
        "    code_lengths_cuda = None\n",
        "\n",
        "    if use_hs:\n",
        "        print(\"Creating Huffman tree for Hierarchical Softmax...\")\n",
        "        hs_start = time.time()\n",
        "        codes_array, points_array, code_lengths = create_huffman_tree(word_counts, MAX_CODE_LENGTH)\n",
        "        syn1 = init_hs_weight_matrix(vocab_size, embed_dim)\n",
        "        print(f\"Huffman tree created in {time.time() - hs_start:.2f}s\")\n",
        "        print(f\" -Codes array shape: {codes_array.shape}\")\n",
        "        print(f\" -Points array shape: {points_array.shape}\")\n",
        "        print(f\" -Syn1 matrix shape: {syn1.shape}\")\n",
        "\n",
        "    data_files = get_data_file_names(data_path, seed=seed)\n",
        "    print(f\"Processing {len(data_files)} data files...\")\n",
        "    if max_words is not None:\n",
        "        print(f\"⚠️ Limiting to {max_words:,} total words (will stop early if reached)\")\n",
        "    inps_, offs_, lens_ = read_all_data_files_ever(data_path, data_files, w_to_i, max_words=max_words)\n",
        "    inps, offs, lens = (np.asarray(inps_, dtype=np.int32),\n",
        "                       np.asarray(offs_, dtype=np.int32),\n",
        "                       np.asarray(lens_, dtype=np.int32))\n",
        "    sentence_count = len(lens)\n",
        "    total_words = len(inps)  # Total words for LR decay calculation\n",
        "\n",
        "    print(f\"Data loaded: {sentence_count:,} sentences, {total_words:,} total words\")\n",
        "\n",
        "    # Initialize weight matrices\n",
        "    data_init_start = time.time()\n",
        "    w1, w2 = init_weight_matrices(vocab_size, embed_dim, seed=seed)\n",
        "    data_size_weights = 4 * (w1.size + w2.size)\n",
        "    data_size_inputs = 4 * (inps.size + offs.size + lens.size + ssw.size + negs.size)\n",
        "\n",
        "    # Calculate memory usage and determine batch size\n",
        "    weights_gb = data_size_weights / (1024**3)\n",
        "    inputs_gb = data_size_inputs / (1024**3)\n",
        "\n",
        "    # Estimate calc_aux memory for full dataset\n",
        "    calc_aux_size_full = sentence_count * embed_dim * 4\n",
        "    calc_aux_gb_full = calc_aux_size_full / (1024**3)\n",
        "    total_memory_gb = weights_gb + inputs_gb + calc_aux_gb_full\n",
        "\n",
        "    # Determine if batch processing is needed\n",
        "    use_batch_processing = (total_memory_gb > max_memory_gb)\n",
        "\n",
        "    if use_batch_processing:\n",
        "        # Calculate batch size based on available memory\n",
        "        available_memory_gb = max_memory_gb - weights_gb - inputs_gb\n",
        "        # Reserve 5GB for overhead\n",
        "        available_memory_gb = max(1.0, available_memory_gb - 5.0)\n",
        "\n",
        "        # Calculate max sentences per batch\n",
        "        bytes_per_sentence = embed_dim * 4  # float32\n",
        "        max_batch_sentences = int((available_memory_gb * 1024**3) / bytes_per_sentence)\n",
        "\n",
        "        if max_batch_sentences >= 10_000_000:\n",
        "            batch_size = 10_000_000\n",
        "        elif max_batch_sentences >= 5_000_000:\n",
        "            batch_size = 5_000_000\n",
        "        elif max_batch_sentences >= 2_000_000:\n",
        "            batch_size = 2_000_000\n",
        "        elif max_batch_sentences >= 1_000_000:\n",
        "            batch_size = 1_000_000\n",
        "        else:\n",
        "            batch_size = max(100_000, max_batch_sentences)\n",
        "\n",
        "        num_batches = math.ceil(sentence_count / batch_size)\n",
        "        batch_aux_gb = (batch_size * embed_dim * 4) / (1024**3)\n",
        "        batch_total_gb = weights_gb + inputs_gb + batch_aux_gb\n",
        "\n",
        "        print(f\"\\n⚠️ Memory usage would be {total_memory_gb:.1f} GB (exceeds {max_memory_gb} GB limit)\")\n",
        "        print(f\"Using batch processing: {num_batches} batches, {batch_size:,} sentences/batch\")\n",
        "        print(f\"Memory per batch: {batch_total_gb:.1f} GB (calc_aux: {batch_aux_gb:.1f} GB)\")\n",
        "    else:\n",
        "        batch_size = sentence_count\n",
        "        num_batches = 1\n",
        "        print(f\"\\n✅ Memory usage: {total_memory_gb:.1f} GB (within {max_memory_gb} GB limit)\")\n",
        "        print(f\"Processing all {sentence_count:,} sentences in one batch\")\n",
        "\n",
        "    blocks: int = math.ceil(batch_size / cuda_threads_per_block)\n",
        "    print(f\"CUDA config: {cuda_threads_per_block} threads/block, {blocks} blocks per batch\")\n",
        "\n",
        "    # Transfer to GPU - Transfer weights and vocab arrays (these are shared across batches)\n",
        "    print(\"Transferring data to GPU...\")\n",
        "    data_transfer_start = time.time()\n",
        "    ssw_cuda, negs_cuda = cuda.to_device(ssw), cuda.to_device(negs)\n",
        "    w1_cuda, w2_cuda = cuda.to_device(w1), cuda.to_device(w2)\n",
        "    exp_table_cuda = cuda.to_device(exp_table)\n",
        "\n",
        "    # Keep input arrays on CPU - will slice and transfer per batch\n",
        "    # This saves GPU memory\n",
        "\n",
        "    if use_hs:\n",
        "        syn1_cuda = cuda.to_device(syn1)\n",
        "        codes_array_cuda = cuda.to_device(codes_array)\n",
        "        points_array_cuda = cuda.to_device(points_array)\n",
        "        code_lengths_cuda = cuda.to_device(code_lengths)\n",
        "\n",
        "    print(f\"Data transfer completed in {time.time()-data_transfer_start:.2f}s\")\n",
        "\n",
        "    stats[\"sentence_count\"] = len(lens)\n",
        "    stats[\"word_count\"] = len(inps)\n",
        "    stats[\"vocab_size\"] = vocab_size\n",
        "    stats[\"approx_data_size_weights\"] = data_size_weights\n",
        "    stats[\"approx_data_size_inputs\"] = data_size_inputs\n",
        "    stats[\"use_batch_processing\"] = use_batch_processing\n",
        "    if use_batch_processing:\n",
        "        stats[\"batch_size\"] = batch_size\n",
        "        stats[\"num_batches\"] = num_batches\n",
        "        batch_aux_size = batch_size * embed_dim * 4\n",
        "        stats[\"approx_data_size_aux_per_batch\"] = batch_aux_size\n",
        "        stats[\"approx_data_size_total\"] = data_size_weights + data_size_inputs + batch_aux_size\n",
        "    else:\n",
        "        data_size_aux = 4 * (sentence_count * embed_dim)\n",
        "        stats[\"approx_data_size_aux\"] = data_size_aux\n",
        "        stats[\"approx_data_size_total\"] = data_size_weights + data_size_inputs + data_size_aux\n",
        "\n",
        "    # Prepare HS parameters (use dummy arrays if HS disabled)\n",
        "    if not use_hs:\n",
        "        # Create dummy arrays for HS (will not be used, but needed for kernel signature)\n",
        "        dummy_syn1 = cuda.device_array((1, embed_dim), dtype=np.float32)\n",
        "        dummy_codes = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)\n",
        "        dummy_points = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)\n",
        "        dummy_lengths = cuda.device_array(vocab_size, dtype=np.int32)\n",
        "        syn1_param = dummy_syn1\n",
        "        codes_param = dummy_codes\n",
        "        points_param = dummy_points\n",
        "        lengths_param = dummy_lengths\n",
        "    else:\n",
        "        syn1_param = syn1_cuda\n",
        "        codes_param = codes_array_cuda\n",
        "        points_param = points_array_cuda\n",
        "        lengths_param = code_lengths_cuda\n",
        "\n",
        "    print_norms(w1_cuda)\n",
        "    print(f\"\\nStarting Skip-gram training - {epochs} epochs...\")\n",
        "    epoch_times = []\n",
        "    calc_start = time.time()\n",
        "\n",
        "    # Track total words processed across all epochs (as per word2vec.c)\n",
        "    # Learning rate decays based on total words processed, not per epoch\n",
        "    # Use int64 to avoid overflow with large datasets and multiple epochs\n",
        "    words_processed_total = np.int64(0)\n",
        "    total_words_for_training = np.int64(epochs) * np.int64(total_words)\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Process each batch\n",
        "        for batch_idx in range(num_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = min((batch_idx + 1) * batch_size, sentence_count)\n",
        "            batch_sentence_count = batch_end - batch_start\n",
        "\n",
        "            if num_batches > 1:\n",
        "                print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{num_batches}: sentences {batch_start:,}-{batch_end:,}\")\n",
        "\n",
        "            # Calculate word offset for this batch (offsets are cumulative)\n",
        "            batch_word_start = offs[batch_start] if batch_start < len(offs) else 0\n",
        "            batch_word_end = offs[batch_end] if batch_end < len(offs) else len(inps)\n",
        "            batch_word_count = batch_word_end - batch_word_start\n",
        "\n",
        "            # Calculate learning rate for this batch (linear decay as per word2vec.c)\n",
        "            # Formula from word2vec.c: alpha = starting_alpha * (1 - word_count_actual / (iter * train_words + 1))\n",
        "            # word_count_actual is total words processed across all epochs\n",
        "            # This ensures LR decreases linearly from lr_max to ~0 over entire training\n",
        "            denominator = total_words_for_training + 1\n",
        "            current_lr = lr_max * (1.0 - words_processed_total / denominator) if denominator > 0 else lr_max\n",
        "\n",
        "            # Apply minimum threshold (as per word2vec.c: min = starting_alpha * 0.0001)\n",
        "            min_lr_threshold = lr_max * 0.0001\n",
        "            current_lr = max(current_lr, min_lr_threshold)\n",
        "\n",
        "            # Also apply lr_min as additional constraint (for multi-epoch training)\n",
        "            if epochs > 1:\n",
        "                current_lr = max(current_lr, lr_min)\n",
        "\n",
        "            if num_batches > 1 and batch_idx == 0:\n",
        "                print(f\"    Learning rate: {current_lr:.6f} (decaying linearly, progress: {words_processed_total/total_words_for_training*100:.1f}%)\")\n",
        "\n",
        "            # Create batch arrays (slicing from CPU arrays)\n",
        "            batch_lens = lens[batch_start:batch_end]\n",
        "            batch_offs_local = offs[batch_start:batch_end] - batch_word_start  # Adjust offsets to start from 0\n",
        "            batch_inps_local = inps[batch_word_start:batch_word_end]\n",
        "\n",
        "            # Transfer batch arrays to GPU\n",
        "            batch_lens_cuda = cuda.to_device(batch_lens)\n",
        "            batch_offs_cuda = cuda.to_device(batch_offs_local)\n",
        "            batch_inps_cuda = cuda.to_device(batch_inps_local)\n",
        "\n",
        "            # Create calc_aux for this batch\n",
        "            batch_calc_aux = np.zeros((batch_sentence_count, embed_dim), dtype=np.float32)\n",
        "            batch_calc_aux_cuda = cuda.to_device(batch_calc_aux)\n",
        "\n",
        "            # Create random states for this batch\n",
        "            batch_random_states_cuda = c_random.create_xoroshiro128p_states(\n",
        "                batch_sentence_count, seed=seed + epoch * 10000 + batch_idx * 100\n",
        "            )\n",
        "\n",
        "            # Launch CUDA kernel for this batch with current learning rate\n",
        "            batch_blocks = math.ceil(batch_sentence_count / cuda_threads_per_block)\n",
        "            calc_skipgram[batch_blocks, cuda_threads_per_block](\n",
        "                batch_sentence_count, c, k, current_lr, w1_cuda, w2_cuda, batch_calc_aux_cuda,\n",
        "                batch_random_states_cuda, ssw_cuda, negs_cuda, batch_inps_cuda,\n",
        "                batch_offs_cuda, batch_lens_cuda,\n",
        "                use_hs, syn1_param, codes_param, points_param, lengths_param,\n",
        "                exp_table_cuda, EXP_TABLE_SIZE, MAX_EXP)\n",
        "\n",
        "            # Update total words processed counter (as per word2vec.c)\n",
        "            # Note: Actual words processed may vary due to subsampling, but this is an approximation\n",
        "            # Use int64 to avoid overflow with large datasets and multiple epochs\n",
        "            words_processed_total = np.int64(words_processed_total) + np.int64(batch_word_count)\n",
        "\n",
        "            # Free batch arrays from GPU memory\n",
        "            del batch_lens_cuda, batch_offs_cuda, batch_inps_cuda, batch_calc_aux_cuda, batch_random_states_cuda\n",
        "\n",
        "        # Synchronize after all batches\n",
        "        sync_start = time.time()\n",
        "        cuda.synchronize()\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "\n",
        "        # Final LR after epoch (using same formula as word2vec.c)\n",
        "        denominator = total_words_for_training + 1\n",
        "        final_lr = lr_max * (1.0 - words_processed_total / denominator) if denominator > 0 else lr_max\n",
        "        final_lr = max(final_lr, lr_max * 0.0001)\n",
        "        if epochs > 1:\n",
        "            final_lr = max(final_lr, lr_min)\n",
        "\n",
        "        progress_percent = (words_processed_total / total_words_for_training * 100) if total_words_for_training > 0 else 0.0\n",
        "        print(f\"  Epoch {epoch+1} completed in {epoch_time:.2f}s (LR: {final_lr:.6f}, Progress: {progress_percent:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nSkip-gram training completed!\")\n",
        "    print(f\"Epoch times - Min: {min(epoch_times):.2f}s, Avg: {np.mean(epoch_times):.2f}s, Max: {max(epoch_times):.2f}s\")\n",
        "    print(f\"Total training time: {time.time()-calc_start:.2f}s\")\n",
        "    print(f\"Total time: {time.time()-start:.2f}s\")\n",
        "\n",
        "    print_norms(w1_cuda)\n",
        "\n",
        "    # Save results\n",
        "    stats[\"epoch_time_min_seconds\"] = min(epoch_times)\n",
        "    stats[\"epoch_time_avg_seconds\"] = np.mean(epoch_times)\n",
        "    stats[\"epoch_time_max_seconds\"] = max(epoch_times)\n",
        "    stats[\"epoch_time_total_seconds\"] = sum(epoch_times)\n",
        "    stats[\"epoch_times_all_seconds\"] = epoch_times\n",
        "\n",
        "    print(f\"Saving Skip-gram vectors to: {out_file_path}\")\n",
        "    write_vectors(w1_cuda, vocab, out_file_path)\n",
        "\n",
        "    print(f\"Saving parameters to: {params_path}\")\n",
        "    write_json(params, params_path)\n",
        "\n",
        "    print(f\"Saving statistics to: {stats_path}\")\n",
        "    write_json(stats, stats_path)\n",
        "\n",
        "    print(\"Skip-gram training completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzcl6Ua8nX70"
      },
      "source": [
        "# V. **CBOW Implementation**\n",
        "\n",
        "This cell implements the Continuous Bag-of-Words (CBOW) architecture for Word2Vec training using GPU-accelerated CUDA kernels. CBOW predicts a center word from surrounding context words, making it faster but typically less accurate than Skip-gram for rare words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4tu20GWnah1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import time\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "from numba import cuda\n",
        "from numba.cuda import random as c_random\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "\n",
        "@cuda.jit\n",
        "def calc_cbow(\n",
        "        rows: int,\n",
        "        c: int,\n",
        "        k: int,\n",
        "        learning_rate: float,\n",
        "        w1,\n",
        "        w2,\n",
        "        calc_aux,\n",
        "        random_states,\n",
        "        subsample_weights,\n",
        "        negsample_array,\n",
        "        inp,\n",
        "        offsets,\n",
        "        lengths,\n",
        "        use_hs,\n",
        "        syn1,\n",
        "        codes_array,\n",
        "        points_array,\n",
        "        code_lengths,\n",
        "        exp_table,\n",
        "        exp_table_size,\n",
        "        max_exp):\n",
        "    \"\"\"\n",
        "    CUDA kernel for CBOW training\n",
        "    Based on word2vec.c CBOW implementation from the original source code of the Word2Vec paper\n",
        "    Supports both Hierarchical Softmax and Negative Sampling\n",
        "    \"\"\"\n",
        "    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    if idx >= rows:\n",
        "        return\n",
        "    le = lengths[idx]\n",
        "    off = offsets[idx]\n",
        "\n",
        "    for centre in range(0, le):\n",
        "        word_idx = inp[off + centre]\n",
        "        prob_to_reject = subsample_weights[word_idx]\n",
        "        rnd = c_random.xoroshiro128p_uniform_float32(random_states, idx)\n",
        "\n",
        "        if rnd > prob_to_reject:\n",
        "            r_f = c_random.xoroshiro128p_uniform_float32(random_states, idx)\n",
        "            r: int = math.ceil(r_f * c)\n",
        "\n",
        "            # Collect context words (before and after center word)\n",
        "            context_words = cuda.local.array(64, dtype=np.int32)  # Max 2*c context words\n",
        "            context_count = 0\n",
        "\n",
        "            # Context before center word\n",
        "            for context_pre in range(max(0, centre-r), centre):\n",
        "                if context_count < 20:  # Prevent overflow\n",
        "                    context_words[context_count] = inp[off+context_pre]\n",
        "                    context_count += 1\n",
        "\n",
        "            # Context after center word\n",
        "            for context_post in range(centre + 1, min(le, centre + 1 + r)):\n",
        "                if context_count < 20:  # Prevent overflow\n",
        "                    context_words[context_count] = inp[off+context_post]\n",
        "                    context_count += 1\n",
        "\n",
        "            # Only proceed if we have context words\n",
        "            if context_count > 0:\n",
        "                step_cbow(idx, w1, w2, calc_aux, context_words, context_count,\n",
        "                         inp[off+centre], k, learning_rate, negsample_array, random_states,\n",
        "                         use_hs, syn1, codes_array, points_array, code_lengths,\n",
        "                         exp_table, exp_table_size, max_exp)\n",
        "\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def fast_sigmoid(f, exp_table, exp_table_size, max_exp):\n",
        "    \"\"\"\n",
        "    Fast sigmoid using precomputed exp table\n",
        "    Based on word2vec.c exp table lookup\n",
        "    \"\"\"\n",
        "    if f <= -max_exp:\n",
        "        return 0.0\n",
        "    elif f >= max_exp:\n",
        "        return 1.0\n",
        "    else:\n",
        "        idx = int((f + max_exp) * (exp_table_size / max_exp / 2.0))\n",
        "        if idx < 0:\n",
        "            idx = 0\n",
        "        if idx >= exp_table_size:\n",
        "            idx = exp_table_size - 1\n",
        "        return exp_table[idx]\n",
        "\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def step_cbow(thread_idx, w1, w2, calc_aux, context_words, context_count,\n",
        "              center_word, k, learning_rate, negsample_array, random_states,\n",
        "              use_hs, syn1, codes_array, points_array, code_lengths,\n",
        "              exp_table, exp_table_size, max_exp):\n",
        "    \"\"\"\n",
        "    Device function for CBOW gradient calculation\n",
        "    Based on word2vec.c CBOW implementation from the original source code of the Word2Vec paper\n",
        "    Supports both Hierarchical Softmax and Negative Sampling\n",
        "    \"\"\"\n",
        "    emb_dim = w1.shape[1]\n",
        "    negs_arr_len = len(negsample_array)\n",
        "\n",
        "    # 1. Calculate neu1 = average of context word vectors\n",
        "    neu1 = cuda.local.array(1000, dtype=np.float32)  # Max embedding dimension\n",
        "    neu1e = cuda.local.array(1000, dtype=np.float32)  # Error accumulation\n",
        "\n",
        "    # Initialize neu1 and neu1e\n",
        "    for i in range(emb_dim):\n",
        "        neu1[i] = 0.0\n",
        "        neu1e[i] = 0.0\n",
        "\n",
        "    # Average context word vectors\n",
        "    for i in range(emb_dim):\n",
        "        for ctx_idx in range(context_count):\n",
        "            neu1[i] += w1[context_words[ctx_idx], i]\n",
        "        neu1[i] /= context_count\n",
        "\n",
        "    # 2. Hierarchical Softmax (if enabled)\n",
        "    if use_hs:\n",
        "        codelen = code_lengths[center_word]\n",
        "        max_code_len = codes_array.shape[1]  # Get max code length from array shape\n",
        "        for d in range(codelen):\n",
        "            if d >= max_code_len:\n",
        "                break\n",
        "            node_idx = points_array[center_word, d]\n",
        "            if node_idx < 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate dot product: neu1 • syn1[node]\n",
        "            f = 0.0\n",
        "            for i in range(emb_dim):\n",
        "                f += neu1[i] * syn1[node_idx, i]\n",
        "\n",
        "            # Early skip if f is outside range (same as original code)\n",
        "            # This prevents unnecessary updates when sigmoid is saturated\n",
        "            if f <= -max_exp:\n",
        "                continue\n",
        "            if f >= max_exp:\n",
        "                continue\n",
        "\n",
        "            # Get sigmoid from exp table (only if in range)\n",
        "            sigmoid_val = fast_sigmoid(f, exp_table, exp_table_size, max_exp)\n",
        "\n",
        "            # Get code bit (0 or 1)\n",
        "            code_bit = codes_array[center_word, d]\n",
        "            if code_bit < 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate gradient: g = (1 - code_bit - sigmoid) * learning_rate\n",
        "            g = (1.0 - float(code_bit) - sigmoid_val) * learning_rate\n",
        "\n",
        "            # Propagate errors output -> hidden\n",
        "            for i in range(emb_dim):\n",
        "                neu1e[i] += g * syn1[node_idx, i]\n",
        "\n",
        "            # Learn weights hidden -> output\n",
        "            for i in range(emb_dim):\n",
        "                syn1[node_idx, i] += g * neu1[i]\n",
        "\n",
        "    # 3. Negative Sampling (if enabled)\n",
        "    if k > 0:\n",
        "        # Positive sample: predict center_word\n",
        "        dot_xy = 0.0\n",
        "        for i in range(emb_dim):\n",
        "            dot_xy += neu1[i] * w2[center_word, i]\n",
        "        s_xdy_m1 = fast_sigmoid(dot_xy, exp_table, exp_table_size, max_exp) - 1.0\n",
        "\n",
        "        # Update w2[center_word] and accumulate neu1e\n",
        "        for i in range(emb_dim):\n",
        "            neu1e[i] += -learning_rate * s_xdy_m1 * w2[center_word, i]\n",
        "            w2[center_word, i] -= learning_rate * s_xdy_m1 * neu1[i]\n",
        "\n",
        "        # Negative samples\n",
        "        for neg_sample in range(0, k):\n",
        "            rnd = c_random.xoroshiro128p_uniform_float32(random_states, thread_idx)\n",
        "            q_idx: int = int(math.floor(negs_arr_len * rnd))\n",
        "            neg = negsample_array[q_idx]\n",
        "            dot_xq = 0.0\n",
        "            for i in range(emb_dim):\n",
        "                dot_xq += neu1[i] * w2[neg, i]\n",
        "            s_dxq = fast_sigmoid(dot_xq, exp_table, exp_table_size, max_exp)\n",
        "\n",
        "            # Update w2[neg] and accumulate neu1e\n",
        "            for i in range(emb_dim):\n",
        "                neu1e[i] -= learning_rate * s_dxq * w2[neg, i]\n",
        "                w2[neg, i] -= learning_rate * s_dxq * neu1[i]\n",
        "\n",
        "    # 4. Backprop neu1e to all context words\n",
        "    # Note: Original code does NOT use gradient clipping, only early skip\n",
        "    # Gradient clipping may reduce training effectiveness\n",
        "    # Update context word vectors (same as original code)\n",
        "    for ctx_idx in range(context_count):\n",
        "        for i in range(emb_dim):\n",
        "            w1[context_words[ctx_idx], i] += neu1e[i]\n",
        "\n",
        "\n",
        "def train_cbow(\n",
        "        data_path: str,\n",
        "        out_file_path: str,\n",
        "        epochs: int,\n",
        "        embed_dim: int = 100,\n",
        "        min_occurs: int = 3,\n",
        "        c: int = 5,\n",
        "        k: int = 5,\n",
        "        t: float = 1e-5,\n",
        "        vocab_freq_exponent: float = 0.75,\n",
        "        lr_max: float = 0.025,\n",
        "        lr_min: float = 0.0025,\n",
        "        cuda_threads_per_block: int = 32,\n",
        "        hs: int = 0,\n",
        "        max_memory_gb: float = 70.0,\n",
        "        max_words: int = None,\n",
        "        vocab: list = None,\n",
        "        w_to_i: dict = None,\n",
        "        word_counts: list = None,\n",
        "        ssw: np.ndarray = None,\n",
        "        negs: np.ndarray = None):\n",
        "    \"\"\"\n",
        "    Train CBOW model\n",
        "    Based on word2vec.c CBOW implementation from the original source code of the Word2Vec paper\n",
        "\n",
        "    Args:\n",
        "        hs: Hierarchical Softmax flag (0=NS only, 1=HS only). Cannot combine with k>0\n",
        "        k: Negative sampling count (0=HS only, >0=NS only). Cannot combine with hs=1\n",
        "        max_memory_gb: Maximum GPU memory usage in GB. If estimated memory exceeds this,\n",
        "                       the dataset will be automatically split into batches for processing\n",
        "                       Default: 70.0 GB (safe for A100 80GB GPU)\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If both hs=1 and k>0 are specified (HS and NS cannot be combined)\n",
        "    \"\"\"\n",
        "    # Validate: HS and NS cannot be used together\n",
        "    if hs == 1 and k > 0:\n",
        "        raise ValueError(\n",
        "            \"Error: Cannot use HS (hs=1) and Negative Sampling (k>0) together. \"\n",
        "            \"Please choose either HS only (hs=1, k=0) or NS only (hs=0, k>0)\"\n",
        "        )\n",
        "\n",
        "    params = {\n",
        "        \"model_type\": \"cbow\",\n",
        "        \"w2v_version\": W2V_VERSION,\n",
        "        \"data_path\": data_path,\n",
        "        \"out_file_path\": out_file_path,\n",
        "        \"epochs\": epochs,\n",
        "        \"embed_dim\": embed_dim,\n",
        "        \"min_occurs\": min_occurs,\n",
        "        \"c\": c,\n",
        "        \"k\": k,\n",
        "        \"t\": t,\n",
        "        \"vocab_freq_exponent\": vocab_freq_exponent,\n",
        "        \"lr_max\": lr_max,\n",
        "        \"lr_min\": lr_min,\n",
        "        \"cuda_threads_per_block\": cuda_threads_per_block,\n",
        "        \"hs\": hs\n",
        "    }\n",
        "    stats = {}\n",
        "    params_path = out_file_path + \"_params.json\"\n",
        "    stats_path = out_file_path + \"_stats.json\"\n",
        "\n",
        "    seed = 12345\n",
        "\n",
        "    # Adjust learning rate based on training method\n",
        "    original_lr_max = lr_max\n",
        "    original_lr_min = lr_min\n",
        "\n",
        "    # Learning rate handling: HS only and NS only use the same learning rate\n",
        "    # (as per word2vec.c original implementation)\n",
        "    # No special adjustment needed for either method\n",
        "\n",
        "    # Learning rate schedule\n",
        "    # For multiple epochs: decrease between epochs\n",
        "    # For all epochs: decrease LINEARLY within epoch (as per word2vec.c)\n",
        "    if epochs > 1:\n",
        "        lr_step = (lr_max - lr_min) / (epochs - 1)\n",
        "    else:\n",
        "        lr_step = 0.0  # Not used for single epoch (LR decays within epoch)\n",
        "\n",
        "    print(f\"CBOW Training Parameters:\")\n",
        "    print(f\"Seed: {seed}\")\n",
        "    print(f\"Window size: {c}\")\n",
        "    if hs == 1:\n",
        "        print(f\"Hierarchical Softmax: Enabled\")\n",
        "    if k > 0:\n",
        "        print(f\"Negative samples: {k}\")\n",
        "    if original_lr_max != lr_max:\n",
        "        print(f\"Learning rate adjusted: {original_lr_max} -> {lr_max} (reduced for stability)\")\n",
        "    if epochs == 1:\n",
        "        print(f\"Learning rate: {lr_max} -> ~0 (will decrease linearly within epoch, as per word2vec.c)\")\n",
        "    else:\n",
        "        print(f\"Learning rate: {lr_max} -> {lr_min} (step: {lr_step:.6f} between epochs, also decreases linearly within each epoch)\")\n",
        "    print(f\"Embedding dimension: {embed_dim}\")\n",
        "    print(f\"Min word count: {min_occurs}\")\n",
        "\n",
        "    # Start timing for total execution\n",
        "    start = time.time()\n",
        "\n",
        "    # Build vocabulary if not provided (for reuse when training both models)\n",
        "    if vocab is None or w_to_i is None or word_counts is None:\n",
        "        print(f\"\\nBuilding vocabulary from: {data_path}\")\n",
        "        vocab_start = time.time()\n",
        "        vocab, w_to_i, word_counts = handle_vocab(data_path, min_occurs, freq_exponent=vocab_freq_exponent, use_cache=True)\n",
        "        vocab_size = len(vocab)\n",
        "        build_time = time.time() - vocab_start\n",
        "        print(f\"Vocabulary {'loaded from cache' if build_time < 1.0 else 'built'} in {build_time:.2f}s. Vocab size: {vocab_size:,}\")\n",
        "    else:\n",
        "        vocab_size = len(vocab)\n",
        "        print(f\"\\nUsing pre-built vocabulary. Vocab size: {vocab_size:,}\")\n",
        "\n",
        "    # Build subsampling weights and negative sampling array if not provided\n",
        "    if ssw is None or negs is None:\n",
        "        ssw, negs = get_subsampling_weights_and_negative_sampling_array(vocab, t=t)\n",
        "\n",
        "    # Create exp table\n",
        "    print(\"Creating exp table for fast sigmoid...\")\n",
        "    exp_table = create_exp_table(EXP_TABLE_SIZE, MAX_EXP)\n",
        "\n",
        "    # Setup Hierarchical Softmax if enabled\n",
        "    use_hs = (hs == 1)\n",
        "    syn1_cuda = None\n",
        "    codes_array_cuda = None\n",
        "    points_array_cuda = None\n",
        "    code_lengths_cuda = None\n",
        "\n",
        "    if use_hs:\n",
        "        print(\"Creating Huffman tree for Hierarchical Softmax...\")\n",
        "        hs_start = time.time()\n",
        "        codes_array, points_array, code_lengths = create_huffman_tree(word_counts, MAX_CODE_LENGTH)\n",
        "        syn1 = init_hs_weight_matrix(vocab_size, embed_dim)\n",
        "        print(f\"Huffman tree created in {time.time() - hs_start:.2f}s\")\n",
        "        print(f\" -Codes array shape: {codes_array.shape}\")\n",
        "        print(f\" -Points array shape: {points_array.shape}\")\n",
        "        print(f\" -Syn1 matrix shape: {syn1.shape}\")\n",
        "\n",
        "    data_files = get_data_file_names(data_path, seed=seed)\n",
        "    print(f\"Processing {len(data_files)} data files...\")\n",
        "    if max_words is not None:\n",
        "        print(f\"⚠️ Limiting to {max_words:,} total words (will stop early if reached)\")\n",
        "    inps_, offs_, lens_ = read_all_data_files_ever(data_path, data_files, w_to_i, max_words=max_words)\n",
        "    inps, offs, lens = (np.asarray(inps_, dtype=np.int32),\n",
        "                       np.asarray(offs_, dtype=np.int32),\n",
        "                       np.asarray(lens_, dtype=np.int32))\n",
        "    sentence_count = len(lens)\n",
        "    total_words = len(inps)  # Total words for LR decay calculation\n",
        "\n",
        "    print(f\"Data loaded: {sentence_count:,} sentences, {total_words:,} total words\")\n",
        "\n",
        "    # Initialize weight matrices\n",
        "    data_init_start = time.time()\n",
        "    w1, w2 = init_weight_matrices(vocab_size, embed_dim, seed=seed)\n",
        "    data_size_weights = 4 * (w1.size + w2.size)\n",
        "    data_size_inputs = 4 * (inps.size + offs.size + lens.size + ssw.size + negs.size)\n",
        "\n",
        "    # Calculate memory usage and determine batch size\n",
        "    weights_gb = data_size_weights / (1024**3)\n",
        "    inputs_gb = data_size_inputs / (1024**3)\n",
        "\n",
        "    # Estimate calc_aux memory for full dataset\n",
        "    calc_aux_size_full = sentence_count * embed_dim * 4\n",
        "    calc_aux_gb_full = calc_aux_size_full / (1024**3)\n",
        "    total_memory_gb = weights_gb + inputs_gb + calc_aux_gb_full\n",
        "\n",
        "    # Determine if batch processing is needed\n",
        "    use_batch_processing = (total_memory_gb > max_memory_gb)\n",
        "\n",
        "    if use_batch_processing:\n",
        "        # Calculate batch size based on available memory\n",
        "        available_memory_gb = max_memory_gb - weights_gb - inputs_gb\n",
        "        # Reserve 5GB for overhead\n",
        "        available_memory_gb = max(1.0, available_memory_gb - 5.0)\n",
        "\n",
        "        # Calculate max sentences per batch\n",
        "        bytes_per_sentence = embed_dim * 4  # float32\n",
        "        max_batch_sentences = int((available_memory_gb * 1024**3) / bytes_per_sentence)\n",
        "\n",
        "        # Round down to nice numbers for better performance\n",
        "        if max_batch_sentences >= 10_000_000:\n",
        "            batch_size = 10_000_000\n",
        "        elif max_batch_sentences >= 5_000_000:\n",
        "            batch_size = 5_000_000\n",
        "        elif max_batch_sentences >= 2_000_000:\n",
        "            batch_size = 2_000_000\n",
        "        elif max_batch_sentences >= 1_000_000:\n",
        "            batch_size = 1_000_000\n",
        "        else:\n",
        "            batch_size = max(100_000, max_batch_sentences)\n",
        "\n",
        "        num_batches = math.ceil(sentence_count / batch_size)\n",
        "        batch_aux_gb = (batch_size * embed_dim * 4) / (1024**3)\n",
        "        batch_total_gb = weights_gb + inputs_gb + batch_aux_gb\n",
        "\n",
        "        print(f\"\\n⚠️ Memory usage would be {total_memory_gb:.1f} GB (exceeds {max_memory_gb} GB limit)\")\n",
        "        print(f\"Using batch processing: {num_batches} batches, {batch_size:,} sentences/batch\")\n",
        "        print(f\"Memory per batch: {batch_total_gb:.1f} GB (calc_aux: {batch_aux_gb:.1f} GB)\")\n",
        "    else:\n",
        "        batch_size = sentence_count\n",
        "        num_batches = 1\n",
        "        print(f\"\\n✅ Memory usage: {total_memory_gb:.1f} GB (within {max_memory_gb} GB limit)\")\n",
        "        print(f\"Processing all {sentence_count:,} sentences in one batch\")\n",
        "\n",
        "    blocks: int = math.ceil(batch_size / cuda_threads_per_block)\n",
        "    print(f\"CUDA config: {cuda_threads_per_block} threads/block, {blocks} blocks per batch\")\n",
        "\n",
        "    # Transfer to GPU - Transfer weights and vocab arrays (these are shared across batches)\n",
        "    print(\"Transferring data to GPU...\")\n",
        "    data_transfer_start = time.time()\n",
        "    ssw_cuda, negs_cuda = cuda.to_device(ssw), cuda.to_device(negs)\n",
        "    w1_cuda, w2_cuda = cuda.to_device(w1), cuda.to_device(w2)\n",
        "    exp_table_cuda = cuda.to_device(exp_table)\n",
        "\n",
        "    # Keep input arrays on CPU - will slice and transfer per batch\n",
        "    # This saves GPU memory\n",
        "\n",
        "    if use_hs:\n",
        "        syn1_cuda = cuda.to_device(syn1)\n",
        "        codes_array_cuda = cuda.to_device(codes_array)\n",
        "        points_array_cuda = cuda.to_device(points_array)\n",
        "        code_lengths_cuda = cuda.to_device(code_lengths)\n",
        "\n",
        "    print(f\"Data transfer completed in {time.time()-data_transfer_start:.2f}s\")\n",
        "\n",
        "    stats[\"sentence_count\"] = len(lens)\n",
        "    stats[\"word_count\"] = len(inps)\n",
        "    stats[\"vocab_size\"] = vocab_size\n",
        "    stats[\"approx_data_size_weights\"] = data_size_weights\n",
        "    stats[\"approx_data_size_inputs\"] = data_size_inputs\n",
        "    stats[\"use_batch_processing\"] = use_batch_processing\n",
        "    if use_batch_processing:\n",
        "        stats[\"batch_size\"] = batch_size\n",
        "        stats[\"num_batches\"] = num_batches\n",
        "        batch_aux_size = batch_size * embed_dim * 4\n",
        "        stats[\"approx_data_size_aux_per_batch\"] = batch_aux_size\n",
        "        stats[\"approx_data_size_total\"] = data_size_weights + data_size_inputs + batch_aux_size\n",
        "    else:\n",
        "        data_size_aux = 4 * (sentence_count * embed_dim)\n",
        "        stats[\"approx_data_size_aux\"] = data_size_aux\n",
        "        stats[\"approx_data_size_total\"] = data_size_weights + data_size_inputs + data_size_aux\n",
        "\n",
        "    # Prepare HS parameters (use dummy arrays if HS disabled)\n",
        "    if not use_hs:\n",
        "        # Create dummy arrays for HS (will not be used, but needed for kernel signature)\n",
        "        dummy_syn1 = cuda.device_array((1, embed_dim), dtype=np.float32)\n",
        "        dummy_codes = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)\n",
        "        dummy_points = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)\n",
        "        dummy_lengths = cuda.device_array(vocab_size, dtype=np.int32)\n",
        "        syn1_param = dummy_syn1\n",
        "        codes_param = dummy_codes\n",
        "        points_param = dummy_points\n",
        "        lengths_param = dummy_lengths\n",
        "    else:\n",
        "        syn1_param = syn1_cuda\n",
        "        codes_param = codes_array_cuda\n",
        "        points_param = points_array_cuda\n",
        "        lengths_param = code_lengths_cuda\n",
        "\n",
        "    print_norms(w1_cuda)\n",
        "    print(f\"\\nStarting CBOW training - {epochs} epochs...\")\n",
        "    epoch_times = []\n",
        "    calc_start = time.time()\n",
        "\n",
        "    # Track total words processed across all epochs (as per word2vec.c)\n",
        "    # Learning rate decays based on total words processed, not per epoch\n",
        "    # Use int64 to avoid overflow with large datasets and multiple epochs\n",
        "    words_processed_total = np.int64(0)\n",
        "    total_words_for_training = np.int64(epochs) * np.int64(total_words)\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Process each batch\n",
        "        for batch_idx in range(num_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = min((batch_idx + 1) * batch_size, sentence_count)\n",
        "            batch_sentence_count = batch_end - batch_start\n",
        "\n",
        "            if num_batches > 1:\n",
        "                print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{num_batches}: sentences {batch_start:,}-{batch_end:,}\")\n",
        "\n",
        "            # Calculate word offset for this batch (offsets are cumulative)\n",
        "            batch_word_start = offs[batch_start] if batch_start < len(offs) else 0\n",
        "            batch_word_end = offs[batch_end] if batch_end < len(offs) else len(inps)\n",
        "            batch_word_count = batch_word_end - batch_word_start\n",
        "\n",
        "            # Calculate learning rate for this batch (linear decay as per word2vec.c)\n",
        "            # Formula from word2vec.c: alpha = starting_alpha * (1 - word_count_actual / (iter * train_words + 1))\n",
        "            # word_count_actual is total words processed across all epochs\n",
        "            # This ensures LR decreases linearly from lr_max to ~0 over entire training\n",
        "            denominator = total_words_for_training + 1\n",
        "            current_lr = lr_max * (1.0 - words_processed_total / denominator) if denominator > 0 else lr_max\n",
        "\n",
        "            # Apply minimum threshold (as per word2vec.c: min = starting_alpha * 0.0001)\n",
        "            min_lr_threshold = lr_max * 0.0001\n",
        "            current_lr = max(current_lr, min_lr_threshold)\n",
        "\n",
        "            # Also apply lr_min as additional constraint (for multi-epoch training)\n",
        "            if epochs > 1:\n",
        "                current_lr = max(current_lr, lr_min)\n",
        "\n",
        "            if num_batches > 1 and batch_idx == 0:\n",
        "                print(f\"    Learning rate: {current_lr:.6f} (decaying linearly, progress: {words_processed_total/total_words_for_training*100:.1f}%)\")\n",
        "\n",
        "            # Create batch arrays (slicing from CPU arrays)\n",
        "            batch_lens = lens[batch_start:batch_end]\n",
        "            batch_offs_local = offs[batch_start:batch_end] - batch_word_start  # Adjust offsets to start from 0\n",
        "            batch_inps_local = inps[batch_word_start:batch_word_end]\n",
        "\n",
        "            # Transfer batch arrays to GPU\n",
        "            batch_lens_cuda = cuda.to_device(batch_lens)\n",
        "            batch_offs_cuda = cuda.to_device(batch_offs_local)\n",
        "            batch_inps_cuda = cuda.to_device(batch_inps_local)\n",
        "\n",
        "            # Create calc_aux for this batch\n",
        "            batch_calc_aux = np.zeros((batch_sentence_count, embed_dim), dtype=np.float32)\n",
        "            batch_calc_aux_cuda = cuda.to_device(batch_calc_aux)\n",
        "\n",
        "            # Create random states for this batch\n",
        "            batch_random_states_cuda = c_random.create_xoroshiro128p_states(\n",
        "                batch_sentence_count, seed=seed + epoch * 10000 + batch_idx * 100\n",
        "            )\n",
        "\n",
        "            # Launch CUDA kernel for this batch with current learning rate\n",
        "            batch_blocks = math.ceil(batch_sentence_count / cuda_threads_per_block)\n",
        "            calc_cbow[batch_blocks, cuda_threads_per_block](\n",
        "                batch_sentence_count, c, k, current_lr, w1_cuda, w2_cuda, batch_calc_aux_cuda,\n",
        "                batch_random_states_cuda, ssw_cuda, negs_cuda, batch_inps_cuda,\n",
        "                batch_offs_cuda, batch_lens_cuda,\n",
        "                use_hs, syn1_param, codes_param, points_param, lengths_param,\n",
        "                exp_table_cuda, EXP_TABLE_SIZE, MAX_EXP)\n",
        "\n",
        "            # Update total words processed counter (as per word2vec.c)\n",
        "            # Note: Actual words processed may vary due to subsampling, but this is an approximation\n",
        "            # Use int64 to avoid overflow with large datasets and multiple epochs\n",
        "            words_processed_total = np.int64(words_processed_total) + np.int64(batch_word_count)\n",
        "\n",
        "            # Free batch arrays from GPU memory\n",
        "            del batch_lens_cuda, batch_offs_cuda, batch_inps_cuda, batch_calc_aux_cuda, batch_random_states_cuda\n",
        "\n",
        "        # Synchronize after all batches\n",
        "        sync_start = time.time()\n",
        "        cuda.synchronize()\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "\n",
        "        # Final LR after epoch (using same formula as word2vec.c)\n",
        "        denominator = total_words_for_training + 1\n",
        "        final_lr = lr_max * (1.0 - words_processed_total / denominator) if denominator > 0 else lr_max\n",
        "        final_lr = max(final_lr, lr_max * 0.0001)\n",
        "        if epochs > 1:\n",
        "            final_lr = max(final_lr, lr_min)\n",
        "\n",
        "        progress_percent = (words_processed_total / total_words_for_training * 100) if total_words_for_training > 0 else 0.0\n",
        "        print(f\"  Epoch {epoch+1} completed in {epoch_time:.2f}s (LR: {final_lr:.6f}, Progress: {progress_percent:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nCBOW training completed!\")\n",
        "    print(f\"Epoch times - Min: {min(epoch_times):.2f}s, Avg: {np.mean(epoch_times):.2f}s, Max: {max(epoch_times):.2f}s\")\n",
        "    print(f\"Total training time: {time.time()-calc_start:.2f}s\")\n",
        "    print(f\"Total time: {time.time()-start:.2f}s\")\n",
        "\n",
        "    print_norms(w1_cuda)\n",
        "\n",
        "    # Save results\n",
        "    stats[\"epoch_time_min_seconds\"] = min(epoch_times)\n",
        "    stats[\"epoch_time_avg_seconds\"] = np.mean(epoch_times)\n",
        "    stats[\"epoch_time_max_seconds\"] = max(epoch_times)\n",
        "    stats[\"epoch_time_total_seconds\"] = sum(epoch_times)\n",
        "    stats[\"epoch_times_all_seconds\"] = epoch_times\n",
        "\n",
        "    print(f\"Saving CBOW vectors to: {out_file_path}\")\n",
        "    write_vectors(w1_cuda, vocab, out_file_path)\n",
        "\n",
        "    print(f\"Saving parameters to: {params_path}\")\n",
        "    write_json(params, params_path)\n",
        "\n",
        "    print(f\"Saving statistics to: {stats_path}\")\n",
        "    write_json(stats, stats_path)\n",
        "\n",
        "    print(\"CBOW training completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZWO2rTrnozl"
      },
      "source": [
        "# VI. **Evaluation**\n",
        "\n",
        "This cell contains functions for evaluating trained Word2Vec embeddings using standard benchmarks. It provides word analogy testing, similarity analysis, and model comparison capabilities.\n",
        "\n",
        "## **Output Format**\n",
        "\n",
        "- Accuracy metrics: Float values (0.0 to 1.0)\n",
        "- Detailed results: List of dictionaries with correct/incorrect examples per category\n",
        "- Comparison JSON: Includes accuracy, training statistics, and summary metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oEp5R7Gnufc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import requests\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "def download_questions_words(output_path: str = \"./data/questions-words.txt\") -> str:\n",
        "    \"\"\"\n",
        "    Download questions-words.txt for word analogy test\n",
        "    \"\"\"\n",
        "    if os.path.isfile(output_path):\n",
        "        print(f\"Questions-words.txt already exists at: {output_path}\")\n",
        "        return output_path\n",
        "\n",
        "    url = \"https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt\"\n",
        "    print(f\"Downloading questions-words.txt from {url}...\")\n",
        "\n",
        "    with requests.get(url, stream=True) as response:\n",
        "        response.raise_for_status()\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "\n",
        "    print(f\"Questions-words.txt downloaded to: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def word_analogy_test(vectors_path: str, questions_path: str = None) -> Tuple[dict, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Run word analogy test on trained vectors\n",
        "    Returns dictionary containing:\n",
        "        - semantic_accuracy\n",
        "        - syntactic_accuracy\n",
        "        - total_accuracy\n",
        "    And details_by_category (list)\n",
        "    \"\"\"\n",
        "\n",
        "    if questions_path is None:\n",
        "        questions_path = download_questions_words()\n",
        "\n",
        "    print(f\"Loading vectors from: {vectors_path}\")\n",
        "    start = time.time()\n",
        "    vecs = KeyedVectors.load_word2vec_format(vectors_path, binary=False)\n",
        "    print(f\"Vectors loaded in {time.time() - start:.2f}s\")\n",
        "\n",
        "    print(f\"Running word analogy test with: {questions_path}\")\n",
        "    eval_start = time.time()\n",
        "    overall_acc, details = vecs.evaluate_word_analogies(questions_path, case_insensitive=True)\n",
        "    print(f\"Word analogy test completed in {time.time() - eval_start:.2f}s\")\n",
        "\n",
        "    semantic_correct = 0\n",
        "    semantic_total = 0\n",
        "\n",
        "    syntactic_correct = 0\n",
        "    syntactic_total = 0\n",
        "\n",
        "    for cat in details:\n",
        "        correct = len(cat[\"correct\"])\n",
        "        total = correct + len(cat[\"incorrect\"])\n",
        "\n",
        "        # Classify semantic vs syntactic based on categories in questions-words.txt\n",
        "        # Semantic (5 categories): capital-common-countries, capital-world, currency, city-in-state, family\n",
        "        # Syntactic (9 categories): gram1-9 (adjective-to-adverb, opposite, comparative, superlative, etc.)\n",
        "        section = cat[\"section\"].lower()\n",
        "\n",
        "        # Semantic categories keywords (5 categories from questions-words.txt)\n",
        "        # 1. capital-common-countries, capital-world -> \"capital\"\n",
        "        # 2. currency -> \"currency\"\n",
        "        # 3. city-in-state -> \"city-in-state\"\n",
        "        # 4. family -> \"family\"\n",
        "        semantic_keywords = [\"capital\", \"currency\", \"family\", \"city-in-state\"]\n",
        "        is_semantic = any(keyword in section for keyword in semantic_keywords)\n",
        "\n",
        "        if is_semantic:\n",
        "            semantic_correct += correct\n",
        "            semantic_total += total\n",
        "        else:\n",
        "            # Syntactic categories (all remaining categories, usually starting with \"gram\")\n",
        "            syntactic_correct += correct\n",
        "            syntactic_total += total\n",
        "\n",
        "    semantic_acc = semantic_correct / semantic_total if semantic_total > 0 else 0\n",
        "    syntactic_acc = syntactic_correct / syntactic_total if syntactic_total > 0 else 0\n",
        "\n",
        "    # Total overall accuracy\n",
        "    total_acc = (\n",
        "        (semantic_correct + syntactic_correct) /\n",
        "        (semantic_total + syntactic_total)\n",
        "        if (semantic_total + syntactic_total) > 0 else 0\n",
        "    )\n",
        "    return (\n",
        "        {\n",
        "            \"semantic_accuracy\": semantic_acc,\n",
        "            \"syntactic_accuracy\": syntactic_acc,\n",
        "            \"total_accuracy\": total_acc\n",
        "        },\n",
        "        details\n",
        "    )\n",
        "\n",
        "def similarity_test(vectors_path: str, test_words: List[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Test word similarity and find most similar words\n",
        "    \"\"\"\n",
        "    if test_words is None:\n",
        "        test_words = [\"king\", \"queen\", \"man\", \"woman\", \"computer\", \"science\", \"university\", \"student\"]\n",
        "\n",
        "    print(f\"Loading vectors for similarity test: {vectors_path}\")\n",
        "    vecs = KeyedVectors.load_word2vec_format(vectors_path, binary=False)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\nMost similar words:\")\n",
        "    for word in test_words:\n",
        "        if word in vecs:\n",
        "            similar = vecs.most_similar(word, topn=5)\n",
        "            results[word] = similar\n",
        "            print(f\"\\n{word}:\")\n",
        "            for sim_word, score in similar:\n",
        "                print(f\"  {sim_word}: {score:.4f}\")\n",
        "        else:\n",
        "            print(f\"Word '{word}' not found in vocabulary\")\n",
        "            results[word] = []\n",
        "\n",
        "    # Test some word pairs for similarity\n",
        "    word_pairs = [\n",
        "        (\"king\", \"queen\"),\n",
        "        (\"man\", \"woman\"),\n",
        "        (\"computer\", \"science\"),\n",
        "        (\"university\", \"student\"),\n",
        "        (\"good\", \"bad\"),\n",
        "        (\"big\", \"small\")\n",
        "    ]\n",
        "\n",
        "    print(\"\\nWord pair similarities:\")\n",
        "    pair_similarities = {}\n",
        "    for word1, word2 in word_pairs:\n",
        "        if word1 in vecs and word2 in vecs:\n",
        "            similarity = vecs.similarity(word1, word2)\n",
        "            pair_similarities[f\"{word1}-{word2}\"] = similarity\n",
        "            print(f\"  {word1} - {word2}: {similarity:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {word1} - {word2}: One or both words not found\")\n",
        "            pair_similarities[f\"{word1}-{word2}\"] = None\n",
        "\n",
        "    results[\"pair_similarities\"] = pair_similarities\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_evaluation_results(results: Dict[str, Any], output_path: str):\n",
        "    \"\"\"\n",
        "    Save evaluation results to JSON file\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    def convert_numpy_types(obj):\n",
        "        \"\"\"\n",
        "        Convert numpy types to Python native types for JSON serialization\n",
        "        \"\"\"\n",
        "        if isinstance(obj, np.float32):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.float64):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.int32):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.int64):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [convert_numpy_types(item) for item in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    # Convert numpy types to Python native types\n",
        "    results_converted = convert_numpy_types(results)\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results_converted, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Evaluation results saved to: {output_path}\")\n",
        "\n",
        "\n",
        "def compare_models(skipgram_path: str, cbow_path: str, output_path: str = \"./output/model_comparison.json\",\n",
        "                   sg_acc: float = None, sg_details: List[Dict] = None,\n",
        "                   cbow_acc: float = None, cbow_details: List[Dict] = None):\n",
        "    \"\"\"\n",
        "    Compare Skip-gram and CBOW models\n",
        "\n",
        "    Args:\n",
        "        skipgram_path: Path to Skip-gram vectors\n",
        "        cbow_path: Path to CBOW vectors\n",
        "        output_path: Output path for comparison JSON\n",
        "        sg_acc: Pre-computed Skip-gram accuracy (optional, will compute if None)\n",
        "        sg_details: Pre-computed Skip-gram evaluation details (optional)\n",
        "        cbow_acc: Pre-computed CBOW accuracy (optional, will compute if None)\n",
        "        cbow_details: Pre-computed CBOW evaluation details (optional)\n",
        "    \"\"\"\n",
        "    print(\"Comparing Skip-gram vs CBOW models...\")\n",
        "\n",
        "    # Evaluate both models only if not provided\n",
        "    if sg_acc is None or sg_details is None:\n",
        "        print(\"Evaluating Skip-gram model...\")\n",
        "        sg_result, sg_details = word_analogy_test(skipgram_path)\n",
        "        sg_acc = sg_result[\"total_accuracy\"]  # Extract total accuracy from dict\n",
        "    else:\n",
        "        print(\"Using pre-computed Skip-gram accuracy...\")\n",
        "\n",
        "    if cbow_acc is None or cbow_details is None:\n",
        "        print(\"Evaluating CBOW model...\")\n",
        "        cbow_result, cbow_details = word_analogy_test(cbow_path)\n",
        "        cbow_acc = cbow_result[\"total_accuracy\"]  # Extract total accuracy from dict\n",
        "    else:\n",
        "        print(\"Using pre-computed CBOW accuracy...\")\n",
        "\n",
        "    # Load statistics\n",
        "    sg_stats_path = skipgram_path + \"_stats.json\"\n",
        "    cbow_stats_path = cbow_path + \"_stats.json\"\n",
        "\n",
        "    sg_stats = {}\n",
        "    cbow_stats = {}\n",
        "\n",
        "    if os.path.isfile(sg_stats_path):\n",
        "        with open(sg_stats_path, \"r\") as f:\n",
        "            sg_stats = json.load(f)\n",
        "\n",
        "    if os.path.isfile(cbow_stats_path):\n",
        "        with open(cbow_stats_path, \"r\") as f:\n",
        "            cbow_stats = json.load(f)\n",
        "\n",
        "    comparison = {\n",
        "        \"models\": {\n",
        "            \"skipgram\": {\n",
        "                \"accuracy\": sg_acc,\n",
        "                \"details\": sg_details,\n",
        "                \"stats\": sg_stats\n",
        "            },\n",
        "            \"cbow\": {\n",
        "                \"accuracy\": cbow_acc,\n",
        "                \"details\": cbow_details,\n",
        "                \"stats\": cbow_stats\n",
        "            }\n",
        "        },\n",
        "        \"summary\": {\n",
        "            \"skipgram_accuracy\": sg_acc,\n",
        "            \"cbow_accuracy\": cbow_acc,\n",
        "            \"accuracy_difference\": sg_acc - cbow_acc,\n",
        "            \"skipgram_training_time\": sg_stats.get(\"epoch_time_total_seconds\", 0),\n",
        "            \"cbow_training_time\": cbow_stats.get(\"epoch_time_total_seconds\", 0),\n",
        "            \"time_difference\": sg_stats.get(\"epoch_time_total_seconds\", 0) - cbow_stats.get(\"epoch_time_total_seconds\", 0)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    save_evaluation_results(comparison, output_path)\n",
        "\n",
        "    print(f\"\\nModel Comparison Summary:\")\n",
        "    print(f\"Skip-gram accuracy: {sg_acc:.4f} ({sg_acc*100:.2f}%)\")\n",
        "    print(f\"CBOW accuracy: {cbow_acc:.4f} ({cbow_acc*100:.2f}%)\")\n",
        "    print(f\"Difference: {sg_acc - cbow_acc:.4f} ({(sg_acc - cbow_acc)*100:.2f}%)\")\n",
        "\n",
        "    if sg_stats and cbow_stats:\n",
        "        sg_time = sg_stats.get(\"epoch_time_total_seconds\", 0)\n",
        "        cbow_time = cbow_stats.get(\"epoch_time_total_seconds\", 0)\n",
        "        print(f\"Skip-gram training time: {sg_time:.2f}s\")\n",
        "        print(f\"CBOW training time: {cbow_time:.2f}s\")\n",
        "        print(f\"Time difference: {sg_time - cbow_time:.2f}s\")\n",
        "\n",
        "    return comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i16qHfHqpBwr"
      },
      "source": [
        "# VII. **Main Pipeline**\n",
        "\n",
        "This cell contains the main execution pipeline that orchestrates the complete Word2Vec training workflow: dataset download/preprocessing, model training, evaluation, and comparison.\n",
        "\n",
        "## **Configuration Parameters**\n",
        "\n",
        "All configuration is set at the top of the cell. Modify these values to customize the training pipeline:\n",
        "\n",
        "### **Dataset Selection**\n",
        "- **`use_wmt14`** (default: `False`): \n",
        "  - `False` = Use Text8 dataset (smaller, faster)\n",
        "  - `True` = Use WMT14/WMT15 News Crawl (larger, higher quality)\n",
        "- **`dataset_name`** (default: `\"Text8\"`): Display name for the dataset\n",
        "\n",
        "### **Dataset Size Limits** (only for WMT14)\n",
        "- **`max_sentences`** (default: `None`): Maximum number of sentences to process (`None` = all)\n",
        "- **`max_files`** (default: `None`): Maximum number of files to create (`None` = all)\n",
        "- **`max_words`** (default: `None`): Maximum total words for training (`None` = no limit, e.g., `700000000` for 700M words)\n",
        "\n",
        "### **Training Method**\n",
        "- **`use_hs_only`** (default: `True`):\n",
        "  - `True` = Hierarchical Softmax only (HS=1, k=0)\n",
        "  - `False` = Negative Sampling only (HS=0, k=5)\n",
        "\n",
        "### **Model Selection**\n",
        "- **`should_train_skipgram`** (default: `True`): Train Skip-gram model\n",
        "- **`should_train_cbow`** (default: `True`): Train CBOW model\n",
        "\n",
        "### **Phrase Detection**\n",
        "- **`use_phrases`** (default: `False`): Enable phrase detection (combines frequent bigrams like \"new_york\")\n",
        "\n",
        "## **Default Training Parameters**\n",
        "\n",
        "The pipeline uses the following default training parameters (defined in `base_params`):\n",
        "- **`epochs`**: 10\n",
        "- **`embed_dim`**: 300\n",
        "- **`min_occurs`**: 5 (minimum word count threshold)\n",
        "- **`c`**: 5 (context window size)\n",
        "- **`k`**: 0 if `use_hs_only=True`, else 5 (negative samples)\n",
        "- **`t`**: 1e-5 (subsampling threshold)\n",
        "- **`vocab_freq_exponent`**: 0.75 (frequency biasing for negative sampling)\n",
        "- **`lr_max`**: 0.025 (maximum learning rate)\n",
        "- **`lr_min`**: 0.025 if epochs=1, else 0.0001 (minimum learning rate)\n",
        "- **`cuda_threads_per_block`**: 32 (optimized for free tier GPU on Google Colab like GPU T4. Can change to higher value like 512 for GPU A100 (paid GPU on Google Colab) )\n",
        "- **`hs`**: 1 if `use_hs_only=True`, else 0\n",
        "- **`max_words`**: Uses value from configuration above\n",
        "\n",
        "## **Pipeline Steps**\n",
        "\n",
        "1. **Download & Preprocessing**: Downloads dataset and preprocesses into sentence files\n",
        "2. **Build Shared Vocabulary** (if training both models): Builds vocabulary once for reuse\n",
        "3. **Train Skip-gram** (if enabled): Trains Skip-gram model with specified parameters\n",
        "4. **Train CBOW** (if enabled): Trains CBOW model with specified parameters\n",
        "5. **Evaluate Skip-gram**: Runs word analogy test and similarity analysis\n",
        "6. **Evaluate CBOW**: Runs word analogy test and similarity analysis\n",
        "7. **Compare Models**: Side-by-side comparison if both models were trained\n",
        "\n",
        "## **Output Files**\n",
        "\n",
        "All results are saved to `./output/` directory:\n",
        "- **Vectors**: `vectors_skipgram`, `vectors_cbow` (word2vec format)\n",
        "- **Evaluations**: `skipgram_eval.json`, `cbow_eval.json`\n",
        "- **Statistics**: `vectors_skipgram_stats.json`, `vectors_cbow_stats.json`\n",
        "- **Comparison**: `model_comparison.json` (if both models trained)\n",
        "\n",
        "## **Notes**\n",
        "\n",
        "- Vocabulary is cached and reused when training both models (saves time)\n",
        "- If both models are trained, vocabulary is built once and shared\n",
        "- Learning rate schedule: Linear decay based on total words processed (matches word2vec.c)\n",
        "- All steps can be skipped individually by setting corresponding flags to `False`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LSFSdgBDpEcb",
        "outputId": "6d6f3654-f7d1-4a89-abe5-e433901726d4"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURATION - Change these values\n",
        "# ============================================\n",
        "# Dataset selection\n",
        "use_wmt14 = False     # True to use WMT14 News\n",
        "dataset_name = \"Text8\" # \"Text8\" or \"WMT14 News\"\n",
        "\n",
        "# Dataset size (only for WMT14)\n",
        "max_sentences = None   # None = full dataset, or number like 100000\n",
        "max_files = None       # None = all files, or number like 10\n",
        "max_words = None       # None = no limit, or number like 700000000 for 700M words\n",
        "\n",
        "# Training method\n",
        "use_hs_only = True    # True = HS only (HS=1, k=0), False = NS only (HS=0, k=5)\n",
        "\n",
        "# Model selection\n",
        "should_train_skipgram = True  # True to train Skip-gram\n",
        "should_train_cbow = True      # True to train CBOW\n",
        "\n",
        "# Phrase detection\n",
        "use_phrases = False    # True to enable phrase detection\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def print_section_header(title: str):\n",
        "    \"\"\"\n",
        "    Print formatted section header\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  {title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "def print_summary(sg_acc: float, cbow_acc: float, sg_stats: dict, cbow_stats: dict,\n",
        "                 sg_sem: float = None, sg_syn: float = None,\n",
        "                 cbow_sem: float = None, cbow_syn: float = None):\n",
        "    \"\"\"\n",
        "    Print final summary of results\n",
        "    \"\"\"\n",
        "    print_section_header(\"FINAL SUMMARY\")\n",
        "\n",
        "    print(f\"Model Performance:\")\n",
        "    if sg_acc is not None:\n",
        "        print(f\"Skip-gram accuracy: {sg_acc:.4f} ({sg_acc*100:.2f}%)\")\n",
        "        if sg_sem is not None and sg_syn is not None:\n",
        "            print(f\" -Semantic:  {sg_sem:.4f} ({sg_sem*100:.2f}%)\")\n",
        "            print(f\" -Syntactic: {sg_syn:.4f} ({sg_syn*100:.2f}%)\")\n",
        "    if cbow_acc is not None:\n",
        "        print(f\"CBOW accuracy: {cbow_acc:.4f} ({cbow_acc*100:.2f}%)\")\n",
        "        if cbow_sem is not None and cbow_syn is not None:\n",
        "            print(f\" -Semantic:  {cbow_sem:.4f} ({cbow_sem*100:.2f}%)\")\n",
        "            print(f\" -Syntactic: {cbow_syn:.4f} ({cbow_syn*100:.2f}%)\")\n",
        "    if sg_acc is not None and cbow_acc is not None:\n",
        "        print(f\"Difference: {sg_acc - cbow_acc:.4f} ({(sg_acc - cbow_acc)*100:+.2f}%)\")\n",
        "\n",
        "    has_stats = sg_stats or cbow_stats\n",
        "    if has_stats:\n",
        "        print(f\"\\nTraining Times:\")\n",
        "        if sg_stats:\n",
        "            sg_time = sg_stats.get('epoch_time_total_seconds', 0)\n",
        "            print(f\" -Skip-gram: {sg_time:.2f}s\")\n",
        "        if cbow_stats:\n",
        "            cbow_time = cbow_stats.get('epoch_time_total_seconds', 0)\n",
        "            print(f\" -CBOW: {cbow_time:.2f}s\")\n",
        "        if sg_stats and cbow_stats:\n",
        "            sg_time = sg_stats.get('epoch_time_total_seconds', 0)\n",
        "            cbow_time = cbow_stats.get('epoch_time_total_seconds', 0)\n",
        "            print(f\"Difference: {sg_time - cbow_time:.2f}s\")\n",
        "\n",
        "        print(f\"\\nData Processed:\")\n",
        "        stats = sg_stats if sg_stats else cbow_stats\n",
        "        if stats:\n",
        "            words = stats.get('word_count', 0)\n",
        "            print(f\" -Words: {words:,}\")\n",
        "            print(f\" -Sentences: {stats.get('sentence_count', 0):,}\")\n",
        "            print(f\" -Vocabulary: {stats.get('vocab_size', 0):,}\")\n",
        "\n",
        "    print(f\"\\nOutput Files:\")\n",
        "    if sg_acc is not None:\n",
        "        print(f\" -Skip-gram vectors: ./output/vectors_skipgram\")\n",
        "        print(f\" -Skip-gram evaluation: ./output/skipgram_eval.json\")\n",
        "        print(f\" -Skip-gram statistics: ./output/vectors_skipgram_stats.json\")\n",
        "    if cbow_acc is not None:\n",
        "        print(f\" -CBOW vectors: ./output/vectors_cbow\")\n",
        "        print(f\" -CBOW evaluation: ./output/cbow_eval.json\")\n",
        "        print(f\" -CBOW statistics: ./output/vectors_cbow_stats.json\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Print configuration\n",
        "    print(f\"\\nDataset: {dataset_name}\")\n",
        "    if use_wmt14:\n",
        "        print(\" -WMT14/WMT15 News Crawl (combines WMT14 2012 + WMT15 2014)\")\n",
        "        print(\" -Higher quality news articles\")\n",
        "        if max_words:\n",
        "            print(f\" -Limited to {max_words:,} words ({max_words/1e6:.1f}M words)\")\n",
        "        elif max_sentences:\n",
        "            print(f\" -Limited to {max_sentences:,} sentences\")\n",
        "    else:\n",
        "        print(\" -Text8\")\n",
        "        print(\" -Smaller, faster to download and process\")\n",
        "\n",
        "    if use_hs_only:\n",
        "        print(\"Training: Hierarchical Softmax ONLY (HS=1, k=0)\")\n",
        "    else:\n",
        "        print(\"Training: Negative Sampling ONLY (HS=0, k=5)\")\n",
        "\n",
        "    if not should_train_skipgram:\n",
        "        print(\"Skip-gram training: Disabled\")\n",
        "    if not should_train_cbow:\n",
        "        print(\"CBOW training: Disabled\")\n",
        "\n",
        "    if use_phrases:\n",
        "        print(\"Phrase detection: Enabled\")\n",
        "\n",
        "    print_section_header(f\"STEP 1: DOWNLOADING & PREPROCESSING {dataset_name.upper()}\")\n",
        "    data_dir = \"./data\"\n",
        "\n",
        "    if use_phrases:\n",
        "        print(\"Phrase detection: Enabled (will combine frequent bigrams)\")\n",
        "\n",
        "    if use_wmt14:\n",
        "        news_file = download_wmt14_news(data_dir)\n",
        "        processed_dir = preprocess_wmt14_news(news_file, \"./data/wmt14_processed\",\n",
        "                                            max_sentences=max_sentences, max_files=max_files,\n",
        "                                            use_phrases=use_phrases)\n",
        "    else:\n",
        "        text8_file = download_text8(data_dir)\n",
        "        processed_dir = preprocess_text8(text8_file, \"./data/text8_processed\",\n",
        "                                        use_phrases=use_phrases)\n",
        "\n",
        "    # Prepare training parameters (used by both models)\n",
        "    epochs_value = 10  # Set epochs here for consistency\n",
        "    base_params = {\n",
        "        \"epochs\": epochs_value,\n",
        "        \"embed_dim\": 300,\n",
        "        \"min_occurs\": 5,\n",
        "        \"c\": 5,\n",
        "        \"k\": 0 if use_hs_only else 5,\n",
        "        \"t\": 1e-5,\n",
        "        \"vocab_freq_exponent\": 0.75,\n",
        "        \"lr_max\": 0.025,\n",
        "        # For 1 epoch with large dataset, keep learning rate high (as in paper)\n",
        "        \"lr_min\": 0.025 if epochs_value == 1 else 0.0001,\n",
        "        \"cuda_threads_per_block\": 32,  # Optimized for A100 GPU\n",
        "        \"hs\": 1 if use_hs_only else 0,\n",
        "        \"max_words\": max_words  # Limit total words for training (None = no limit)\n",
        "    }\n",
        "\n",
        "    # Build vocabulary once if training both models (to save time)\n",
        "    shared_vocab = None\n",
        "    shared_w_to_i = None\n",
        "    shared_word_counts = None\n",
        "    shared_ssw = None\n",
        "    shared_negs = None\n",
        "\n",
        "    if should_train_skipgram and should_train_cbow:\n",
        "        print_section_header(\"STEP 2: BUILDING SHARED VOCABULARY\")\n",
        "        print(\"Building vocabulary once for both Skip-gram and CBOW models\")\n",
        "        print(\"Vocabulary will be cached for future runs (even with different epochs/dim)\")\n",
        "        import time\n",
        "        start = time.time()\n",
        "        shared_vocab, shared_w_to_i, shared_word_counts = handle_vocab(\n",
        "            processed_dir, base_params[\"min_occurs\"], freq_exponent=base_params[\"vocab_freq_exponent\"], use_cache=True\n",
        "        )\n",
        "        shared_ssw, shared_negs = get_subsampling_weights_and_negative_sampling_array(shared_vocab, t=base_params[\"t\"])\n",
        "        vocab_size = len(shared_vocab)\n",
        "        build_time = time.time() - start\n",
        "        print(f\"✅ Vocabulary {'loaded from cache' if build_time < 1.0 else 'built'} in {build_time:.2f}s. Vocab size: {vocab_size:,}\")\n",
        "        print(f\"✅ Vocabulary will be reused for both models\\n\")\n",
        "\n",
        "    # 2/3. Train Skip-gram (if selected)\n",
        "    if should_train_skipgram:\n",
        "        step_num = 3 if (should_train_skipgram and should_train_cbow) else 2\n",
        "        print_section_header(f\"STEP {step_num}: TRAINING SKIP-GRAM MODEL\")\n",
        "        skipgram_params = base_params.copy()\n",
        "\n",
        "        if epochs_value == 1:\n",
        "            print(\"Using 1 epoch: Learning rate will be kept constant at 0.025 (as per paper)\")\n",
        "\n",
        "        print(\"Skip-gram parameters:\")\n",
        "        for key, value in skipgram_params.items():\n",
        "            print(f\" {key}: {value}\")\n",
        "\n",
        "        # Validate: HS and NS cannot be used together\n",
        "        if skipgram_params[\"hs\"] == 1 and skipgram_params[\"k\"] > 0:\n",
        "            raise ValueError(\"Error: Cannot use HS (hs=1) and Negative Sampling (k>0) together. Please choose either HS only (hs=1, k=0) or NS only (hs=0, k>0).\")\n",
        "\n",
        "        # Pass shared vocabulary if available\n",
        "        if shared_vocab is not None:\n",
        "            train_skipgram(processed_dir, \"./output/vectors_skipgram\",\n",
        "                          vocab=shared_vocab, w_to_i=shared_w_to_i, word_counts=shared_word_counts,\n",
        "                          ssw=shared_ssw, negs=shared_negs, **skipgram_params)\n",
        "        else:\n",
        "            train_skipgram(processed_dir, \"./output/vectors_skipgram\", **skipgram_params)\n",
        "    else:\n",
        "        step_num = 3 if (should_train_skipgram and should_train_cbow) else 2\n",
        "        print_section_header(f\"STEP {step_num}: SKIPPING SKIP-GRAM TRAINING\")\n",
        "        print(\"Skip-gram training skipped as requested\")\n",
        "        skipgram_params = base_params.copy()  # Still need params for CBOW if training both\n",
        "\n",
        "    # 4. Train CBOW (if selected)\n",
        "    if should_train_cbow:\n",
        "        print_section_header(\"STEP 4: TRAINING CBOW MODEL\")\n",
        "        cbow_params = base_params.copy()\n",
        "        # CBOW uses same learning rate as Skip-gram (0.025) to prevent gradient explosion\n",
        "        cbow_params[\"lr_max\"] = 0.025\n",
        "        # For 1 epoch with large dataset, keep learning rate high (same as Skip-gram)\n",
        "        cbow_params[\"lr_min\"] = 0.025 if epochs_value == 1 else 0.0001\n",
        "\n",
        "        if epochs_value == 1:\n",
        "            print(\"Using 1 epoch: Learning rate will be kept constant at 0.025 (same as Skip-gram)\")\n",
        "\n",
        "        print(\"CBOW parameters:\")\n",
        "        for key, value in cbow_params.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "        # Validate: HS and NS cannot be used together\n",
        "        if cbow_params[\"hs\"] == 1 and cbow_params[\"k\"] > 0:\n",
        "            raise ValueError(\"Error: Cannot use HS (hs=1) and Negative Sampling (k>0) together. Please choose either HS only (hs=1, k=0) or NS only (hs=0, k>0).\")\n",
        "\n",
        "        # Pass shared vocabulary if available\n",
        "        if shared_vocab is not None:\n",
        "            train_cbow(processed_dir, \"./output/vectors_cbow\",\n",
        "                      vocab=shared_vocab, w_to_i=shared_w_to_i, word_counts=shared_word_counts,\n",
        "                      ssw=shared_ssw, negs=shared_negs, **cbow_params)\n",
        "        else:\n",
        "            train_cbow(processed_dir, \"./output/vectors_cbow\", **cbow_params)\n",
        "    else:\n",
        "        print_section_header(\"STEP 4: SKIPPING CBOW TRAINING\")\n",
        "        print(\"CBOW training skipped as requested\")\n",
        "\n",
        "    # 5. Evaluate Skip-gram (if trained)\n",
        "    sg_result = None\n",
        "    sg_details = None\n",
        "    sg_sem = None\n",
        "    sg_syn = None\n",
        "    sg_total = None\n",
        "    sg_acc = None\n",
        "    sg_sim = None\n",
        "\n",
        "    if should_train_skipgram:\n",
        "        print_section_header(\"STEP 5: EVALUATING SKIP-GRAM MODEL\")\n",
        "        sg_result, sg_details = word_analogy_test(\"./output/vectors_skipgram\")\n",
        "\n",
        "        sg_sem   = sg_result[\"semantic_accuracy\"]\n",
        "        sg_syn   = sg_result[\"syntactic_accuracy\"]\n",
        "        sg_total = sg_result[\"total_accuracy\"]\n",
        "        sg_acc   = sg_total  # Total accuracy for comparison functions\n",
        "\n",
        "        sg_sim = similarity_test(\"./output/vectors_skipgram\")\n",
        "\n",
        "        save_evaluation_results({\n",
        "            \"semantic_accuracy\": sg_sem,\n",
        "            \"syntactic_accuracy\": sg_syn,\n",
        "            \"total_accuracy\": sg_total,\n",
        "            \"details\": sg_details,\n",
        "            \"similarity_test\": sg_sim\n",
        "        }, \"./output/skipgram_eval.json\")\n",
        "    else:\n",
        "        print_section_header(\"STEP 5: SKIPPING SKIP-GRAM EVALUATION\")\n",
        "        print(\"Skip-gram evaluation skipped (model not trained)\")\n",
        "\n",
        "    # 6. Evaluate CBOW (if trained)\n",
        "    cbow_result = None\n",
        "    cbow_details = None\n",
        "    cbow_sem = None\n",
        "    cbow_syn = None\n",
        "    cbow_total = None\n",
        "    cbow_acc = None\n",
        "    cbow_sim = None\n",
        "\n",
        "    if should_train_cbow:\n",
        "        print_section_header(\"STEP 6: EVALUATING CBOW MODEL\")\n",
        "        cbow_result, cbow_details = word_analogy_test(\"./output/vectors_cbow\")\n",
        "\n",
        "        cbow_sem   = cbow_result[\"semantic_accuracy\"]\n",
        "        cbow_syn   = cbow_result[\"syntactic_accuracy\"]\n",
        "        cbow_total = cbow_result[\"total_accuracy\"]\n",
        "        cbow_acc   = cbow_total  # Total accuracy for comparison functions\n",
        "\n",
        "        cbow_sim = similarity_test(\"./output/vectors_cbow\")\n",
        "\n",
        "        save_evaluation_results({\n",
        "            \"semantic_accuracy\": cbow_sem,\n",
        "            \"syntactic_accuracy\": cbow_syn,\n",
        "            \"total_accuracy\": cbow_total,\n",
        "            \"details\": cbow_details,\n",
        "            \"similarity_test\": cbow_sim\n",
        "        }, \"./output/cbow_eval.json\")\n",
        "    else:\n",
        "        print_section_header(\"STEP 6: SKIPPING CBOW EVALUATION\")\n",
        "        print(\"CBOW evaluation skipped (model not trained)\")\n",
        "\n",
        "    # 7. Model Comparison (Custom Skip-gram vs CBOW) - only if both trained\n",
        "    if should_train_skipgram and should_train_cbow:\n",
        "        print_section_header(\"STEP 7: COMPARING CUSTOM MODELS (Skip-gram vs CBOW)\")\n",
        "        # Pass pre-computed accuracy values to avoid re-evaluating\n",
        "        comparison = compare_models(\"./output/vectors_skipgram\", \"./output/vectors_cbow\",\n",
        "                                    sg_acc=sg_acc, sg_details=sg_details,\n",
        "                                    cbow_acc=cbow_acc, cbow_details=cbow_details)\n",
        "    else:\n",
        "        print_section_header(\"STEP 7: SKIPPING MODEL COMPARISON\")\n",
        "        if should_train_skipgram:\n",
        "            print(\"Model comparison skipped (CBOW not trained)\")\n",
        "        elif should_train_cbow:\n",
        "            print(\"Model comparison skipped (Skip-gram not trained)\")\n",
        "\n",
        "    # Load statistics for summary\n",
        "    sg_stats = {}\n",
        "    cbow_stats = {}\n",
        "\n",
        "    try:\n",
        "        import json\n",
        "        if should_train_skipgram:\n",
        "            try:\n",
        "                with open(\"./output/vectors_skipgram_stats.json\", \"r\") as f:\n",
        "                    sg_stats = json.load(f)\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "        if should_train_cbow:\n",
        "            try:\n",
        "                with open(\"./output/vectors_cbow_stats.json\", \"r\") as f:\n",
        "                    cbow_stats = json.load(f)\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "    except Exception:\n",
        "        print(\"Warning: Could not load statistics files\")\n",
        "\n",
        "    # Final Summary\n",
        "    print_summary(sg_acc, cbow_acc, sg_stats, cbow_stats,\n",
        "                 sg_sem=sg_sem, sg_syn=sg_syn,\n",
        "                 cbow_sem=cbow_sem, cbow_syn=cbow_syn)\n",
        "\n",
        "    print(f\"\\nWord2Vec training and evaluation completed successfully!\")\n",
        "    print(f\"Check the ./output/ directory for all results.\")\n",
        "\n",
        "    print(f\"\\nDataset used: {dataset_name}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n⚠️ Training interrupted by user.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "M0jK_Z_hxwGz",
        "y1FBpun1jxD1",
        "MZI-FINNkFma",
        "h5KqsC97lmeI",
        "jy1oEFb5mTRG",
        "xzcl6Ua8nX70",
        "VZWO2rTrnozl"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
