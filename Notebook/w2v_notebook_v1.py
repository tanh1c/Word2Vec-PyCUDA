# -*- coding: utf-8 -*-
"""W2V_Notebook_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cbNCcj05R5kbG17hUEXgW8iYenK1noAu

# **Install library**
"""

!pip install pynvjitlink-cu12

"""# **Setup & Configuration**"""

#!/usr/bin/env python3
# Copyright 2024 Word2Vec Implementation
# Complete Google Colab setup script - g·ªôp t·∫•t c·∫£ setup v√†o 1 file
# Thay th·∫ø cho c√°c l·ªánh:
#   (User ph·∫£i t·ª± ch·∫°y tr∆∞·ªõc: !uv pip install -q --system numba-cuda==0.4.0)
#   !python setup_numba_cuda.py
#   !python colab_setup.py

import os
import subprocess
import sys

def install_package_with_uv(package: str, quiet: bool = True) -> bool:
    """
    Install package using uv pip (t∆∞∆°ng ƒë∆∞∆°ng v·ªõi: !uv pip install -q --system package)

    Args:
        package: Package name with version (e.g., "numba-cuda==0.4.0")
        quiet: If True, suppress output (equivalent to -q flag)

    Returns:
        True if successful, False otherwise
    """
    try:
        cmd = ["uv", "pip", "install"]
        if quiet:
            cmd.append("-q")
        cmd.extend(["--system", package])

        result = subprocess.run(
            cmd,
            capture_output=quiet,
            text=True,
            check=True
        )

        if not quiet:
            print(f"‚úì {package} installed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Failed to install {package}: {e}")
        if not quiet and e.stdout:
            print(f"  stdout: {e.stdout}")
        if not quiet and e.stderr:
            print(f"  stderr: {e.stderr}")
        return False
    except FileNotFoundError:
        # Fallback to regular pip if uv is not available
        print(f"‚ö†Ô∏è  uv not found, trying regular pip for {package}...")
        try:
            cmd = [sys.executable, "-m", "pip", "install"]
            if quiet:
                cmd.append("-q")
            cmd.append(package)

            subprocess.check_call(cmd)
            if not quiet:
                print(f"‚úì {package} installed successfully (via pip)")
            return True
        except Exception as e2:
            print(f"‚ùå Failed to install {package} with pip: {e2}")
            return False

def check_numba_cuda_installed():
    """
    Check if numba-cuda is already installed (kh√¥ng install n·ªØa).
    User ph·∫£i t·ª± install: !uv pip install -q --system numba-cuda==0.4.0
    """
    print("\n" + "=" * 60)
    print("STEP 1: Checking numba-cuda installation")
    print("=" * 60)
    print("Checking if numba-cuda is installed...")

    try:
        import numba
        from numba import cuda
        print("‚úì numba-cuda is already installed")

        # Check version if possible
        try:
            import numba_cuda
            print(f"  numba version: {numba.__version__ if hasattr(numba, '__version__') else 'unknown'}")
        except:
            pass

        return True
    except ImportError:
        print("‚ùå numba-cuda is NOT installed")
        print("‚ö†Ô∏è  Please install manually first:")
        print("   !uv pip install -q --system numba-cuda==0.4.0")
        return False

def setup_numba_cuda_config():
    """
    Setup numba-cuda configuration (from setup_numba_cuda.py).
    T∆∞∆°ng ƒë∆∞∆°ng v·ªõi: !python setup_numba_cuda.py
    """
    print("\n" + "=" * 60)
    print("STEP 2: Configuring numba-cuda")
    print("=" * 60)
    print("üîß Setting up numba-cuda (Official Solution)")
    print("Based on: https://github.com/googlecolab/colabtools/issues/5081")
    print()

    # Configure numba-cuda
    print("Configuring numba-cuda...")
    try:
        from numba import config
        config.CUDA_ENABLE_PYNVJITLINK = 1
        config.CUDA_LOW_OCCUPANCY_WARNINGS = 0
        print("‚úì numba-cuda configuration set")
        print("  - CUDA_ENABLE_PYNVJITLINK = 1")
        print("  - CUDA_LOW_OCCUPANCY_WARNINGS = 0")
    except ImportError:
        print("‚ùå numba not installed - cannot configure")
        return False
    except Exception as e:
        print(f"‚ùå Failed to configure numba-cuda: {e}")
        return False

    # Test CUDA functionality
    print("\nTesting CUDA functionality...")
    try:
        from numba import cuda
        import numpy as np

        if cuda.is_available():
            device = cuda.get_current_device()
            print(f"‚úì CUDA available: {device.name}")

            # Test simple kernel
            @cuda.jit
            def increment_by_one(an_array):
                pos = cuda.grid(1)
                if pos < an_array.size:
                    an_array[pos] += 1

            test_array = np.zeros(10, dtype=np.float32)
            increment_by_one[16, 16](test_array)

            expected = np.ones(10, dtype=np.float32)
            if np.allclose(test_array, expected):
                print("‚úì CUDA kernel test passed!")
                return True
            else:
                print("‚ùå CUDA kernel test failed")
                return False
        else:
            print("‚ùå CUDA not available")
            return False

    except Exception as e:
        print(f"‚ùå CUDA test failed: {e}")
        return False

def install_all_requirements():
    """
    Install all required packages (from colab_setup.py).
    T∆∞∆°ng ƒë∆∞∆°ng v·ªõi m·ªôt ph·∫ßn c·ªßa: !python colab_setup.py
    """
    print("\n" + "=" * 60)
    print("STEP 3: Installing all required packages")
    print("=" * 60)
    print("Installing required packages for Google Colab...")

    packages = [
        "numpy>=1.20.0",
        "gensim>=4.0.0",
        "scikit-learn>=1.0.0",
        "matplotlib>=3.5.0",
        "seaborn>=0.11.0",
        "tqdm>=4.60.0",
        "requests>=2.25.0",
        "pynvml>=11.0.0"
    ]

    success_count = 0
    failed_packages = []

    for package in packages:
        print(f"Installing {package}...", end=" ", flush=True)
        if install_package_with_uv(package, quiet=True):
            print("‚úì")
            success_count += 1
        else:
            print("‚ùå")
            failed_packages.append(package)

    print(f"\nInstalled {success_count}/{len(packages)} packages successfully")

    if failed_packages:
        print(f"‚ö†Ô∏è  Failed packages: {', '.join(failed_packages)}")
        return False

    return True

def check_gpu():
    """Check GPU availability."""
    print("\n" + "=" * 60)
    print("Checking GPU availability...")
    print("=" * 60)

    try:
        result = subprocess.run(["nvidia-smi"], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úì NVIDIA GPU detected:")
            print(result.stdout)
            return True
        else:
            print("‚ùå No NVIDIA GPU detected")
            return False
    except FileNotFoundError:
        print("‚ùå nvidia-smi not found")
        return False

def check_cuda():
    """Check CUDA availability with Numba."""
    print("\n" + "=" * 60)
    print("Checking CUDA availability...")
    print("=" * 60)

    try:
        from numba import cuda
        if cuda.is_available():
            device = cuda.get_current_device()
            print(f"‚úì CUDA available: {device.name}")

            # Try to get memory info using pynvml if available
            try:
                import pynvml
                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                total_memory = memory_info.total / 1024**3
                print(f"  Memory: {total_memory:.1f} GB")
            except (ImportError, Exception) as e:
                # Fallback: just show device name without memory info
                print(f"  Device: {device.name}")
                print(f"  (Memory info unavailable: {e})")

            return True
        else:
            print("‚ùå CUDA not available")
            return False
    except ImportError:
        print("‚ùå Numba not installed")
        return False

def main():
    """Main setup function - g·ªôp t·∫•t c·∫£ setup."""
    print("=" * 60)
    print("  Word2Vec Implementation - Complete Google Colab Setup")
    print("=" * 60)
    print("\nThis script combines all setup steps:")
    print("  1. Check numba-cuda installation (must be installed separately)")
    print("  2. Configure numba-cuda")
    print("  3. Install all required packages")
    print("  4. Check GPU and CUDA availability")
    print()
    print("‚ö†Ô∏è  NOTE: Please install numba-cuda manually first:")
    print("   !uv pip install -q --system numba-cuda==0.4.0")
    print()

    results = {
        "numba_cuda_installed": False,
        "numba_cuda_configured": False,
        "requirements_installed": False,
        "gpu_available": False,
        "cuda_available": False
    }

    # Step 1: Check numba-cuda installation (kh√¥ng install n·ªØa)
    results["numba_cuda_installed"] = check_numba_cuda_installed()

    if not results["numba_cuda_installed"]:
        print("\n‚ö†Ô∏è  Warning: numba-cuda is not installed. Please install it first:")
        print("   !uv pip install -q --system numba-cuda==0.4.0")
        print("   Continuing with other setup steps...")

    # Step 2: Setup numba-cuda configuration
    results["numba_cuda_configured"] = setup_numba_cuda_config()

    if not results["numba_cuda_configured"]:
        print("\n‚ö†Ô∏è  Warning: Failed to configure numba-cuda. Continuing anyway...")

    # Step 3: Install all requirements
    results["requirements_installed"] = install_all_requirements()

    # Step 4: Check GPU
    results["gpu_available"] = check_gpu()

    # Step 5: Check CUDA
    results["cuda_available"] = check_cuda()

    # Summary
    print("\n" + "=" * 60)
    print("  SETUP SUMMARY")
    print("=" * 60)
    print(f"  ‚úì numba-cuda installed: {'‚úì' if results['numba_cuda_installed'] else '‚ùå'}")
    print(f"  ‚úì numba-cuda configured: {'‚úì' if results['numba_cuda_configured'] else '‚ùå'}")
    print(f"  ‚úì Requirements installed: {'‚úì' if results['requirements_installed'] else '‚ùå'}")
    print(f"  ‚úì GPU available: {'‚úì' if results['gpu_available'] else '‚ùå'}")
    print(f"  ‚úì CUDA available: {'‚úì' if results['cuda_available'] else '‚ùå'}")
    print("=" * 60)

    # Final message
    if results['gpu_available'] and results['cuda_available']:
        print("\nüéâ Setup complete! Ready to run Word2Vec training.")
        print("\nTo run the full pipeline:")
        print("  !python run_all.py")
    elif results['numba_cuda_installed'] and results['numba_cuda_configured']:
        print("\n‚úÖ Setup completed successfully!")
        print("‚ö†Ô∏è  Note: GPU/CUDA may not be available, but CPU training is still possible.")
        print("\nTo run the full pipeline:")
        print("  !python run_all.py")
    else:
        print("\n‚ö†Ô∏è  Setup completed with some warnings.")
        print("Some features may not work correctly.")
        print("\nTo run anyway:")
        print("  !python run_all.py")

    return 0

if __name__ == "__main__":
    # Trong notebook (Colab/Jupyter), kh√¥ng n√™n d√πng sys.exit()
    # v√¨ n√≥ s·∫Ω g√¢y SystemExit exception v√† warning
    # Ch·ªâ g·ªçi main() tr·ª±c ti·∫øp
    main()

    # Note: N·∫øu ch·∫°y t·ª´ command line, c√≥ th·ªÉ d√πng sys.exit(main())
    # nh∆∞ng trong notebook th√¨ kh√¥ng c·∫ßn

"""# **Common Utilities**"""

# Copyright 2024 Word2Vec Implementation
# Based on myw2v by Taneli Saastamoinen
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

import json
import math
import os
import pathlib
import re
import time
from collections import defaultdict
from typing import List, Tuple, Dict, Any

from numba import cuda
import numpy as np
from numpy import linalg, ndarray


W2V_VERSION = "1.0"
BLANK_TOKEN = "<BLANK>"

# Constants for Hierarchical Softmax and Exp Table
EXP_TABLE_SIZE = 1000
MAX_EXP = 6
MAX_CODE_LENGTH = 40


def build_vocab(data_path: str) -> List[Tuple[str, int, int]]:
    """
    Build vocabulary from data files.
    Returns list of (word, total_count, sentence_count).
    """
    files = [fn for fn in os.listdir(data_path) if fn.startswith("0")]
    sentences_per_word = defaultdict(int)
    totals_per_word = defaultdict(int)

    for file in files:
        with open(os.path.join(data_path, file), encoding="utf-8") as f:
            for line in f:
                less_spacey = re.sub(r"[ ]{2,}", " ", line.strip())
                words = less_spacey.split(" ")
                if len(words) > 1:
                    uniques = set()
                    for word in words:
                        uniques.add(word)
                        totals_per_word[word] += 1
                    for deduped in uniques:
                        sentences_per_word[deduped] += 1

    r = []
    for word, total in totals_per_word.items():
        sent = sentences_per_word[word]
        r.append((word, total, sent))
    return r


def sort_vocab(my_vocab: List[Tuple[str, int, int]]) -> List[Tuple[str, int, int]]:
    """Sort vocabulary by frequency (descending), then alphabetically."""
    vs = [(BLANK_TOKEN, 0, 0)] + sorted(my_vocab, key=lambda t: (-t[1], t[0]))
    return vs


def prune_vocab(min_occrs: int, my_vocab: List[Tuple[str, int, int]]) -> List[Tuple[str, int]]:
    """
    Prune vocabulary based on minimum sentence occurrences.
    Returns only total counts.
    """
    if min_occrs > 1:
        totals = [(wrd, total_count) for wrd, total_count, sentence_count in my_vocab
                 if sentence_count >= min_occrs or wrd == BLANK_TOKEN]
        return totals
    else:
        return [(word, total) for word, total, _ in my_vocab]


def bias_freq_counts(vocab: List[Tuple[str, int]], exponent: float) -> List[Tuple[str, float]]:
    """Apply frequency biasing with given exponent for negative sampling."""
    totalsson = sum(count for _, count in vocab)
    plain = [(word, count / totalsson) for word, count in vocab]

    if exponent == 1.0:
        return plain

    exped = [(word, math.pow(count, exponent)) for word, count in plain]
    sum_exped = sum([q for _, q in exped])
    jooh = [(word, f/sum_exped) for word, f in exped]
    return jooh


def handle_vocab(data_path: str, min_occurs_by_sentence: int, freq_exponent: float):
    """
    Complete vocabulary handling pipeline.
    Returns: (biased_vocab, w_to_i, word_counts)
    - biased_vocab: List of (word, frequency) for negative sampling
    - w_to_i: Dictionary mapping word to index
    - word_counts: List of word counts (for Huffman tree construction)
    """
    vocab: List[Tuple[str, int, int]] = build_vocab(data_path)
    sorted_vocab: List[Tuple[str, int, int]] = sort_vocab(vocab)
    pruned_vocab: List[Tuple[str, int]] = prune_vocab(min_occurs_by_sentence, sorted_vocab)
    # Store word counts before biasing
    word_counts = [count for _, count in pruned_vocab]
    biased_vocab: List[Tuple[str, float]] = bias_freq_counts(pruned_vocab, freq_exponent)
    w_to_i: Dict[str, int] = {word: idx for idx, (word, _) in enumerate(biased_vocab)}
    return biased_vocab, w_to_i, word_counts


def get_subsampling_weights_and_negative_sampling_array(vocab: List[Tuple[str, float]], t: float) -> Tuple[ndarray, ndarray]:
    """
    Calculate subsampling weights and create negative sampling array.

    Negative sampling array size is dynamically adjusted based on vocabulary size:
    - For small vocabs (< 10k): uses 1M (original default)
    - For medium vocabs (10k-100k): uses 10M
    - For large vocabs (> 100k): uses 100M (same as word2vec.c original)

    This ensures all words appear in the array and maintains distribution accuracy.
    """
    # Subsampling weights
    tot_wgt: int = sum([c for _, c in vocab])
    freqs: List[float] = [c/tot_wgt for _, c in vocab]
    # Clamp negative probabilities to zero
    probs: List[float] = [max(0.0, 1-math.sqrt(t/freq)) if freq > 0 else 0.0 for freq in freqs]

    # Negative sampling array - precompute for efficient sampling
    vocab_size = len(vocab)

    # Dynamically adjust arr_len based on vocabulary size
    # Word2vec.c original uses 1e8 (100M), we scale based on vocab size
    if vocab_size < 10000:
        arr_len = 1000000  # 1M for small vocabs
    elif vocab_size < 100000:
        arr_len = 10000000  # 10M for medium vocabs
    else:
        arr_len = 100000000  # 100M for large vocabs (same as word2vec.c)

    print(f"Creating negative sampling array with size {arr_len:,} for vocab size {vocab_size:,}")

    w2 = [round(f*arr_len) for f in freqs]

    # Check if any words would be excluded (rounded to 0)
    excluded_count = sum(1 for scaled in w2 if scaled == 0)
    if excluded_count > 0:
        print(f"‚ö†Ô∏è  WARNING: {excluded_count} words have frequency too low and will be excluded from negative sampling")
        print(f"   Consider increasing arr_len or reducing min_occurs threshold")

    neg_arr = []
    for i, scaled in enumerate(w2):
        if scaled > 0:  # Only add words that appear at least once
            neg_arr.extend([i]*scaled)

    actual_arr_size = len(neg_arr)
    print(f"Negative sampling array created: {actual_arr_size:,} entries ({actual_arr_size/1e6:.2f}M)")

    return np.asarray(probs, dtype=np.float32), np.asarray(neg_arr, dtype=np.int32)


def get_data_file_names(path: str, seed: int) -> List[str]:
    """Get shuffled list of data file names."""
    rng = np.random.default_rng(seed=seed)
    qq = [fn for fn in os.listdir(path) if fn.startswith("0")]
    # Sort first to ensure consistent shuffling
    data_files = sorted(qq)
    rng.shuffle(data_files)
    return data_files


def read_all_data_files_ever(dat_path: str, file_names: List[str], w_to_i: Dict[str, int]) -> Tuple[List[int], List[int], List[int]]:
    """Read all data files and convert to indices."""
    start = time.time()
    inps, offs, lens = [], [], []
    offset_total = 0
    stats = defaultdict(int)

    for fn in file_names:
        fp = os.path.join(dat_path, fn)
        ok_lines = 0
        too_short_lines = 0
        with open(fp, encoding="utf-8") as f:
            for line in f:
                words = [word for word in re.split(r"[ .]+", line.strip()) if word]
                if len(words) < 2:
                    too_short_lines += 1
                    continue
                idcs = [w_to_i[w] for w in words if w in w_to_i]
                le = len(idcs)
                ok_lines += 1
                offs.append(offset_total)
                lens.append(le)
                inps.extend(idcs)
                offset_total += le
        stats["file_read_lines_ok"] += ok_lines
        stats["one_word_sentence_lines_which_were_ignored"] += too_short_lines

    print(f"read_all_data_files_ever() STATS: {stats}")
    tot_tm = time.time()-start
    print(f"read_all_data_files_ever() Total time {tot_tm} s for {len(file_names)} files (avg {tot_tm/len(file_names)} s/file)")
    return inps, offs, lens


def init_weight_matrices(vocab_size: int, embed_dim: int, seed: int) -> Tuple[ndarray, ndarray]:
    """Initialize weight matrices with Gaussian distribution."""
    rng = np.random.default_rng(seed=seed)
    rows, cols = vocab_size, embed_dim
    sigma: float = math.sqrt(1.0/cols)
    zs = rng.standard_normal(size=(rows, cols), dtype=np.float32)
    xs = sigma * zs
    # First row all zero since it represents the blank token
    xs[0, :] = 0.0
    zs2 = rng.standard_normal(size=(rows, cols), dtype=np.float32)
    xs2 = sigma * zs2
    xs2[0, :] = 0.0
    return xs, xs2


def print_norms(weights_cuda):
    """Print statistics about vector norms."""
    w = weights_cuda.copy_to_host()
    norms = [linalg.norm(v) for v in w]
    a, med, b = np.percentile(norms, [2.5, 50, 97.5])
    avg = float(sum(norms) / len(norms))
    print(f"Vector norms (count {len(norms)}) 2.5% median mean 97.5%: {a:0.4f}  {med:0.4f}  {avg:0.4f}  {b:0.4f}")


def write_vectors(weights_cuda, vocab: List[Tuple[str, float]], out_path: str):
    """Write vectors to file in word2vec format."""
    w = weights_cuda.copy_to_host()
    pathlib.Path(os.path.dirname(out_path)).mkdir(parents=True, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        # len-1: skip first which is the blank token & all zero
        f.write(f"{len(w)-1} {len(w[0])}\n")
        for i, v in enumerate(w):
            # skip first which is the blank token & all zero
            if i == 0:
                continue
            v_str = " ".join([str(f) for f in v])
            word, _ = vocab[i]
            f.write(f"{word} {v_str}\n")


def write_json(to_jsonify: Dict[str, Any], json_path: str):
    """Write dictionary to JSON file."""
    with open(json_path, "w", encoding="utf-8") as f:
        f.write(json.dumps(to_jsonify))
        f.write("\n")
        f.flush()


def create_exp_table(exp_table_size: int = EXP_TABLE_SIZE, max_exp: float = MAX_EXP) -> ndarray:
    """
    Create precomputed exp table for fast sigmoid calculation.
    Based on word2vec.c lines 708-712.

    Args:
        exp_table_size: Size of the exp table (default: 1000)
        max_exp: Maximum exponent value (default: 6)

    Returns:
        numpy array of precomputed sigmoid values
    """
    exp_table = np.zeros(exp_table_size, dtype=np.float32)
    for i in range(exp_table_size):
        # Precompute exp((i / exp_table_size * 2 - 1) * max_exp)
        exp_value = math.exp((i / exp_table_size * 2 - 1) * max_exp)
        # Precompute sigmoid: exp(x) / (exp(x) + 1)
        exp_table[i] = exp_value / (exp_value + 1)
    return exp_table


def init_hs_weight_matrix(vocab_size: int, embed_dim: int) -> ndarray:
    """
    Initialize Hierarchical Softmax weight matrix (syn1).
    Based on word2vec.c lines 356-359.

    Args:
        vocab_size: Vocabulary size
        embed_dim: Embedding dimension

    Returns:
        Weight matrix for internal nodes: (vocab_size - 1, embed_dim)
        Initialized with zeros
    """
    # Internal nodes: vocab_size - 1
    syn1 = np.zeros((vocab_size - 1, embed_dim), dtype=np.float32)
    return syn1


def create_huffman_tree(word_counts: List[int], max_code_length: int = MAX_CODE_LENGTH) -> Tuple[ndarray, ndarray, ndarray]:
    """
    Create binary Huffman tree from word counts.
    Based on word2vec.c lines 205-270.

    Frequent words will have short unique binary codes.

    Args:
        word_counts: List of word counts (frequencies)
        max_code_length: Maximum code length (default: 40)

    Returns:
        Tuple of (codes_array, points_array, code_lengths):
        - codes_array: (vocab_size, max_code_length) binary codes, padded with -1
        - points_array: (vocab_size, max_code_length) node indices in path, padded with -1
        - code_lengths: (vocab_size,) code length for each word
    """
    vocab_size = len(word_counts)

    # Initialize arrays
    count = np.zeros(vocab_size * 2 + 1, dtype=np.int64)
    binary = np.zeros(vocab_size * 2 + 1, dtype=np.int32)
    parent_node = np.zeros(vocab_size * 2 + 1, dtype=np.int64)

    # Set initial counts
    for a in range(vocab_size):
        count[a] = word_counts[a]
    for a in range(vocab_size, vocab_size * 2):
        count[a] = int(1e15)  # Large value for internal nodes

    # Build Huffman tree
    pos1 = vocab_size - 1
    pos2 = vocab_size

    for a in range(vocab_size - 1):
        # Find two smallest nodes
        if pos1 >= 0:
            if count[pos1] < count[pos2]:
                min1i = pos1
                pos1 -= 1
            else:
                min1i = pos2
                pos2 += 1
        else:
            min1i = pos2
            pos2 += 1

        if pos1 >= 0:
            if count[pos1] < count[pos2]:
                min2i = pos1
                pos1 -= 1
            else:
                min2i = pos2
                pos2 += 1
        else:
            min2i = pos2
            pos2 += 1

        count[vocab_size + a] = count[min1i] + count[min2i]
        parent_node[min1i] = vocab_size + a
        parent_node[min2i] = vocab_size + a
        binary[min2i] = 1

    # Assign binary codes to each word
    codes_array = np.full((vocab_size, max_code_length), -1, dtype=np.int32)
    points_array = np.full((vocab_size, max_code_length), -1, dtype=np.int32)
    code_lengths = np.zeros(vocab_size, dtype=np.int32)

    for a in range(vocab_size):
        b = a
        i = 0
        code = np.zeros(max_code_length, dtype=np.int32)
        point = np.zeros(max_code_length, dtype=np.int64)

        # Traverse from leaf to root
        while True:
            code[i] = binary[b]
            point[i] = b
            i += 1
            b = parent_node[b]
            if b == vocab_size * 2 - 2:
                break
            if i >= max_code_length:
                break  # Safety check

        code_lengths[a] = i
        # Store code and point arrays (reversed)
        points_array[a, 0] = vocab_size - 2  # Root node
        for b_idx in range(i):
            codes_array[a, i - b_idx - 1] = code[b_idx]
            if b_idx < i - 1:
                points_array[a, i - b_idx] = int(point[b_idx] - vocab_size)

    return codes_array, points_array, code_lengths

"""# **Data Handler**"""

# Copyright 2024 Word2Vec Implementation
# Data handling utilities for text8 dataset

import os
import pathlib
import re
import time
import zipfile
import gzip
import json
from typing import List, Tuple, Dict
from collections import defaultdict
import requests
import tqdm


def clean_text_remove_punctuation(text: str) -> str:
    """
    Clean text by removing punctuation and normalizing whitespace.
    Similar to word2vec preprocessing - only keeps letters and spaces.

    Args:
        text: Input text line

    Returns:
        Cleaned text with only lowercase letters and spaces
    """
    if not text:
        return ""

    # Replace tabs and newlines with spaces
    text = re.sub(r'[\t\n]', ' ', text)

    # Normalize multiple spaces to single space
    text = re.sub(r'[ ]{2,}', ' ', text)

    # Remove all punctuation, keep only letters and spaces
    text = re.sub(r'[^a-zA-Z ]', '', text)

    # Convert to lowercase and strip
    text = text.lower().strip()

    return text


def detect_phrases(text: str, word_counts: Dict[str, int], bigram_counts: Dict[Tuple[str, str], int],
                   train_words: int, min_count: int = 5, threshold: float = 100.0) -> str:
    """
    Detect and combine phrases in text based on bigram scores.
    Based on word2phrase.c TrainModel() function.

    Args:
        text: Input text (space-separated words)
        word_counts: Dictionary mapping words to their counts
        bigram_counts: Dictionary mapping (word1, word2) tuples to bigram counts
        train_words: Total number of words in training data
        min_count: Minimum word count threshold
        threshold: Score threshold for phrase formation (higher = fewer phrases)

    Returns:
        Text with phrases combined (e.g., "new york" -> "new_york")
    """
    words = text.split()
    if len(words) < 2:
        return text

    result = []
    i = 0
    while i < len(words):
        if i == len(words) - 1:
            # Last word, no bigram possible
            result.append(words[i])
            break

        word1 = words[i]
        word2 = words[i + 1]

        # Check if both words meet min_count
        count1 = word_counts.get(word1, 0)
        count2 = word_counts.get(word2, 0)

        if count1 < min_count or count2 < min_count:
            # One word doesn't meet threshold, keep as separate
            result.append(word1)
            i += 1
            continue

        # Calculate bigram score
        bigram = (word1, word2)
        count_bigram = bigram_counts.get(bigram, 0)

        if count_bigram == 0:
            # Bigram not found, keep as separate
            result.append(word1)
            i += 1
            continue

        # Score formula from word2phrase.c line 285
        # score = (pab - min_count) / pa / pb * train_words
        score = (count_bigram - min_count) / count1 / count2 * train_words

        if score > threshold:
            # Combine into phrase
            result.append(f"{word1}_{word2}")
            i += 2  # Skip both words
        else:
            # Keep as separate
            result.append(word1)
            i += 1

    return " ".join(result)


def learn_phrase_vocab(data_path: str, min_count: int = 5) -> Tuple[Dict[str, int], Dict[Tuple[str, str], int], int]:
    """
    Learn vocabulary and bigram counts from training data.
    Based on word2phrase.c LearnVocabFromTrainFile() function.

    Args:
        data_path: Path to training data directory
        min_count: Minimum word count threshold

    Returns:
        Tuple of (word_counts, bigram_counts, total_words)
    """
    word_counts = defaultdict(int)
    bigram_counts = defaultdict(int)
    total_words = 0

    # Get all data files
    data_files = [f for f in os.listdir(data_path) if f.startswith("0")]
    data_files.sort()

    print(f"Learning phrase vocabulary from {len(data_files)} files...")

    for file_idx, filename in enumerate(data_files):
        filepath = os.path.join(data_path, filename)
        last_word = None
        start = True

        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    start = True
                    last_word = None
                    continue

                words = line.split()
                for word in words:
                    word = word.lower().strip()
                    if not word:
                        continue

                    total_words += 1

                    # Count unigram
                    word_counts[word] += 1

                    # Count bigram (if not at start of sentence)
                    if not start and last_word:
                        bigram = (last_word, word)
                        bigram_counts[bigram] += 1

                    last_word = word
                    start = False

                # Reset at end of line
                start = True
                last_word = None

        if (file_idx + 1) % 10 == 0:
            print(f"  Processed {file_idx + 1}/{len(data_files)} files...")

    # Filter words below min_count
    filtered_word_counts = {w: c for w, c in word_counts.items() if c >= min_count}

    print(f"Vocabulary: {len(filtered_word_counts):,} words (min_count={min_count})")
    print(f"Bigrams: {len(bigram_counts):,} unique bigrams")
    print(f"Total words: {total_words:,}")

    return filtered_word_counts, bigram_counts, total_words


def apply_phrases_to_data(data_path: str, output_path: str, word_counts: Dict[str, int],
                          bigram_counts: Dict[Tuple[str, str], int], train_words: int,
                          min_count: int = 5, threshold: float = 100.0) -> str:
    """
    Apply phrase detection to all data files.

    Args:
        data_path: Input data directory
        output_path: Output data directory
        word_counts: Word count dictionary
        bigram_counts: Bigram count dictionary
        train_words: Total number of words
        min_count: Minimum word count
        threshold: Phrase score threshold

    Returns:
        Path to output directory
    """
    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)

    data_files = [f for f in os.listdir(data_path) if f.startswith("0")]
    data_files.sort()

    print(f"Applying phrase detection (threshold={threshold}) to {len(data_files)} files...")

    for file_idx, filename in enumerate(data_files):
        input_filepath = os.path.join(data_path, filename)
        output_filepath = os.path.join(output_path, filename)

        with open(input_filepath, 'r', encoding='utf-8') as fin, \
             open(output_filepath, 'w', encoding='utf-8') as fout:

            for line in fin:
                line = line.strip()
                if not line:
                    fout.write('\n')
                    continue

                # Apply phrase detection
                processed_line = detect_phrases(line, word_counts, bigram_counts,
                                                train_words, min_count, threshold)
                fout.write(processed_line + '\n')

        if (file_idx + 1) % 10 == 0:
            print(f"  Processed {file_idx + 1}/{len(data_files)} files...")

    print(f"Phrase detection complete. Output: {output_path}")
    return output_path


def preprocess_with_phrases(data_path: str, output_path: str, min_count: int = 5,
                            threshold1: float = 200.0, threshold2: float = 100.0) -> str:
    """
    Preprocess data with phrase detection (2 passes, like word2phrase).

    Args:
        data_path: Input data directory
        output_path: Final output directory
        min_count: Minimum word count
        threshold1: First pass threshold (higher, fewer phrases)
        threshold2: Second pass threshold (lower, more phrases)

    Returns:
        Path to final output directory
    """
    print(f"Preprocessing with phrase detection...")
    print(f"  Input: {data_path}")
    print(f"  Output: {output_path}")
    print(f"  Threshold 1: {threshold1} (first pass)")
    print(f"  Threshold 2: {threshold2} (second pass)")

    # Step 1: Learn vocabulary and bigram counts
    print("\nStep 1: Learning vocabulary and bigram counts...")
    word_counts, bigram_counts, train_words = learn_phrase_vocab(data_path, min_count)

    # Step 2: First pass (threshold1)
    print(f"\nStep 2: First pass phrase detection (threshold={threshold1})...")
    temp_path1 = output_path + "_phrase1"
    apply_phrases_to_data(data_path, temp_path1, word_counts, bigram_counts,
                          train_words, min_count, threshold1)

    # Step 3: Relearn vocabulary from first pass
    print("\nStep 3: Relearning vocabulary from first pass...")
    word_counts2, bigram_counts2, train_words2 = learn_phrase_vocab(temp_path1, min_count)

    # Step 4: Second pass (threshold2)
    print(f"\nStep 4: Second pass phrase detection (threshold={threshold2})...")
    apply_phrases_to_data(temp_path1, output_path, word_counts2, bigram_counts2,
                          train_words2, min_count, threshold2)

    # Cleanup temp directory
    import shutil
    if os.path.exists(temp_path1):
        shutil.rmtree(temp_path1)
        print(f"Cleaned up temporary directory: {temp_path1}")

    print(f"\nPhrase preprocessing complete: {output_path}")
    return output_path


def download_wmt14_news(output_dir: str = "./data") -> str:
    """
    Download WMT14 News Crawl dataset from http://www.statmt.org/wmt14/training-monolingual-news-crawl/
    Returns path to downloaded news file.
    """
    train_file = "news.2012.en.shuffled"
    train_gz = f"{train_file}.gz"
    train_url = "http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2012.en.shuffled.gz"

    output_path = os.path.join(output_dir, "wmt14")
    news_file = os.path.join(output_path, train_file)

    # Create output directory
    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)

    # Check if already exists
    if os.path.isfile(news_file):
        print(f"WMT14 News file already exists at: {news_file}")
        return news_file

    gz_path = os.path.join(output_path, train_gz)

    # Download if missing
    if not os.path.isfile(gz_path):
        print(f"Downloading {train_gz} from {train_url}...")
        with requests.get(train_url, stream=True) as response:
            response.raise_for_status()
            total_size = int(response.headers.get('content-length', 0))

            with open(gz_path, 'wb') as f:
                with tqdm.tqdm(total=total_size, unit='B', unit_scale=True, desc="Downloading") as pbar:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))

    # Extract if needed
    if not os.path.isfile(news_file):
        print(f"Extracting {gz_path}...")
        with gzip.open(gz_path, "rb") as source, open(news_file, "wb") as target:
            target.write(source.read())
        # Remove gz file to save space
        os.remove(gz_path)

    print(f"WMT14 News dataset ready at: {news_file}")
    return news_file


def download_text8(output_dir: str = "./data") -> str:
    """
    Download text8 dataset from http://mattmahoney.net/dc/text8.zip
    Returns path to downloaded text8 file.
    """
    url = "http://mattmahoney.net/dc/text8.zip"
    output_path = os.path.join(output_dir, "text8")
    text8_file = os.path.join(output_path, "text8")

    # Create output directory
    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)

    # Check if already exists
    if os.path.isfile(text8_file):
        print(f"Text8 file already exists at: {text8_file}")
        return text8_file

    zip_path = os.path.join(output_path, "text8.zip")

    print(f"Downloading text8 from {url}...")
    with requests.get(url, stream=True) as response:
        response.raise_for_status()
        total_size = int(response.headers.get('content-length', 0))

        with open(zip_path, 'wb') as f:
            with tqdm.tqdm(total=total_size, unit='B', unit_scale=True, desc="Downloading") as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))

    print(f"Extracting {zip_path}...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(output_path)

    # Remove zip file to save space
    os.remove(zip_path)

    print(f"Text8 dataset ready at: {text8_file}")
    return text8_file


def preprocess_wmt14_news(news_file_path: str, output_dir: str, words_per_sentence: int = 1000,
                        max_sentences: int = None, max_files: int = None, use_phrases: bool = False,
                        phrase_threshold1: float = 200.0, phrase_threshold2: float = 100.0) -> str:
    """
    Preprocess WMT14 news file into sentence files compatible with myw2v format.

    This function now removes punctuation (commas, periods, etc.) and normalizes text.
    NOTE: If you have previously processed data that still contains punctuation,
    you need to delete the old processed files and reprocess to apply the cleaning.

    Args:
        news_file_path: Path to WMT14 news file
        output_dir: Output directory for processed files
        words_per_sentence: Number of words per sentence (default: 1000)
        max_sentences: Maximum number of sentences to process (None = all)
        max_files: Maximum number of files to create (None = all)
        use_phrases: Whether to apply phrase detection (default: False)
        phrase_threshold1: First pass phrase threshold (default: 200.0)
        phrase_threshold2: Second pass phrase threshold (default: 100.0)

    Returns:
        Path to output directory
    """
    print(f"Preprocessing WMT14 news file: {news_file_path}")
    print(f"Output directory: {output_dir}")
    print(f"Words per sentence: {words_per_sentence}")
    print("Note: Punctuation will be removed from text (commas, periods, etc.)")
    if max_sentences:
        print(f"Max sentences: {max_sentences:,}")
    if max_files:
        print(f"Max files: {max_files}")
    if use_phrases:
        print(f"Phrase detection: Enabled (threshold1={phrase_threshold1}, threshold2={phrase_threshold2})")

    # Create output directory
    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)

    # Check if already processed
    existing_files = [f for f in os.listdir(output_dir) if f.startswith("0")]
    if existing_files:
        print(f"Found {len(existing_files)} existing processed files. Skipping preprocessing.")
        print("‚ö†Ô∏è  WARNING: If these files contain punctuation, delete them and reprocess to apply cleaning.")
        return output_dir

    # Step 1: Basic preprocessing
    temp_dir = output_dir + "_temp"
    pathlib.Path(temp_dir).mkdir(parents=True, exist_ok=True)

    # Read news file (one sentence per line)
    sentences = []
    sentence_count = 0

    with open(news_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            # Clean text: remove punctuation and normalize
            cleaned_line = clean_text_remove_punctuation(line)
            if cleaned_line:  # Skip empty lines after cleaning
                # Split into words and group into chunks
                words = cleaned_line.split()
                for i in range(0, len(words), words_per_sentence):
                    sentence_words = words[i:i + words_per_sentence]
                    if len(sentence_words) >= 2:  # Skip very short sentences
                        sentences.append(" ".join(sentence_words))
                        sentence_count += 1

                        # Stop if we've reached max_sentences
                        if max_sentences and sentence_count >= max_sentences:
                            print(f"Reached max_sentences limit: {max_sentences:,}")
                            break

                # Break outer loop if we've reached max_sentences
                if max_sentences and sentence_count >= max_sentences:
                    break

    print(f"Total sentences: {len(sentences):,}")

    # Save to temporary files (similar to myw2v format)
    sentences_per_file = 100000
    file_count = 0
    current_file_sentences = []

    for i, sentence in enumerate(sentences):
        current_file_sentences.append(sentence)

        # Write file when it reaches sentences_per_file or we're at the end
        if len(current_file_sentences) >= sentences_per_file or i == len(sentences) - 1:
            filename = f"{file_count:04d}"
            filepath = os.path.join(temp_dir, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                for sent in current_file_sentences:
                    f.write(sent + '\n')

            print(f"Wrote {len(current_file_sentences):,} sentences to {filename}")
            file_count += 1
            current_file_sentences = []

            # Stop if we've reached max_files
            if max_files and file_count >= max_files:
                print(f"Reached max_files limit: {max_files}")
                break

    # Step 2: Apply phrase detection if enabled
    if use_phrases:
        print("\nApplying phrase detection...")
        preprocess_with_phrases(temp_dir, output_dir, min_count=5,
                               threshold1=phrase_threshold1, threshold2=phrase_threshold2)
        # Cleanup temp directory
        import shutil
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
    else:
        # Just move files from temp to output
        import shutil
        if os.path.exists(output_dir):
            shutil.rmtree(output_dir)
        shutil.move(temp_dir, output_dir)

    print(f"Preprocessing complete. Created {file_count} files in {output_dir}")
    return output_dir


def preprocess_text8(text8_file_path: str, output_dir: str, words_per_sentence: int = 1000,
                    use_phrases: bool = False, phrase_threshold1: float = 200.0,
                    phrase_threshold2: float = 100.0) -> str:
    """
    Preprocess text8 file into sentence files compatible with myw2v format.

    Args:
        text8_file_path: Path to text8 file
        output_dir: Output directory for processed files
        words_per_sentence: Number of words per sentence (default: 1000)
        use_phrases: Whether to apply phrase detection (default: False)
        phrase_threshold1: First pass phrase threshold (default: 200.0)
        phrase_threshold2: Second pass phrase threshold (default: 100.0)

    Returns:
        Path to output directory
    """
    print(f"Preprocessing text8 file: {text8_file_path}")
    print(f"Output directory: {output_dir}")
    print(f"Words per sentence: {words_per_sentence}")
    if use_phrases:
        print(f"Phrase detection: Enabled (threshold1={phrase_threshold1}, threshold2={phrase_threshold2})")

    # Create output directory
    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)

    # Check if already processed
    existing_files = [f for f in os.listdir(output_dir) if f.startswith("0")]
    if existing_files:
        print(f"Found {len(existing_files)} existing processed files. Skipping preprocessing.")
        return output_dir

    # Step 1: Basic preprocessing
    temp_dir = output_dir + "_temp"
    pathlib.Path(temp_dir).mkdir(parents=True, exist_ok=True)

    # Read text8 file (single long line)
    with open(text8_file_path, 'r', encoding='utf-8') as f:
        text = f.read().strip()

    # Split into words
    words = text.split()
    print(f"Total words: {len(words):,}")

    # Group into sentences
    sentences = []
    for i in range(0, len(words), words_per_sentence):
        sentence_words = words[i:i + words_per_sentence]
        if len(sentence_words) >= 2:  # Skip very short sentences
            sentences.append(" ".join(sentence_words))

    print(f"Created {len(sentences):,} sentences")

    # Save to temporary files (similar to myw2v format)
    sentences_per_file = 100000
    file_count = 0
    current_file_sentences = []

    for i, sentence in enumerate(sentences):
        current_file_sentences.append(sentence)

        # Write file when it reaches sentences_per_file or we're at the end
        if len(current_file_sentences) >= sentences_per_file or i == len(sentences) - 1:
            filename = f"{file_count:04d}"
            filepath = os.path.join(temp_dir, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                for sent in current_file_sentences:
                    f.write(sent + '\n')

            print(f"Wrote {len(current_file_sentences):,} sentences to {filename}")
            file_count += 1
            current_file_sentences = []

    # Step 2: Apply phrase detection if enabled
    if use_phrases:
        print("\nApplying phrase detection...")
        preprocess_with_phrases(temp_dir, output_dir, min_count=5,
                               threshold1=phrase_threshold1, threshold2=phrase_threshold2)
        # Cleanup temp directory
        import shutil
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
    else:
        # Just move files from temp to output
        import shutil
        if os.path.exists(output_dir):
            shutil.rmtree(output_dir)
        shutil.move(temp_dir, output_dir)

    print(f"Preprocessing complete. Created {file_count} files in {output_dir}")
    return output_dir


def get_data_file_names(path: str, seed: int) -> List[str]:
    """Get shuffled list of data file names."""
    import numpy as np
    rng = np.random.default_rng(seed=seed)
    qq = [fn for fn in os.listdir(path) if fn.startswith("0")]
    # Sort first to ensure consistent shuffling
    data_files = sorted(qq)
    rng.shuffle(data_files)
    return data_files


def read_all_data_files(data_path: str, file_names: List[str], word_to_idx: dict) -> Tuple[List[int], List[int], List[int]]:
    """
    Read all data files and convert words to indices.
    Returns (inputs, offsets, lengths) compatible with myw2v format.
    """
    from collections import defaultdict

    start = time.time()
    inps, offs, lens = [], [], []
    offset_total = 0
    stats = defaultdict(int)

    for fn in file_names:
        fp = os.path.join(data_path, fn)
        ok_lines = 0
        too_short_lines = 0
        with open(fp, encoding="utf-8") as f:
            for line in f:
                words = [word for word in re.split(r"[ .]+", line.strip()) if word]
                if len(words) < 2:
                    too_short_lines += 1
                    continue
                idcs = [word_to_idx[w] for w in words if w in word_to_idx]
                le = len(idcs)
                ok_lines += 1
                offs.append(offset_total)
                lens.append(le)
                inps.extend(idcs)
                offset_total += le
        stats["file_read_lines_ok"] += ok_lines
        stats["one_word_sentence_lines_which_were_ignored"] += too_short_lines

    print(f"read_all_data_files() STATS: {stats}")
    tot_tm = time.time()-start
    print(f"read_all_data_files() Total time {tot_tm} s for {len(file_names)} files (avg {tot_tm/len(file_names)} s/file)")
    return inps, offs, lens

"""# **Skip-gram Implementation**"""

# Copyright 2024 Word2Vec Implementation
# Skip-gram implementation with Numba CUDA

import math
import os
import time
from typing import List, Tuple, Dict, Any

from numba import cuda
from numba.cuda import random as c_random
import numpy as np
from numpy import ndarray

# C√°c h√†m t·ª´ w2v_common ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong cell "Common Utilities" ·ªü tr√™n
# Kh√¥ng c·∫ßn import trong notebook


@cuda.jit
def calc_skipgram(
        rows: int,
        c: int,
        k: int,
        learning_rate: float,
        w1,
        w2,
        calc_aux,
        random_states,
        subsample_weights,
        negsample_array,
        inp,
        offsets,
        lengths,
        use_hs,
        syn1,
        codes_array,
        points_array,
        code_lengths,
        exp_table,
        exp_table_size,
        max_exp):
    """
    CUDA kernel for Skip-gram training.
    Based on word2vec.c Skip-gram implementation (lines 495-543).
    Supports both Hierarchical Softmax and Negative Sampling.
    """
    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x
    if idx >= rows:
        return
    le = lengths[idx]
    off = offsets[idx]

    for centre in range(0, le):
        word_idx = inp[off + centre]
        prob_to_reject = subsample_weights[word_idx]
        rnd = c_random.xoroshiro128p_uniform_float32(random_states, idx)

        if rnd > prob_to_reject:
            r_f = c_random.xoroshiro128p_uniform_float32(random_states, idx)
            r: int = math.ceil(r_f * c)

            # Context before center word
            for context_pre in range(max(0, centre-r), centre):
                step_skipgram(idx, w1, w2, calc_aux, inp[off+centre], inp[off+context_pre],
                             k, learning_rate, negsample_array, random_states,
                             use_hs, syn1, codes_array, points_array, code_lengths,
                             exp_table, exp_table_size, max_exp)

            # Context after center word
            for context_post in range(centre + 1, min(le, centre + 1 + r)):
                step_skipgram(idx, w1, w2, calc_aux, inp[off+centre], inp[off+context_post],
                             k, learning_rate, negsample_array, random_states,
                             use_hs, syn1, codes_array, points_array, code_lengths,
                             exp_table, exp_table_size, max_exp)


@cuda.jit(device=True)
def fast_sigmoid(f, exp_table, exp_table_size, max_exp):
    """
    Fast sigmoid using precomputed exp table.
    Based on word2vec.c exp table lookup.
    """
    if f <= -max_exp:
        return 0.0
    elif f >= max_exp:
        return 1.0
    else:
        idx = int((f + max_exp) * (exp_table_size / max_exp / 2.0))
        if idx < 0:
            idx = 0
        if idx >= exp_table_size:
            idx = exp_table_size - 1
        return exp_table[idx]


@cuda.jit(device=True)
def step_skipgram(thread_idx, w1, w2, calc_aux, x, y, k, learning_rate, negsample_array, random_states,
                  use_hs, syn1, codes_array, points_array, code_lengths,
                  exp_table, exp_table_size, max_exp):
    """
    Device function for Skip-gram gradient calculation.
    Based on word2vec.c Skip-gram implementation (lines 505-519).
    Supports both Hierarchical Softmax and Negative Sampling.
    """
    emb_dim = w1.shape[1]
    negs_arr_len = len(negsample_array)

    # Initialize error accumulator
    for i in range(emb_dim):
        calc_aux[thread_idx, i] = 0.0

    # Hierarchical Softmax (if enabled) - traverse tree for context word y
    if use_hs:
        codelen = code_lengths[y]
        max_code_len = codes_array.shape[1]  # Get max code length from array shape
        for d in range(codelen):
            if d >= max_code_len:
                break
            node_idx = points_array[y, d]
            if node_idx < 0:
                continue

            # Calculate dot product: w1[x] ¬∑ syn1[node]
            f = 0.0
            for i in range(emb_dim):
                f += w1[x, i] * syn1[node_idx, i]

            # Early skip if f is outside range (same as original code)
            # This prevents unnecessary updates when sigmoid is saturated
            if f <= -max_exp:
                continue
            if f >= max_exp:
                continue

            # Get sigmoid from exp table (only if in range)
            sigmoid_val = fast_sigmoid(f, exp_table, exp_table_size, max_exp)

            # Get code bit (0 or 1)
            code_bit = codes_array[y, d]
            if code_bit < 0:
                continue

            # Calculate gradient: g = (1 - code_bit - sigmoid) * learning_rate
            g = (1.0 - float(code_bit) - sigmoid_val) * learning_rate

            # Propagate errors output -> hidden
            for i in range(emb_dim):
                calc_aux[thread_idx, i] += g * syn1[node_idx, i]

            # Learn weights hidden -> output
            for i in range(emb_dim):
                syn1[node_idx, i] += g * w1[x, i]

    # Negative Sampling (if enabled)
    if k > 0:
        # Positive sample: predict context word y
        dot_xy = 0.0
        for i in range(emb_dim):
            dot_xy += w1[x, i] * w2[y, i]
        s_xdy_m1 = fast_sigmoid(dot_xy, exp_table, exp_table_size, max_exp) - 1.0

        # Positive sample gradients
        for i in range(emb_dim):
            calc_aux[thread_idx, i] += -learning_rate * s_xdy_m1 * w2[y, i]
            w2[y, i] -= learning_rate * s_xdy_m1 * w1[x, i]

        # Negative samples
        for neg_sample in range(0, k):
            rnd = c_random.xoroshiro128p_uniform_float32(random_states, thread_idx)
            q_idx: int = int(math.floor(negs_arr_len * rnd))
            neg = negsample_array[q_idx]
            dot_xq = 0.0
            for i in range(emb_dim):
                dot_xq += w1[x, i] * w2[neg, i]
            s_dxq = fast_sigmoid(dot_xq, exp_table, exp_table_size, max_exp)

            # Negative sample gradients
            for i in range(emb_dim):
                calc_aux[thread_idx, i] -= learning_rate * s_dxq * w2[neg, i]
                w2[neg, i] -= learning_rate * s_dxq * w1[x, i]

    # Note: Original code does NOT use gradient clipping, only early skip
    # Gradient clipping may reduce training effectiveness
    # Update center word vector (same as original code)
    for i in range(emb_dim):
        w1[x, i] += calc_aux[thread_idx, i]


def train_skipgram(
        data_path: str,
        out_file_path: str,
        epochs: int,
        embed_dim: int = 100,
        min_occurs: int = 3,
        c: int = 5,
        k: int = 5,
        t: float = 1e-5,
        vocab_freq_exponent: float = 0.75,
        lr_max: float = 0.025,
        lr_min: float = 0.0025,
        cuda_threads_per_block: int = 32,
        hs: int = 0):
    """
    Train Skip-gram model.
    Based on word2vec.c Skip-gram implementation.

    Args:
        hs: Hierarchical Softmax flag (0=NS only, 1=HS, can combine with k>0)
    """
    params = {
        "model_type": "skipgram",
        "w2v_version": W2V_VERSION,
        "data_path": data_path,
        "out_file_path": out_file_path,
        "epochs": epochs,
        "embed_dim": embed_dim,
        "min_occurs": min_occurs,
        "c": c,
        "k": k,
        "t": t,
        "vocab_freq_exponent": vocab_freq_exponent,
        "lr_max": lr_max,
        "lr_min": lr_min,
        "cuda_threads_per_block": cuda_threads_per_block,
        "hs": hs
    }
    stats = {}
    params_path = out_file_path + "_params.json"
    stats_path = out_file_path + "_stats.json"

    seed = 12345

    # Adjust learning rate based on training method
    original_lr_max = lr_max
    original_lr_min = lr_min

    if hs == 1 and k == 0:
        # HS only: Use same learning rate as NS (as per word2vec.c original)
        # No learning rate reduction needed - HS and NS use same LR schedule
        pass
    elif hs == 1 and k > 0:
        # HS + NS: Reduce learning rate to prevent gradient explosion
        print(f"‚ö†Ô∏è  WARNING: Using both HS and NS together may cause issues.")
        print(f"   Consider using only one (hs=1, k=0) or (hs=0, k>0) for better results.")
        print(f"   Learning rate reduced by 50% to prevent gradient explosion.")
        lr_max = lr_max * 0.5
        lr_min = lr_min * 0.5

    lr_step = (lr_max - lr_min) / (epochs - 1)

    print(f"Skip-gram Training Parameters:")
    print(f"Seed: {seed}")
    print(f"Window size: {c}")
    if hs == 1:
        print(f"Hierarchical Softmax: Enabled")
    if k > 0:
        print(f"Negative samples: {k}")
    if original_lr_max != lr_max:
        print(f"Learning rate adjusted: {original_lr_max} ‚Üí {lr_max} (reduced for stability)")
    print(f"Learning rate: {lr_max} ‚Üí {lr_min} (step: {lr_step:.6f})")
    print(f"Embedding dimension: {embed_dim}")
    print(f"Min word count: {min_occurs}")

    print(f"\nBuilding vocabulary from: {data_path}")
    start = time.time()

    vocab, w_to_i, word_counts = handle_vocab(data_path, min_occurs, freq_exponent=vocab_freq_exponent)
    ssw, negs = get_subsampling_weights_and_negative_sampling_array(vocab, t=t)
    vocab_size = len(vocab)
    print(f"Vocabulary built in {time.time() - start:.2f}s. Vocab size: {vocab_size:,}")

    # Create exp table
    print("Creating exp table for fast sigmoid...")
    exp_table = create_exp_table(EXP_TABLE_SIZE, MAX_EXP)

    # Setup Hierarchical Softmax if enabled
    use_hs = (hs == 1)
    syn1_cuda = None
    codes_array_cuda = None
    points_array_cuda = None
    code_lengths_cuda = None

    if use_hs:
        print("Creating Huffman tree for Hierarchical Softmax...")
        hs_start = time.time()
        codes_array, points_array, code_lengths = create_huffman_tree(word_counts, MAX_CODE_LENGTH)
        syn1 = init_hs_weight_matrix(vocab_size, embed_dim)
        print(f"Huffman tree created in {time.time() - hs_start:.2f}s")
        print(f"  Codes array shape: {codes_array.shape}")
        print(f"  Points array shape: {points_array.shape}")
        print(f"  Syn1 matrix shape: {syn1.shape}")

    data_files = get_data_file_names(data_path, seed=seed)
    print(f"Processing {len(data_files)} data files...")
    inps_, offs_, lens_ = read_all_data_files_ever(data_path, data_files, w_to_i)
    inps, offs, lens = (np.asarray(inps_, dtype=np.int32),
                       np.asarray(offs_, dtype=np.int32),
                       np.asarray(lens_, dtype=np.int32))
    sentence_count = len(lens)
    blocks: int = math.ceil(sentence_count / cuda_threads_per_block)

    print(f"Data loaded: {sentence_count:,} sentences, {len(inps):,} total words")
    print(f"CUDA config: {cuda_threads_per_block} threads/block, {blocks} blocks")

    # Initialize weight matrices
    data_init_start = time.time()
    w1, w2 = init_weight_matrices(vocab_size, embed_dim, seed=seed)
    calc_aux = np.zeros((sentence_count, embed_dim), dtype=np.float32)
    data_size_weights = 4 * (w1.size + w2.size)
    data_size_inputs = 4 * (inps.size + offs.size + lens.size + ssw.size + negs.size)
    data_size_aux = 4 * calc_aux.size
    print(f"Weight matrices initialized in {time.time()-data_init_start:.2f}s")
    print(f"Memory usage: {data_size_weights:,} weights + {data_size_inputs:,} inputs + {data_size_aux:,} aux = {data_size_weights+data_size_inputs+data_size_aux:,} bytes")

    # Transfer to GPU
    print("Transferring data to GPU...")
    data_transfer_start = time.time()
    inps_cuda, offs_cuda, lens_cuda = cuda.to_device(inps), cuda.to_device(offs), cuda.to_device(lens)
    ssw_cuda, negs_cuda = cuda.to_device(ssw), cuda.to_device(negs)
    w1_cuda, w2_cuda = cuda.to_device(w1), cuda.to_device(w2)
    calc_aux_cuda = cuda.to_device(calc_aux)
    exp_table_cuda = cuda.to_device(exp_table)

    if use_hs:
        syn1_cuda = cuda.to_device(syn1)
        codes_array_cuda = cuda.to_device(codes_array)
        points_array_cuda = cuda.to_device(points_array)
        code_lengths_cuda = cuda.to_device(code_lengths)

    print(f"Data transfer completed in {time.time()-data_transfer_start:.2f}s")

    stats["sentence_count"] = len(lens)
    stats["word_count"] = len(inps)
    stats["vocab_size"] = vocab_size
    stats["approx_data_size_weights"] = data_size_weights
    stats["approx_data_size_inputs"] = data_size_inputs
    stats["approx_data_size_aux"] = data_size_aux
    stats["approx_data_size_total"] = data_size_weights + data_size_inputs + data_size_aux

    # Initialize CUDA random states
    print(f"Initializing CUDA random states for {sentence_count:,} threads...")
    random_init_start = time.time()
    random_states_cuda = c_random.create_xoroshiro128p_states(sentence_count, seed=seed)
    print(f"CUDA random states initialized in {time.time()-random_init_start:.2f}s")

    # Prepare HS parameters (use dummy arrays if HS disabled)
    if not use_hs:
        # Create dummy arrays for HS (will not be used, but needed for kernel signature)
        dummy_syn1 = cuda.device_array((1, embed_dim), dtype=np.float32)
        dummy_codes = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)
        dummy_points = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)
        dummy_lengths = cuda.device_array(vocab_size, dtype=np.int32)
        syn1_param = dummy_syn1
        codes_param = dummy_codes
        points_param = dummy_points
        lengths_param = dummy_lengths
    else:
        syn1_param = syn1_cuda
        codes_param = codes_array_cuda
        points_param = points_array_cuda
        lengths_param = code_lengths_cuda

    print_norms(w1_cuda)
    print(f"\nStarting Skip-gram training - {epochs} epochs...")
    epoch_times = []
    calc_start = time.time()

    for epoch in range(0, epochs):
        lr = lr_max - (epoch * lr_step)
        epoch_start = time.time()

        # Launch CUDA kernel
        calc_skipgram[blocks, cuda_threads_per_block](
            sentence_count, c, k, lr, w1_cuda, w2_cuda, calc_aux_cuda,
            random_states_cuda, ssw_cuda, negs_cuda, inps_cuda, offs_cuda, lens_cuda,
            use_hs, syn1_param, codes_param, points_param, lengths_param,
            exp_table_cuda, EXP_TABLE_SIZE, MAX_EXP)

        print(f"  Epoch {epoch+1} kernel launched in {time.time()-epoch_start:.2f}s (LR: {lr:.6f})")

        # Synchronize
        sync_start = time.time()
        cuda.synchronize()
        epoch_times.append(time.time()-epoch_start)
        print(f"  Synchronized in {time.time()-sync_start:.2f}s")
        print(f"  ‚Üí Epoch {epoch+1} completed in {epoch_times[-1]:.2f}s")

    print(f"\nSkip-gram training completed!")
    print(f"Epoch times - Min: {min(epoch_times):.2f}s, Avg: {np.mean(epoch_times):.2f}s, Max: {max(epoch_times):.2f}s")
    print(f"Total training time: {time.time()-calc_start:.2f}s")
    print(f"Total time: {time.time()-start:.2f}s")

    print_norms(w1_cuda)

    # Save results
    stats["epoch_time_min_seconds"] = min(epoch_times)
    stats["epoch_time_avg_seconds"] = np.mean(epoch_times)
    stats["epoch_time_max_seconds"] = max(epoch_times)
    stats["epoch_time_total_seconds"] = sum(epoch_times)
    stats["epoch_times_all_seconds"] = epoch_times

    print(f"Saving Skip-gram vectors to: {out_file_path}")
    write_vectors(w1_cuda, vocab, out_file_path)

    print(f"Saving parameters to: {params_path}")
    write_json(params, params_path)

    print(f"Saving statistics to: {stats_path}")
    write_json(stats, stats_path)

    print("Skip-gram training completed successfully!")

"""# **CBOW Implementation**"""

# Copyright 2024 Word2Vec Implementation
# CBOW implementation with Numba CUDA

import math
import os
import time
from typing import List, Tuple, Dict, Any

from numba import cuda
from numba.cuda import random as c_random
import numpy as np
from numpy import ndarray

# C√°c h√†m t·ª´ w2v_common ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong cell "Common Utilities" ·ªü tr√™n
# Kh√¥ng c·∫ßn import trong notebook


@cuda.jit
def calc_cbow(
        rows: int,
        c: int,
        k: int,
        learning_rate: float,
        w1,
        w2,
        calc_aux,
        random_states,
        subsample_weights,
        negsample_array,
        inp,
        offsets,
        lengths,
        use_hs,
        syn1,
        codes_array,
        points_array,
        code_lengths,
        exp_table,
        exp_table_size,
        max_exp):
    """
    CUDA kernel for CBOW training.
    Based on word2vec.c CBOW implementation (lines 435-494).
    Supports both Hierarchical Softmax and Negative Sampling.
    """
    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x
    if idx >= rows:
        return
    le = lengths[idx]
    off = offsets[idx]

    for centre in range(0, le):
        word_idx = inp[off + centre]
        prob_to_reject = subsample_weights[word_idx]
        rnd = c_random.xoroshiro128p_uniform_float32(random_states, idx)

        if rnd > prob_to_reject:
            r_f = c_random.xoroshiro128p_uniform_float32(random_states, idx)
            r: int = math.ceil(r_f * c)

            # Collect context words (before and after center word)
            context_words = cuda.local.array(20, dtype=np.int32)  # Max 2*c context words
            context_count = 0

            # Context before center word
            for context_pre in range(max(0, centre-r), centre):
                if context_count < 20:  # Prevent overflow
                    context_words[context_count] = inp[off+context_pre]
                    context_count += 1

            # Context after center word
            for context_post in range(centre + 1, min(le, centre + 1 + r)):
                if context_count < 20:  # Prevent overflow
                    context_words[context_count] = inp[off+context_post]
                    context_count += 1

            # Only proceed if we have context words
            if context_count > 0:
                step_cbow(idx, w1, w2, calc_aux, context_words, context_count,
                         inp[off+centre], k, learning_rate, negsample_array, random_states,
                         use_hs, syn1, codes_array, points_array, code_lengths,
                         exp_table, exp_table_size, max_exp)


@cuda.jit(device=True)
def fast_sigmoid(f, exp_table, exp_table_size, max_exp):
    """
    Fast sigmoid using precomputed exp table.
    Based on word2vec.c exp table lookup.
    """
    if f <= -max_exp:
        return 0.0
    elif f >= max_exp:
        return 1.0
    else:
        idx = int((f + max_exp) * (exp_table_size / max_exp / 2.0))
        if idx < 0:
            idx = 0
        if idx >= exp_table_size:
            idx = exp_table_size - 1
        return exp_table[idx]


@cuda.jit(device=True)
def step_cbow(thread_idx, w1, w2, calc_aux, context_words, context_count,
              center_word, k, learning_rate, negsample_array, random_states,
              use_hs, syn1, codes_array, points_array, code_lengths,
              exp_table, exp_table_size, max_exp):
    """
    Device function for CBOW gradient calculation.
    Based on word2vec.c CBOW implementation (lines 435-494).
    Supports both Hierarchical Softmax and Negative Sampling.
    """
    emb_dim = w1.shape[1]
    negs_arr_len = len(negsample_array)

    # 1. Calculate neu1 = average of context word vectors
    neu1 = cuda.local.array(100, dtype=np.float32)  # Max embedding dimension
    neu1e = cuda.local.array(100, dtype=np.float32)  # Error accumulation

    # Initialize neu1 and neu1e
    for i in range(emb_dim):
        neu1[i] = 0.0
        neu1e[i] = 0.0

    # Average context word vectors
    for i in range(emb_dim):
        for ctx_idx in range(context_count):
            neu1[i] += w1[context_words[ctx_idx], i]
        neu1[i] /= context_count

    # 2. Hierarchical Softmax (if enabled)
    if use_hs:
        codelen = code_lengths[center_word]
        max_code_len = codes_array.shape[1]  # Get max code length from array shape
        for d in range(codelen):
            if d >= max_code_len:
                break
            node_idx = points_array[center_word, d]
            if node_idx < 0:
                continue

            # Calculate dot product: neu1 ¬∑ syn1[node]
            f = 0.0
            for i in range(emb_dim):
                f += neu1[i] * syn1[node_idx, i]

            # Early skip if f is outside range (same as original code)
            # This prevents unnecessary updates when sigmoid is saturated
            if f <= -max_exp:
                continue
            if f >= max_exp:
                continue

            # Get sigmoid from exp table (only if in range)
            sigmoid_val = fast_sigmoid(f, exp_table, exp_table_size, max_exp)

            # Get code bit (0 or 1)
            code_bit = codes_array[center_word, d]
            if code_bit < 0:
                continue

            # Calculate gradient: g = (1 - code_bit - sigmoid) * learning_rate
            g = (1.0 - float(code_bit) - sigmoid_val) * learning_rate

            # Propagate errors output -> hidden
            for i in range(emb_dim):
                neu1e[i] += g * syn1[node_idx, i]

            # Learn weights hidden -> output
            for i in range(emb_dim):
                syn1[node_idx, i] += g * neu1[i]

    # 3. Negative Sampling (if enabled)
    if k > 0:
        # Positive sample: predict center_word
        dot_xy = 0.0
        for i in range(emb_dim):
            dot_xy += neu1[i] * w2[center_word, i]
        s_xdy_m1 = fast_sigmoid(dot_xy, exp_table, exp_table_size, max_exp) - 1.0

        # Update w2[center_word] and accumulate neu1e
        for i in range(emb_dim):
            neu1e[i] += -learning_rate * s_xdy_m1 * w2[center_word, i]
            w2[center_word, i] -= learning_rate * s_xdy_m1 * neu1[i]

        # Negative samples
        for neg_sample in range(0, k):
            rnd = c_random.xoroshiro128p_uniform_float32(random_states, thread_idx)
            q_idx: int = int(math.floor(negs_arr_len * rnd))
            neg = negsample_array[q_idx]
            dot_xq = 0.0
            for i in range(emb_dim):
                dot_xq += neu1[i] * w2[neg, i]
            s_dxq = fast_sigmoid(dot_xq, exp_table, exp_table_size, max_exp)

            # Update w2[neg] and accumulate neu1e
            for i in range(emb_dim):
                neu1e[i] -= learning_rate * s_dxq * w2[neg, i]
                w2[neg, i] -= learning_rate * s_dxq * neu1[i]

    # 4. Backprop neu1e to all context words
    # Note: Original code does NOT use gradient clipping, only early skip
    # Gradient clipping may reduce training effectiveness
    # Update context word vectors (same as original code)
    for ctx_idx in range(context_count):
        for i in range(emb_dim):
            w1[context_words[ctx_idx], i] += neu1e[i]


def train_cbow(
        data_path: str,
        out_file_path: str,
        epochs: int,
        embed_dim: int = 100,
        min_occurs: int = 3,
        c: int = 5,
        k: int = 5,
        t: float = 1e-5,
        vocab_freq_exponent: float = 0.75,
        lr_max: float = 0.025,
        lr_min: float = 0.0025,
        cuda_threads_per_block: int = 32,
        hs: int = 0):
    """
    Train CBOW model.
    Based on word2vec.c CBOW implementation.

    Args:
        hs: Hierarchical Softmax flag (0=NS only, 1=HS, can combine with k>0)
    """
    params = {
        "model_type": "cbow",
        "w2v_version": W2V_VERSION,
        "data_path": data_path,
        "out_file_path": out_file_path,
        "epochs": epochs,
        "embed_dim": embed_dim,
        "min_occurs": min_occurs,
        "c": c,
        "k": k,
        "t": t,
        "vocab_freq_exponent": vocab_freq_exponent,
        "lr_max": lr_max,
        "lr_min": lr_min,
        "cuda_threads_per_block": cuda_threads_per_block,
        "hs": hs
    }
    stats = {}
    params_path = out_file_path + "_params.json"
    stats_path = out_file_path + "_stats.json"

    seed = 12345

    # Adjust learning rate based on training method
    original_lr_max = lr_max
    original_lr_min = lr_min

    if hs == 1 and k == 0:
        # HS only: Use same learning rate as NS (as per word2vec.c original)
        # No learning rate reduction needed - HS and NS use same LR schedule
        pass
    elif hs == 1 and k > 0:
        # HS + NS: Reduce learning rate more to prevent gradient explosion
        print(f"‚ö†Ô∏è  WARNING: Using both HS and NS together may cause issues.")
        print(f"   Consider using only one (hs=1, k=0) or (hs=0, k>0) for better results.")
        print(f"   Learning rate reduced by 50% to prevent gradient explosion.")
        lr_max = lr_max * 0.5
        lr_min = lr_min * 0.5

    lr_step = (lr_max - lr_min) / (epochs - 1)

    print(f"CBOW Training Parameters:")
    print(f"Seed: {seed}")
    print(f"Window size: {c}")
    if hs == 1:
        print(f"Hierarchical Softmax: Enabled")
    if k > 0:
        print(f"Negative samples: {k}")
    if original_lr_max != lr_max:
        print(f"Learning rate adjusted: {original_lr_max} ‚Üí {lr_max} (reduced for stability)")
    print(f"Learning rate: {lr_max} ‚Üí {lr_min} (step: {lr_step:.6f})")
    print(f"Embedding dimension: {embed_dim}")
    print(f"Min word count: {min_occurs}")

    print(f"\nBuilding vocabulary from: {data_path}")
    start = time.time()

    vocab, w_to_i, word_counts = handle_vocab(data_path, min_occurs, freq_exponent=vocab_freq_exponent)
    ssw, negs = get_subsampling_weights_and_negative_sampling_array(vocab, t=t)
    vocab_size = len(vocab)
    print(f"Vocabulary built in {time.time() - start:.2f}s. Vocab size: {vocab_size:,}")

    # Create exp table
    print("Creating exp table for fast sigmoid...")
    exp_table = create_exp_table(EXP_TABLE_SIZE, MAX_EXP)

    # Setup Hierarchical Softmax if enabled
    use_hs = (hs == 1)
    syn1_cuda = None
    codes_array_cuda = None
    points_array_cuda = None
    code_lengths_cuda = None

    if use_hs:
        print("Creating Huffman tree for Hierarchical Softmax...")
        hs_start = time.time()
        codes_array, points_array, code_lengths = create_huffman_tree(word_counts, MAX_CODE_LENGTH)
        syn1 = init_hs_weight_matrix(vocab_size, embed_dim)
        print(f"Huffman tree created in {time.time() - hs_start:.2f}s")
        print(f"  Codes array shape: {codes_array.shape}")
        print(f"  Points array shape: {points_array.shape}")
        print(f"  Syn1 matrix shape: {syn1.shape}")

    data_files = get_data_file_names(data_path, seed=seed)
    print(f"Processing {len(data_files)} data files...")
    inps_, offs_, lens_ = read_all_data_files_ever(data_path, data_files, w_to_i)
    inps, offs, lens = (np.asarray(inps_, dtype=np.int32),
                       np.asarray(offs_, dtype=np.int32),
                       np.asarray(lens_, dtype=np.int32))
    sentence_count = len(lens)
    blocks: int = math.ceil(sentence_count / cuda_threads_per_block)

    print(f"Data loaded: {sentence_count:,} sentences, {len(inps):,} total words")
    print(f"CUDA config: {cuda_threads_per_block} threads/block, {blocks} blocks")

    # Initialize weight matrices
    data_init_start = time.time()
    w1, w2 = init_weight_matrices(vocab_size, embed_dim, seed=seed)
    calc_aux = np.zeros((sentence_count, embed_dim), dtype=np.float32)
    data_size_weights = 4 * (w1.size + w2.size)
    data_size_inputs = 4 * (inps.size + offs.size + lens.size + ssw.size + negs.size)
    data_size_aux = 4 * calc_aux.size
    print(f"Weight matrices initialized in {time.time()-data_init_start:.2f}s")
    print(f"Memory usage: {data_size_weights:,} weights + {data_size_inputs:,} inputs + {data_size_aux:,} aux = {data_size_weights+data_size_inputs+data_size_aux:,} bytes")

    # Transfer to GPU
    print("Transferring data to GPU...")
    data_transfer_start = time.time()
    inps_cuda, offs_cuda, lens_cuda = cuda.to_device(inps), cuda.to_device(offs), cuda.to_device(lens)
    ssw_cuda, negs_cuda = cuda.to_device(ssw), cuda.to_device(negs)
    w1_cuda, w2_cuda = cuda.to_device(w1), cuda.to_device(w2)
    calc_aux_cuda = cuda.to_device(calc_aux)
    exp_table_cuda = cuda.to_device(exp_table)

    if use_hs:
        syn1_cuda = cuda.to_device(syn1)
        codes_array_cuda = cuda.to_device(codes_array)
        points_array_cuda = cuda.to_device(points_array)
        code_lengths_cuda = cuda.to_device(code_lengths)

    print(f"Data transfer completed in {time.time()-data_transfer_start:.2f}s")

    stats["sentence_count"] = len(lens)
    stats["word_count"] = len(inps)
    stats["vocab_size"] = vocab_size
    stats["approx_data_size_weights"] = data_size_weights
    stats["approx_data_size_inputs"] = data_size_inputs
    stats["approx_data_size_aux"] = data_size_aux
    stats["approx_data_size_total"] = data_size_weights + data_size_inputs + data_size_aux

    # Initialize CUDA random states
    print(f"Initializing CUDA random states for {sentence_count:,} threads...")
    random_init_start = time.time()
    random_states_cuda = c_random.create_xoroshiro128p_states(sentence_count, seed=seed)
    print(f"CUDA random states initialized in {time.time()-random_init_start:.2f}s")

    # Prepare HS parameters (use dummy arrays if HS disabled)
    if not use_hs:
        # Create dummy arrays for HS (will not be used, but needed for kernel signature)
        dummy_syn1 = cuda.device_array((1, embed_dim), dtype=np.float32)
        dummy_codes = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)
        dummy_points = cuda.device_array((vocab_size, MAX_CODE_LENGTH), dtype=np.int32)
        dummy_lengths = cuda.device_array(vocab_size, dtype=np.int32)
        syn1_param = dummy_syn1
        codes_param = dummy_codes
        points_param = dummy_points
        lengths_param = dummy_lengths
    else:
        syn1_param = syn1_cuda
        codes_param = codes_array_cuda
        points_param = points_array_cuda
        lengths_param = code_lengths_cuda

    print_norms(w1_cuda)
    print(f"\nStarting CBOW training - {epochs} epochs...")
    epoch_times = []
    calc_start = time.time()

    for epoch in range(0, epochs):
        lr = lr_max - (epoch * lr_step)
        epoch_start = time.time()

        # Launch CUDA kernel
        calc_cbow[blocks, cuda_threads_per_block](
            sentence_count, c, k, lr, w1_cuda, w2_cuda, calc_aux_cuda,
            random_states_cuda, ssw_cuda, negs_cuda, inps_cuda, offs_cuda, lens_cuda,
            use_hs, syn1_param, codes_param, points_param, lengths_param,
            exp_table_cuda, EXP_TABLE_SIZE, MAX_EXP)

        print(f"  Epoch {epoch+1} kernel launched in {time.time()-epoch_start:.2f}s (LR: {lr:.6f})")

        # Synchronize
        sync_start = time.time()
        cuda.synchronize()
        epoch_times.append(time.time()-epoch_start)
        print(f"  Synchronized in {time.time()-sync_start:.2f}s")
        print(f"  ‚Üí Epoch {epoch+1} completed in {epoch_times[-1]:.2f}s")

    print(f"\nCBOW training completed!")
    print(f"Epoch times - Min: {min(epoch_times):.2f}s, Avg: {np.mean(epoch_times):.2f}s, Max: {max(epoch_times):.2f}s")
    print(f"Total training time: {time.time()-calc_start:.2f}s")
    print(f"Total time: {time.time()-start:.2f}s")

    print_norms(w1_cuda)

    # Save results
    stats["epoch_time_min_seconds"] = min(epoch_times)
    stats["epoch_time_avg_seconds"] = np.mean(epoch_times)
    stats["epoch_time_max_seconds"] = max(epoch_times)
    stats["epoch_time_total_seconds"] = sum(epoch_times)
    stats["epoch_times_all_seconds"] = epoch_times

    print(f"Saving CBOW vectors to: {out_file_path}")
    write_vectors(w1_cuda, vocab, out_file_path)

    print(f"Saving parameters to: {params_path}")
    write_json(params, params_path)

    print(f"Saving statistics to: {stats_path}")
    write_json(stats, stats_path)

    print("CBOW training completed successfully!")

"""# **Evaluation**"""

# Copyright 2024 Word2Vec Implementation
# Evaluation utilities for word2vec models

import json
import os
import time
import re
from typing import List, Tuple, Dict, Any
import requests
from gensim.models import KeyedVectors, Word2Vec
from gensim.test.utils import datapath


def download_questions_words(output_path: str = "./data/questions-words.txt") -> str:
    """Download questions-words.txt for word analogy test."""
    if os.path.isfile(output_path):
        print(f"Questions-words.txt already exists at: {output_path}")
        return output_path

    url = "https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt"
    print(f"Downloading questions-words.txt from {url}...")

    with requests.get(url, stream=True) as response:
        response.raise_for_status()
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

    print(f"Questions-words.txt downloaded to: {output_path}")
    return output_path


def word_analogy_test(vectors_path: str, questions_path: str = None) -> Tuple[float, List[Dict]]:
    """
    Run word analogy test on trained vectors.
    Returns (overall_accuracy, details_by_category).
    """
    if questions_path is None:
        questions_path = download_questions_words()

    print(f"Loading vectors from: {vectors_path}")
    start = time.time()
    vecs = KeyedVectors.load_word2vec_format(vectors_path, binary=False)
    print(f"Vectors loaded in {time.time() - start:.2f}s")

    print(f"Running word analogy test with: {questions_path}")
    eval_start = time.time()
    acc, details_dict = vecs.evaluate_word_analogies(questions_path, case_insensitive=True)
    print(f"Word analogy test completed in {time.time() - eval_start:.2f}s")
    print(f"Overall accuracy: {acc:.4f} ({acc*100:.2f}%)")

    # Print results by category
    for category in details_dict:
        correct = len(category["correct"])
        total = correct + len(category["incorrect"])
        cat_acc = correct / total if total > 0 else 0
        print(f"  {category['section']}: {cat_acc:.4f} ({cat_acc*100:.2f}%) - {correct}/{total}")

    return acc, details_dict


def similarity_test(vectors_path: str, test_words: List[str] = None) -> Dict[str, Any]:
    """
    Test word similarity and find most similar words.
    """
    if test_words is None:
        test_words = ["king", "queen", "man", "woman", "computer", "science", "university", "student"]

    print(f"Loading vectors for similarity test: {vectors_path}")
    vecs = KeyedVectors.load_word2vec_format(vectors_path, binary=False)

    results = {}

    print("\nMost similar words:")
    for word in test_words:
        if word in vecs:
            similar = vecs.most_similar(word, topn=5)
            results[word] = similar
            print(f"\n{word}:")
            for sim_word, score in similar:
                print(f"  {sim_word}: {score:.4f}")
        else:
            print(f"Word '{word}' not found in vocabulary")
            results[word] = []

    # Test some word pairs for similarity
    word_pairs = [
        ("king", "queen"),
        ("man", "woman"),
        ("computer", "science"),
        ("university", "student"),
        ("good", "bad"),
        ("big", "small")
    ]

    print("\nWord pair similarities:")
    pair_similarities = {}
    for word1, word2 in word_pairs:
        if word1 in vecs and word2 in vecs:
            similarity = vecs.similarity(word1, word2)
            pair_similarities[f"{word1}-{word2}"] = similarity
            print(f"  {word1} - {word2}: {similarity:.4f}")
        else:
            print(f"  {word1} - {word2}: One or both words not found")
            pair_similarities[f"{word1}-{word2}"] = None

    results["pair_similarities"] = pair_similarities
    return results


def save_evaluation_results(results: Dict[str, Any], output_path: str):
    """Save evaluation results to JSON file."""
    import numpy as np

    def convert_numpy_types(obj):
        """Convert numpy types to Python native types for JSON serialization."""
        if isinstance(obj, np.float32):
            return float(obj)
        elif isinstance(obj, np.float64):
            return float(obj)
        elif isinstance(obj, np.int32):
            return int(obj)
        elif isinstance(obj, np.int64):
            return int(obj)
        elif isinstance(obj, dict):
            return {key: convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        else:
            return obj

    # Convert numpy types to Python native types
    results_converted = convert_numpy_types(results)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results_converted, f, indent=2, ensure_ascii=False)
    print(f"Evaluation results saved to: {output_path}")


def compare_models(skipgram_path: str, cbow_path: str, output_path: str = "./output/model_comparison.json",
                   sg_acc: float = None, sg_details: List[Dict] = None,
                   cbow_acc: float = None, cbow_details: List[Dict] = None):
    """
    Compare Skip-gram and CBOW models.

    Args:
        skipgram_path: Path to Skip-gram vectors
        cbow_path: Path to CBOW vectors
        output_path: Output path for comparison JSON
        sg_acc: Pre-computed Skip-gram accuracy (optional, will compute if None)
        sg_details: Pre-computed Skip-gram evaluation details (optional)
        cbow_acc: Pre-computed CBOW accuracy (optional, will compute if None)
        cbow_details: Pre-computed CBOW evaluation details (optional)
    """
    print("Comparing Skip-gram vs CBOW models...")

    # Evaluate both models only if not provided
    if sg_acc is None or sg_details is None:
        print("Evaluating Skip-gram model...")
        sg_acc, sg_details = word_analogy_test(skipgram_path)
    else:
        print("Using pre-computed Skip-gram accuracy...")

    if cbow_acc is None or cbow_details is None:
        print("Evaluating CBOW model...")
        cbow_acc, cbow_details = word_analogy_test(cbow_path)
    else:
        print("Using pre-computed CBOW accuracy...")

    # Load statistics
    sg_stats_path = skipgram_path + "_stats.json"
    cbow_stats_path = cbow_path + "_stats.json"

    sg_stats = {}
    cbow_stats = {}

    if os.path.isfile(sg_stats_path):
        with open(sg_stats_path, "r") as f:
            sg_stats = json.load(f)

    if os.path.isfile(cbow_stats_path):
        with open(cbow_stats_path, "r") as f:
            cbow_stats = json.load(f)

    comparison = {
        "models": {
            "skipgram": {
                "accuracy": sg_acc,
                "details": sg_details,
                "stats": sg_stats
            },
            "cbow": {
                "accuracy": cbow_acc,
                "details": cbow_details,
                "stats": cbow_stats
            }
        },
        "summary": {
            "skipgram_accuracy": sg_acc,
            "cbow_accuracy": cbow_acc,
            "accuracy_difference": sg_acc - cbow_acc,
            "skipgram_training_time": sg_stats.get("epoch_time_total_seconds", 0),
            "cbow_training_time": cbow_stats.get("epoch_time_total_seconds", 0),
            "time_difference": sg_stats.get("epoch_time_total_seconds", 0) - cbow_stats.get("epoch_time_total_seconds", 0)
        }
    }

    save_evaluation_results(comparison, output_path)

    print(f"\nModel Comparison Summary:")
    print(f"Skip-gram accuracy: {sg_acc:.4f} ({sg_acc*100:.2f}%)")
    print(f"CBOW accuracy: {cbow_acc:.4f} ({cbow_acc*100:.2f}%)")
    print(f"Difference: {sg_acc - cbow_acc:.4f} ({(sg_acc - cbow_acc)*100:.2f}%)")

    if sg_stats and cbow_stats:
        sg_time = sg_stats.get("epoch_time_total_seconds", 0)
        cbow_time = cbow_stats.get("epoch_time_total_seconds", 0)
        print(f"Skip-gram training time: {sg_time:.2f}s")
        print(f"CBOW training time: {cbow_time:.2f}s")
        print(f"Time difference: {sg_time - cbow_time:.2f}s")

    return comparison


def train_gensim_models(data_path: str, output_dir: str = "./output/gensim",
                        epochs: int = 10, embed_dim: int = 100, min_count: int = 5,
                        window: int = 5, negative: int = 5, hs: int = 0,
                        alpha: float = 0.025, min_alpha: float = 0.0001,
                        sample: float = 1e-5, workers: int = 4) -> Tuple[str, str, float, float]:
    """
    Train Skip-gram and CBOW models using Gensim library.

    Args:
        data_path: Path to preprocessed data directory
        output_dir: Output directory for Gensim models
        epochs: Number of training epochs
        embed_dim: Embedding dimension
        min_count: Minimum word count
        window: Window size
        negative: Number of negative samples (if hs=0)
        hs: Hierarchical Softmax (1=HS, 0=NS)
        alpha: Initial learning rate
        min_alpha: Minimum learning rate
        sample: Subsampling threshold
        workers: Number of worker threads

    Returns:
        Tuple of (skipgram_path, cbow_path, skipgram_time, cbow_time)
    """
    import glob
    # get_data_file_names ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong cell "Common Utilities" ·ªü tr√™n

    os.makedirs(output_dir, exist_ok=True)

    skipgram_path = os.path.join(output_dir, "vectors_skipgram_gensim")
    cbow_path = os.path.join(output_dir, "vectors_cbow_gensim")

    # Read all sentences from data files
    print("Reading data files...")
    data_files = get_data_file_names(data_path, seed=12345)
    sentences = []

    start = time.time()
    for filename in data_files:
        filepath = os.path.join(data_path, filename)
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    words = [word for word in re.split(r"[ .]+", line) if word]
                    if len(words) >= 2:
                        sentences.append(words)

    print(f"Read {len(sentences):,} sentences in {time.time() - start:.2f}s")

    # Common parameters (Gensim 4.0+ uses vector_size instead of size)
    common_params = {
        "vector_size": embed_dim,  # Changed from "size" in Gensim 4.0+
        "window": window,
        "min_count": min_count,
        "hs": hs,
        "negative": 0 if hs == 1 else negative,
        "alpha": alpha,
        "min_alpha": min_alpha,
        "sample": sample,
        "workers": workers,
        "sg": 0  # Will be set per model
    }

    # Train Skip-gram model
    print("\n" + "="*60)
    print("  TRAINING GENSIM SKIP-GRAM MODEL")
    print("="*60)
    skipgram_params = common_params.copy()
    skipgram_params["sg"] = 1  # Skip-gram

    print(f"Parameters: {skipgram_params}")
    print("Building vocabulary...")
    sg_model = Word2Vec(**skipgram_params)
    sg_model.build_vocab(sentences)
    vocab_size = len(sg_model.wv.vocab) if hasattr(sg_model.wv, 'vocab') else len(sg_model.wv.key_to_index)
    print(f"Vocabulary size: {vocab_size:,}")

    print(f"Training Skip-gram model for {epochs} epochs...")
    sg_start = time.time()
    sg_model.train(sentences, total_examples=sg_model.corpus_count, epochs=epochs)
    sg_time = time.time() - sg_start

    print(f"Skip-gram training completed in {sg_time:.2f}s")
    print(f"Saving Skip-gram vectors to: {skipgram_path}")
    sg_model.wv.save_word2vec_format(skipgram_path, binary=False)

    # Train CBOW model
    print("\n" + "="*60)
    print("  TRAINING GENSIM CBOW MODEL")
    print("="*60)
    cbow_params = common_params.copy()
    cbow_params["sg"] = 0  # CBOW
    cbow_params["alpha"] = alpha * 2  # CBOW typically uses higher LR

    print(f"Parameters: {cbow_params}")
    print("Building vocabulary...")
    cbow_model = Word2Vec(**cbow_params)
    cbow_model.build_vocab(sentences)
    vocab_size = len(cbow_model.wv.vocab) if hasattr(cbow_model.wv, 'vocab') else len(cbow_model.wv.key_to_index)
    print(f"Vocabulary size: {vocab_size:,}")

    print(f"Training CBOW model for {epochs} epochs...")
    cbow_start = time.time()
    cbow_model.train(sentences, total_examples=cbow_model.corpus_count, epochs=epochs)
    cbow_time = time.time() - cbow_start

    print(f"CBOW training completed in {cbow_time:.2f}s")
    print(f"Saving CBOW vectors to: {cbow_path}")
    cbow_model.wv.save_word2vec_format(cbow_path, binary=False)

    return skipgram_path, cbow_path, sg_time, cbow_time


def evaluate_gensim_models(skipgram_path: str, cbow_path: str,
                           output_dir: str = "./output/gensim",
                           questions_path: str = None) -> Tuple[float, List[Dict], float, List[Dict]]:
    """
    Evaluate Gensim-trained Skip-gram and CBOW models.

    Args:
        skipgram_path: Path to Skip-gram vectors
        cbow_path: Path to CBOW vectors
        output_dir: Output directory for evaluation results
        questions_path: Path to questions-words.txt file

    Returns:
        Tuple of (skipgram_acc, skipgram_details, cbow_acc, cbow_details)
    """
    os.makedirs(output_dir, exist_ok=True)

    if questions_path is None:
        questions_path = download_questions_words()

    # Evaluate Skip-gram
    print("\n" + "="*60)
    print("  EVALUATING GENSIM SKIP-GRAM MODEL")
    print("="*60)
    sg_acc, sg_details = word_analogy_test(skipgram_path, questions_path)
    sg_sim = similarity_test(skipgram_path)

    sg_eval_path = os.path.join(output_dir, "skipgram_eval.json")
    save_evaluation_results({
        "accuracy": sg_acc,
        "details": sg_details,
        "similarity_test": sg_sim
    }, sg_eval_path)

    # Evaluate CBOW
    print("\n" + "="*60)
    print("  EVALUATING GENSIM CBOW MODEL")
    print("="*60)
    cbow_acc, cbow_details = word_analogy_test(cbow_path, questions_path)
    cbow_sim = similarity_test(cbow_path)

    cbow_eval_path = os.path.join(output_dir, "cbow_eval.json")
    save_evaluation_results({
        "accuracy": cbow_acc,
        "details": cbow_details,
        "similarity_test": cbow_sim
    }, cbow_eval_path)

    return sg_acc, sg_details, cbow_acc, cbow_details


def compare_with_gensim(custom_sg_path: str, custom_cbow_path: str,
                        gensim_sg_path: str, gensim_cbow_path: str,
                        gensim_sg_time: float, gensim_cbow_time: float,
                        gensim_sg_acc: float, gensim_sg_details: List[Dict],
                        gensim_cbow_acc: float, gensim_cbow_details: List[Dict],
                        custom_sg_time: float = None, custom_cbow_time: float = None,
                        output_path: str = "./output/gensim_comparison.json") -> Dict[str, Any]:
    """
    Compare custom implementation with Gensim models.

    Args:
        custom_sg_path: Path to custom Skip-gram vectors
        custom_cbow_path: Path to custom CBOW vectors
        gensim_sg_path: Path to Gensim Skip-gram vectors
        gensim_cbow_path: Path to Gensim CBOW vectors
        gensim_sg_time: Gensim Skip-gram training time
        gensim_cbow_time: Gensim CBOW training time
        gensim_sg_acc: Gensim Skip-gram accuracy
        gensim_sg_details: Gensim Skip-gram evaluation details
        gensim_cbow_acc: Gensim CBOW accuracy
        gensim_cbow_details: Gensim CBOW evaluation details
        custom_sg_time: Custom Skip-gram training time (optional)
        custom_cbow_time: Custom CBOW training time (optional)
        output_path: Output path for comparison JSON

    Returns:
        Comparison dictionary
    """
    print("\n" + "="*60)
    print("  COMPARING CUSTOM vs GENSIM MODELS")
    print("="*60)

    # Evaluate custom models
    print("Evaluating custom models...")
    custom_sg_acc, custom_sg_details = word_analogy_test(custom_sg_path)
    custom_cbow_acc, custom_cbow_details = word_analogy_test(custom_cbow_path)

    # Load custom model statistics if available
    custom_sg_stats = {}
    custom_cbow_stats = {}

    custom_sg_stats_path = custom_sg_path + "_stats.json"
    custom_cbow_stats_path = custom_cbow_path + "_stats.json"

    if os.path.isfile(custom_sg_stats_path):
        with open(custom_sg_stats_path, "r") as f:
            custom_sg_stats = json.load(f)
            if custom_sg_time is None:
                custom_sg_time = custom_sg_stats.get("epoch_time_total_seconds", 0)

    if os.path.isfile(custom_cbow_stats_path):
        with open(custom_cbow_stats_path, "r") as f:
            custom_cbow_stats = json.load(f)
            if custom_cbow_time is None:
                custom_cbow_time = custom_cbow_stats.get("epoch_time_total_seconds", 0)

    # Create comparison
    comparison = {
        "skipgram": {
            "custom": {
                "accuracy": custom_sg_acc,
                "training_time": custom_sg_time,
                "details": custom_sg_details
            },
            "gensim": {
                "accuracy": gensim_sg_acc,
                "training_time": gensim_sg_time,
                "details": gensim_sg_details
            },
            "accuracy_difference": custom_sg_acc - gensim_sg_acc,
            "time_difference": (custom_sg_time or 0) - gensim_sg_time,
            "speedup": gensim_sg_time / (custom_sg_time or 1) if custom_sg_time else None
        },
        "cbow": {
            "custom": {
                "accuracy": custom_cbow_acc,
                "training_time": custom_cbow_time,
                "details": custom_cbow_details
            },
            "gensim": {
                "accuracy": gensim_cbow_acc,
                "training_time": gensim_cbow_time,
                "details": gensim_cbow_details
            },
            "accuracy_difference": custom_cbow_acc - gensim_cbow_acc,
            "time_difference": (custom_cbow_time or 0) - gensim_cbow_time,
            "speedup": gensim_cbow_time / (custom_cbow_time or 1) if custom_cbow_time else None
        },
        "summary": {
            "custom_skipgram_accuracy": custom_sg_acc,
            "gensim_skipgram_accuracy": gensim_sg_acc,
            "custom_cbow_accuracy": custom_cbow_acc,
            "gensim_cbow_accuracy": gensim_cbow_acc,
            "skipgram_accuracy_diff": custom_sg_acc - gensim_sg_acc,
            "cbow_accuracy_diff": custom_cbow_acc - gensim_cbow_acc
        }
    }

    save_evaluation_results(comparison, output_path)

    # Print summary
    print("\n" + "="*60)
    print("  COMPARISON SUMMARY")
    print("="*60)
    print("\nSkip-gram:")
    print(f"  Custom:  {custom_sg_acc:.4f} ({custom_sg_acc*100:.2f}%) - {custom_sg_time:.2f}s" if custom_sg_time else f"  Custom:  {custom_sg_acc:.4f} ({custom_sg_acc*100:.2f}%)")
    print(f"  Gensim:  {gensim_sg_acc:.4f} ({gensim_sg_acc*100:.2f}%) - {gensim_sg_time:.2f}s")
    print(f"  Diff:    {custom_sg_acc - gensim_sg_acc:+.4f} ({(custom_sg_acc - gensim_sg_acc)*100:+.2f}%)")
    if custom_sg_time:
        print(f"  Speedup: {gensim_sg_time / custom_sg_time:.2f}x {'(Gensim faster)' if gensim_sg_time < custom_sg_time else '(Custom faster)'}")

    print("\nCBOW:")
    print(f"  Custom:  {custom_cbow_acc:.4f} ({custom_cbow_acc*100:.2f}%) - {custom_cbow_time:.2f}s" if custom_cbow_time else f"  Custom:  {custom_cbow_acc:.4f} ({custom_cbow_acc*100:.2f}%)")
    print(f"  Gensim:  {gensim_cbow_acc:.4f} ({gensim_cbow_acc*100:.2f}%) - {gensim_cbow_time:.2f}s")
    print(f"  Diff:    {custom_cbow_acc - gensim_cbow_acc:+.4f} ({(custom_cbow_acc - gensim_cbow_acc)*100:+.2f}%)")
    if custom_cbow_time:
        print(f"  Speedup: {gensim_cbow_time / custom_cbow_time:.2f}x {'(Gensim faster)' if gensim_cbow_time < custom_cbow_time else '(Custom faster)'}")

    return comparison

"""# **Visualization**"""

# Copyright 2024 Word2Vec Implementation
# Visualization utilities for word2vec models

import os
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any
from gensim.models import KeyedVectors
from sklearn.manifold import TSNE
from collections import Counter


def plot_tsne(vectors_path: str, output_path: str, n_words: int = 500, perplexity: int = 30,
              random_state: int = 42, figsize: tuple = (12, 10)):
    """
    Create t-SNE visualization of word embeddings.
    """
    print(f"Creating t-SNE visualization for: {vectors_path}")
    print(f"Parameters: n_words={n_words}, perplexity={perplexity}")

    # Load vectors
    vecs = KeyedVectors.load_word2vec_format(vectors_path, binary=False)
    print(f"Loaded {len(vecs)} word vectors")

    # Select most frequent words for visualization
    if n_words < len(vecs):
        # Get most frequent words (assuming they are ordered by frequency in the file)
        words = list(vecs.key_to_index.keys())[:n_words]
    else:
        words = list(vecs.key_to_index.keys())
        n_words = len(words)

    print(f"Selected {n_words} words for visualization")

    # Get vectors for selected words
    vectors = np.array([vecs[word] for word in words])

    # Apply t-SNE
    print("Applying t-SNE dimensionality reduction...")
    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state,
                n_iter=1000, verbose=1)
    vectors_2d = tsne.fit_transform(vectors)

    # Create plot
    plt.figure(figsize=figsize)
    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.6, s=20)

    # Annotate some interesting words
    interesting_words = ["king", "queen", "man", "woman", "computer", "science",
                        "university", "student", "good", "bad", "big", "small",
                        "the", "and", "of", "to", "in", "is", "it", "you"]

    for i, word in enumerate(words):
        if word in interesting_words:
            plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),
                        xytext=(5, 5), textcoords='offset points', fontsize=8)

    plt.title(f"t-SNE Visualization of Word Embeddings\n({n_words} words, perplexity={perplexity})")
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    plt.grid(True, alpha=0.3)

    # Save plot
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"t-SNE plot saved to: {output_path}")


def plot_similarity_heatmap(vectors_path: str, words: List[str], output_path: str,
                           figsize: tuple = (10, 8)):
    """
    Create similarity heatmap for selected words.
    """
    print(f"Creating similarity heatmap for: {vectors_path}")
    print(f"Words: {words}")

    # Load vectors
    vecs = KeyedVectors.load_word2vec_format(vectors_path, binary=False)

    # Filter words that exist in vocabulary
    existing_words = [word for word in words if word in vecs]
    if len(existing_words) != len(words):
        missing = set(words) - set(existing_words)
        print(f"Warning: Missing words: {missing}")

    if len(existing_words) < 2:
        print("Error: Need at least 2 words for heatmap")
        return

    # Calculate similarity matrix
    n = len(existing_words)
    similarity_matrix = np.zeros((n, n))

    for i, word1 in enumerate(existing_words):
        for j, word2 in enumerate(existing_words):
            if i == j:
                similarity_matrix[i, j] = 1.0
            else:
                similarity_matrix[i, j] = vecs.similarity(word1, word2)

    # Create heatmap
    plt.figure(figsize=figsize)
    sns.heatmap(similarity_matrix,
                xticklabels=existing_words,
                yticklabels=existing_words,
                annot=True,
                fmt='.3f',
                cmap='coolwarm',
                center=0,
                square=True)

    plt.title("Word Similarity Heatmap")
    plt.xlabel("Words")
    plt.ylabel("Words")
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)

    # Save plot
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Similarity heatmap saved to: {output_path}")


def plot_training_comparison(skipgram_stats_path: str, cbow_stats_path: str,
                           output_path: str, figsize: tuple = (15, 5)):
    """
    Create comparison plots for Skip-gram vs CBOW training.
    """
    print(f"Creating training comparison plots...")

    # Load statistics
    with open(skipgram_stats_path, 'r') as f:
        sg_stats = json.load(f)
    with open(cbow_stats_path, 'r') as f:
        cbow_stats = json.load(f)

    # Create subplots
    fig, axes = plt.subplots(1, 3, figsize=figsize)

    # 1. Training time comparison
    models = ['Skip-gram', 'CBOW']
    times = [sg_stats.get('epoch_time_total_seconds', 0),
             cbow_stats.get('epoch_time_total_seconds', 0)]

    axes[0].bar(models, times, color=['skyblue', 'lightcoral'])
    axes[0].set_title('Training Time Comparison')
    axes[0].set_ylabel('Time (seconds)')
    for i, v in enumerate(times):
        axes[0].text(i, v + max(times)*0.01, f'{v:.1f}s', ha='center', va='bottom')

    # 2. Epoch times comparison
    sg_epochs = sg_stats.get('epoch_times_all_seconds', [])
    cbow_epochs = cbow_stats.get('epoch_times_all_seconds', [])

    if sg_epochs and cbow_epochs:
        epochs = range(1, len(sg_epochs) + 1)
        axes[1].plot(epochs, sg_epochs, 'o-', label='Skip-gram', color='blue')
        axes[1].plot(epochs, cbow_epochs, 's-', label='CBOW', color='red')
        axes[1].set_title('Epoch Time Progression')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Time (seconds)')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

    # 3. Data statistics
    sg_sentences = sg_stats.get('sentence_count', 0)
    sg_words = sg_stats.get('word_count', 0)
    cbow_sentences = cbow_stats.get('sentence_count', 0)
    cbow_words = cbow_stats.get('word_count', 0)

    x = np.arange(2)
    width = 0.35

    axes[2].bar(x - width/2, [sg_sentences, cbow_sentences], width,
                label='Sentences', color='lightgreen')
    axes[2].bar(x + width/2, [sg_words, cbow_words], width,
                label='Words', color='orange')

    axes[2].set_title('Data Statistics')
    axes[2].set_ylabel('Count')
    axes[2].set_xticks(x)
    axes[2].set_xticklabels(models)
    axes[2].legend()

    # Add value labels on bars
    for i, (sent, words) in enumerate([(sg_sentences, sg_words), (cbow_sentences, cbow_words)]):
        axes[2].text(i - width/2, sent + max(sg_sentences, cbow_sentences)*0.01,
                    f'{sent:,}', ha='center', va='bottom', fontsize=8)
        axes[2].text(i + width/2, words + max(sg_words, cbow_words)*0.01,
                    f'{words:,}', ha='center', va='bottom', fontsize=8)

    plt.tight_layout()

    # Save plot
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Training comparison plot saved to: {output_path}")


def plot_accuracy_comparison(skipgram_acc: float, cbow_acc: float, output_path: str,
                            figsize: tuple = (8, 6)):
    """
    Create accuracy comparison bar chart.
    """
    print(f"Creating accuracy comparison plot...")

    models = ['Skip-gram', 'CBOW']
    accuracies = [skipgram_acc, cbow_acc]

    plt.figure(figsize=figsize)
    bars = plt.bar(models, accuracies, color=['skyblue', 'lightcoral'])

    # Add value labels on bars
    for i, (bar, acc) in enumerate(zip(bars, accuracies)):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                f'{acc:.4f}\n({acc*100:.2f}%)',
                ha='center', va='bottom', fontweight='bold')

    plt.title('Word Analogy Test Accuracy Comparison')
    plt.ylabel('Accuracy')
    plt.ylim(0, max(accuracies) * 1.2)
    plt.grid(True, alpha=0.3, axis='y')

    # Add difference annotation
    diff = skipgram_acc - cbow_acc
    plt.text(0.5, max(accuracies) * 0.9,
             f'Difference: {diff:.4f} ({(diff*100):+.2f}%)',
             ha='center', va='center',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

    # Save plot
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Accuracy comparison plot saved to: {output_path}")

"""# **Main Pipeline**"""

#!/usr/bin/env python3
# Copyright 2024 Word2Vec Implementation
# Main script to run complete Word2Vec pipeline - NOTEBOOK VERSION
# ƒê√£ ch·ªânh s·ª≠a ƒë·ªÉ ch·∫°y trong notebook (b·ªè interactive menu, thay b·∫±ng config variables)

# ============================================
# CONFIGURATION - Thay ƒë·ªïi c√°c gi√° tr·ªã n√†y
# ============================================
# Dataset selection
use_wmt14 = False      # True ƒë·ªÉ d√πng WMT14 News, False = Text8
dataset_name = "Text8" # "Text8" ho·∫∑c "WMT14 News"

# Dataset size (ch·ªâ cho WMT14)
max_sentences = None   # None = full dataset, ho·∫∑c s·ªë nh∆∞ 100000
max_files = None       # None = all files, ho·∫∑c s·ªë nh∆∞ 10

# Training method
use_hs_only = False    # True = HS only (HS=1, k=0)
use_hs = False         # True = HS + NS (HS=1, k=5) ho·∫∑c HS only
# use_hs_only v√† use_hs kh√¥ng th·ªÉ c√πng True (use_hs_only s·∫Ω override)

# Phrase detection
use_phrases = False    # True ƒë·ªÉ enable phrase detection

# Gensim training
use_gensim = False     # True ƒë·ªÉ train Gensim models

# Stop after evaluation
stop_after_eval = False # True ƒë·ªÉ skip visualization

# ============================================
# IMPORTS
# ============================================
import os
import sys
from pathlib import Path

# Configure numba-cuda for CUDA PTX compatibility (Official Solution)
# Based on: https://github.com/googlecolab/colabtools/issues/5081
try:
    from numba import config
    config.CUDA_ENABLE_PYNVJITLINK = 1
    config.CUDA_LOW_OCCUPANCY_WARNINGS = 0
    print("‚úì numba-cuda configuration set for CUDA PTX compatibility")
except ImportError:
    print("‚ö†Ô∏è  numba not available - CUDA configuration skipped")
except Exception as e:
    print(f"‚ö†Ô∏è  Could not configure numba: {e}")

def check_gpu_availability():
    """Check if CUDA GPU is available."""
    try:
        from numba import cuda
        if cuda.is_available():
            device = cuda.get_current_device()
            print(f"‚úì CUDA GPU available: {device.name}")

            # Try to get memory info using pynvml if available
            try:
                import pynvml
                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                total_memory = memory_info.total / 1024**3
                print(f"  Memory: {total_memory:.1f} GB")
            except (ImportError, Exception) as e:
                # Fallback: just show device name without memory info
                print(f"  Device: {device.name}")
                print(f"  (Memory info unavailable: {e})")

            return True
        else:
            print("‚úó CUDA GPU not available")
            return False
    except ImportError:
        print("‚úó Numba CUDA not available")
        return False


def print_section_header(title: str):
    """Print formatted section header."""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}")


def print_summary(sg_acc: float, cbow_acc: float, sg_stats: dict, cbow_stats: dict):
    """Print final summary of results."""
    print_section_header("FINAL SUMMARY")

    print(f"Model Performance:")
    print(f"  Skip-gram accuracy: {sg_acc:.4f} ({sg_acc*100:.2f}%)")
    print(f"  CBOW accuracy: {cbow_acc:.4f} ({cbow_acc*100:.2f}%)")
    print(f"  Difference: {sg_acc - cbow_acc:.4f} ({(sg_acc - cbow_acc)*100:+.2f}%)")

    if sg_stats and cbow_stats:
        sg_time = sg_stats.get('epoch_time_total_seconds', 0)
        cbow_time = cbow_stats.get('epoch_time_total_seconds', 0)
        print(f"\nTraining Times:")
        print(f"  Skip-gram: {sg_time:.2f}s")
        print(f"  CBOW: {cbow_time:.2f}s")
        print(f"  Difference: {sg_time - cbow_time:.2f}s")

        sg_words = sg_stats.get('word_count', 0)
        cbow_words = cbow_stats.get('word_count', 0)
        print(f"\nData Processed:")
        print(f"  Words: {sg_words:,}")
        print(f"  Sentences: {sg_stats.get('sentence_count', 0):,}")
        print(f"  Vocabulary: {sg_stats.get('vocab_size', 0):,}")

    print(f"\nOutput Files:")
    print(f"  Skip-gram vectors: ./output/vectors_skipgram")
    print(f"  CBOW vectors: ./output/vectors_cbow")
    print(f"  Visualizations: ./output/*.png")
    print(f"  Evaluation results: ./output/*.json")


def main():
    """Main pipeline execution - NOTEBOOK VERSION."""
    # Config ƒë∆∞·ª£c set ·ªü ƒë·∫ßu file (kh√¥ng c·∫ßn interactive menu)

    # Print configuration
    print(f"\nDataset: {dataset_name}")
    if use_wmt14:
        print("  - WMT14 News Crawl (850M words, ~3.2GB)")
        print("  - Higher quality news articles")
        if max_sentences:
            print(f"  - Limited to {max_sentences:,} sentences")
    else:
        print("  - Text8 Wikipedia (17M words, ~100MB)")
        print("  - Smaller, faster to download and process")

    if use_hs_only:
        print("  üéØ Training: Hierarchical Softmax ONLY (HS=1, k=0)")
    elif use_hs:
        print("  üéØ Training: Hierarchical Softmax + Negative Sampling (HS=1, k>0)")
    else:
        print("  üéØ Training: Negative Sampling ONLY (HS=0, k>0)")

    if use_phrases:
        print("  üîó Phrase detection: Enabled")

    if use_gensim:
        print("  üìö Gensim training: Enabled")

    if stop_after_eval:
        print("  ‚èπÔ∏è  Will stop after Step 6 (Evaluation)")

    # 1. Setup & GPU Check
    print_section_header("STEP 1: GPU AVAILABILITY CHECK")
    if not check_gpu_availability():
        print("‚ö†Ô∏è  Warning: No GPU detected. Training will be slow on CPU.")
        print("Continuing anyway...")
        # B·ªè input() - kh√¥ng ho·∫°t ƒë·ªông t·ªët trong notebook

    # 2. Download & Preprocess Data
    print_section_header(f"STEP 2: DOWNLOADING & PREPROCESSING {dataset_name.upper()}")
    data_dir = "./data"

    if use_phrases:
        print("  üîó Phrase detection: Enabled (will combine frequent bigrams)")

    if use_wmt14:
        news_file = download_wmt14_news(data_dir)
        processed_dir = preprocess_wmt14_news(news_file, "./data/wmt14_processed",
                                            max_sentences=max_sentences, max_files=max_files,
                                            use_phrases=use_phrases)
    else:
        text8_file = download_text8(data_dir)
        processed_dir = preprocess_text8(text8_file, "./data/text8_processed",
                                        use_phrases=use_phrases)

    # 3. Train Skip-gram
    print_section_header("STEP 3: TRAINING SKIP-GRAM MODEL")
    skipgram_params = {
        "epochs": 10,
        "embed_dim": 100,
        "min_occurs": 5,
        "c": 5,
        "k": 0 if use_hs_only else 5,
        "t": 1e-5,
        "vocab_freq_exponent": 0.75,
        "lr_max": 0.025,
        "lr_min": 0.0001,
        "cuda_threads_per_block": 32,
        "hs": 1 if use_hs else 0
    }

    print("Skip-gram parameters:")
    for key, value in skipgram_params.items():
        print(f"  {key}: {value}")

    if skipgram_params["hs"] == 1 and skipgram_params["k"] > 0:
        print("  ‚ö†Ô∏è  Note: Learning rate will be automatically reduced by 50% to prevent gradient explosion")

    train_skipgram(processed_dir, "./output/vectors_skipgram", **skipgram_params)

    # 4. Train CBOW
    print_section_header("STEP 4: TRAINING CBOW MODEL")
    cbow_params = skipgram_params.copy()
    cbow_params["lr_max"] = 0.05
    cbow_params["lr_min"] = 0.0001

    print("CBOW parameters:")
    for key, value in cbow_params.items():
        print(f"  {key}: {value}")

    if cbow_params["hs"] == 1 and cbow_params["k"] > 0:
        print("  ‚ö†Ô∏è  Note: Learning rate will be automatically reduced by 50% to prevent gradient explosion")

    train_cbow(processed_dir, "./output/vectors_cbow", **cbow_params)

    # 5. Evaluate Skip-gram
    print_section_header("STEP 5: EVALUATING SKIP-GRAM MODEL")
    sg_acc, sg_details = word_analogy_test("./output/vectors_skipgram")
    sg_sim = similarity_test("./output/vectors_skipgram")
    save_evaluation_results({
        "accuracy": sg_acc,
        "details": sg_details,
        "similarity_test": sg_sim
    }, "./output/skipgram_eval.json")

    # 6. Evaluate CBOW
    print_section_header("STEP 6: EVALUATING CBOW MODEL")
    cbow_acc, cbow_details = word_analogy_test("./output/vectors_cbow")
    cbow_sim = similarity_test("./output/vectors_cbow")
    save_evaluation_results({
        "accuracy": cbow_acc,
        "details": cbow_details,
        "similarity_test": cbow_sim
    }, "./output/cbow_eval.json")

    # Check if should stop after evaluation (only if not using Gensim)
    if stop_after_eval and not use_gensim:
        print_section_header("STOPPING AFTER STEP 6")
        print("Skipping visualization and comparison steps as requested.")
        print(f"\n‚úÖ Training and evaluation completed!")
        print(f"üìÅ Check the ./output/ directory for:")
        print(f"  - Vectors: ./output/vectors_skipgram, ./output/vectors_cbow")
        print(f"  - Evaluation results: ./output/skipgram_eval.json, ./output/cbow_eval.json")
        print(f"  - Statistics: ./output/vectors_skipgram_stats.json, ./output/vectors_cbow_stats.json")
        print(f"\nüìä Evaluation Results:")
        print(f"  Skip-gram accuracy: {sg_acc:.4f} ({sg_acc*100:.2f}%)")
        print(f"  CBOW accuracy: {cbow_acc:.4f} ({cbow_acc*100:.2f}%)")
        return

    # 7. Train Gensim Models (if enabled)
    gensim_sg_path = None
    gensim_cbow_path = None
    gensim_sg_time = None
    gensim_cbow_time = None
    gensim_sg_acc = None
    gensim_sg_details = None
    gensim_cbow_acc = None
    gensim_cbow_details = None

    if use_gensim:
        print_section_header("STEP 7: TRAINING GENSIM MODELS")
        gensim_sg_path, gensim_cbow_path, gensim_sg_time, gensim_cbow_time = train_gensim_models(
            processed_dir,
            output_dir="./output/gensim",
            epochs=skipgram_params["epochs"],
            embed_dim=skipgram_params["embed_dim"],
            min_count=skipgram_params["min_occurs"],
            window=skipgram_params["c"],
            negative=skipgram_params["k"],
            hs=skipgram_params["hs"],
            alpha=skipgram_params["lr_max"],
            min_alpha=skipgram_params["lr_min"]
        )

        # 8. Evaluate Gensim Models
        print_section_header("STEP 8: EVALUATING GENSIM MODELS")
        gensim_sg_acc, gensim_sg_details, gensim_cbow_acc, gensim_cbow_details = evaluate_gensim_models(
            gensim_sg_path,
            gensim_cbow_path,
            output_dir="./output/gensim"
        )

        # 9. Compare with Gensim
        print_section_header("STEP 9: COMPARING WITH GENSIM")
        # Load custom model statistics
        custom_sg_time = None
        custom_cbow_time = None
        try:
            import json
            with open("./output/vectors_skipgram_stats.json", "r") as f:
                custom_sg_stats = json.load(f)
                custom_sg_time = custom_sg_stats.get("epoch_time_total_seconds", None)
            with open("./output/vectors_cbow_stats.json", "r") as f:
                custom_cbow_stats = json.load(f)
                custom_cbow_time = custom_cbow_stats.get("epoch_time_total_seconds", None)
        except FileNotFoundError:
            pass

        gensim_comparison = compare_with_gensim(
            "./output/vectors_skipgram",
            "./output/vectors_cbow",
            gensim_sg_path,
            gensim_cbow_path,
            gensim_sg_time,
            gensim_cbow_time,
            gensim_sg_acc,
            gensim_sg_details,
            gensim_cbow_acc,
            gensim_cbow_details,
            custom_sg_time,
            custom_cbow_time
        )

        if stop_after_eval:
            print_section_header("STOPPING AFTER GENSIM COMPARISON")
            print("Skipping visualization steps as requested.")
            print(f"\n‚úÖ Training, evaluation and comparison completed!")
            print(f"üìÅ Check the ./output/ directory for:")
            print(f"  - Custom vectors: ./output/vectors_skipgram, ./output/vectors_cbow")
            print(f"  - Gensim vectors: ./output/gensim/vectors_skipgram_gensim, ./output/gensim/vectors_cbow_gensim")
            print(f"  - Evaluation results: ./output/skipgram_eval.json, ./output/cbow_eval.json")
            print(f"  - Gensim evaluation: ./output/gensim/skipgram_eval.json, ./output/gensim/cbow_eval.json")
            print(f"  - Comparison: ./output/gensim_comparison.json")
            print(f"\nüìä Evaluation Results:")
            print(f"  Custom Skip-gram: {sg_acc:.4f} ({sg_acc*100:.2f}%)")
            print(f"  Custom CBOW: {cbow_acc:.4f} ({cbow_acc*100:.2f}%)")
            print(f"  Gensim Skip-gram: {gensim_sg_acc:.4f} ({gensim_sg_acc*100:.2f}%)")
            print(f"  Gensim CBOW: {gensim_cbow_acc:.4f} ({gensim_cbow_acc*100:.2f}%)")
            return

    # 10. Model Comparison (Custom Skip-gram vs CBOW)
    step_num = 7 if not use_gensim else 10
    print_section_header(f"STEP {step_num}: COMPARING CUSTOM MODELS (Skip-gram vs CBOW)")
    # Pass pre-computed accuracy values to avoid re-evaluating
    comparison = compare_models("./output/vectors_skipgram", "./output/vectors_cbow",
                                sg_acc=sg_acc, sg_details=sg_details,
                                cbow_acc=cbow_acc, cbow_details=cbow_details)

    # 11. Visualizations
    step_num = 8 if not use_gensim else 11
    print_section_header(f"STEP {step_num}: CREATING VISUALIZATIONS")

    # t-SNE plots
    print("Creating t-SNE visualizations...")
    plot_tsne("./output/vectors_skipgram", "./output/skipgram_tsne.png")
    plot_tsne("./output/vectors_cbow", "./output/cbow_tsne.png")

    # Similarity heatmaps
    print("Creating similarity heatmaps...")
    test_words = ["king", "queen", "man", "woman", "computer", "science", "university", "student"]
    plot_similarity_heatmap("./output/vectors_skipgram", test_words, "./output/skipgram_heatmap.png")
    plot_similarity_heatmap("./output/vectors_cbow", test_words, "./output/cbow_heatmap.png")

    # Training comparison
    print("Creating training comparison plots...")
    plot_training_comparison(
        "./output/vectors_skipgram_stats.json",
        "./output/vectors_cbow_stats.json",
        "./output/training_comparison.png"
    )

    # Accuracy comparison
    plot_accuracy_comparison(sg_acc, cbow_acc, "./output/accuracy_comparison.png")

    # 12. Load statistics for summary
    sg_stats = {}
    cbow_stats = {}

    try:
        import json
        with open("./output/vectors_skipgram_stats.json", "r") as f:
            sg_stats = json.load(f)
        with open("./output/vectors_cbow_stats.json", "r") as f:
            cbow_stats = json.load(f)
    except FileNotFoundError:
        print("Warning: Could not load statistics files")

    # 13. Final Summary
    print_summary(sg_acc, cbow_acc, sg_stats, cbow_stats)

    print(f"\nüéâ Word2Vec training and evaluation completed successfully!")
    print(f"üìÅ Check the ./output/ directory for all results and visualizations.")

    print(f"\nDataset used: {dataset_name}")


if __name__ == "__main__":
    # Trong notebook, kh√¥ng d√πng sys.exit() v√¨ s·∫Ω g√¢y SystemExit exception
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Training interrupted by user.")
        # Kh√¥ng d√πng sys.exit() trong notebook
        raise  # Re-raise ƒë·ªÉ notebook hi·ªÉn th·ªã error
    except Exception as e:
        print(f"\n‚ùå Error occurred: {e}")
        import traceback
        traceback.print_exc()
        # Kh√¥ng d√πng sys.exit() trong notebook - re-raise exception
        raise  # Re-raise ƒë·ªÉ notebook hi·ªÉn th·ªã error